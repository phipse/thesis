% vim:set ft=tex:
\chapter{Introduction}
\label{sec:intro}

% Die Einleitung schreibt man zuletzt, wenn die Arbeit im Großen und
% Ganzen schon fertig ist. (Wenn man mit der Einleitung beginnt - ein
% häufiger Fehler - braucht man viel länger und wirft sie später doch
% wieder weg). Sie hat als wesentliche Aufgabe, den Kontext für die
% unterschiedlichen Klassen von Lesern herzustellen. Man muß hier die
% Leser für sich gewinnen. Das Problem, mit dem sich die Arbeit befaßt,
% sollte am Ende wenigsten in Grundzügen klar sein und dem Leser
% interessant erscheinen. Das Kapitel schließt mit einer Übersicht über
% den Rest der Arbeit. Meist braucht man mindestens 4 Seiten dafür, mehr
% als 10 Seiten liest keiner.

% list of glossary entries
%\newacronym{intel}{}{INTEL\textregistered{}}
\newglossaryentry{intel}{name=Intel\textregistered{},
description={Intel\textregistered{} Corp.} }

% list of acronyms
\newacronym{ht}{HT}{hyper-threading}
\newacronym{smt}{SMT}{simultaneous multi-threading}
\newacronym{cmp}{CMP}{chip multiprocessor}
\newacronym{smp}{SMP}{symmetric multiprocessor}
\newacronym{utcb}{UTCB}{user thread control block}
\newacronym{sc}{SC}{scheduling context}
\newacronym{fifo}{FIFO}{first-in-first-out}
\newacronym{lru}{LRU}{least-recently-used}
\newacronym{llc}{LLC}{last-level cache}
\newacronym{ipc}{L4-IPC}{L4 inter-process communication}
\newacronym{instpc}{IPC}{instructions per cycle}
\newacronym{isa}{ISA}{Instruction Set Architecture}

%todo adopt title page

The operating system manages all hardware resources of a computer.
It makes these resources available to all tasks by assigning them an exclusive
share for an amount of time.
% improve above sentence
In doing so, it distinguishes between time and space shares.
% smoothe transition
% throw above away ?
Since 1980 a gap between issue rate and service time of memory requests
developed.
The time between two memory request a single core can issue decreased faster, than
the time it takes memory to answer this request.
Figure \ref{intro:fig:cpuMemGap} visualises this development between 1980 and
2010.
%
\begin{figure}[h]
  \KOMAoption{captions}{bottombeside}
  \setcapindent*{1em}
  \begin{captionbeside}[]{CPU and memory performance development since 1980.
      CPU performance is measured as time between memory requests; memory
      performance describes DRAM access latency.
      Figure from \cite[73]{hennessy_computer_2012}.}
    \includegraphics[width=0.65\textwidth]{images/cpu_mem_gap}
  \end{captionbeside}
  \label{intro:fig:cpuMemGap}
\end{figure}
%
To reduce the number of waits, several levels of caches are present on the CPU
nowadays, thus decreasing the impact of memory latency.
The drawback of several layers of caches and increasing size of caches is the
access latency and the additional cost for a cache miss, when the data
requested is not in any cache layer.
Each cache level adds the time it takes to search it, to the overall
time it takes to request the data from memory.
Thus, three cache levels cost search time for the first, second, and third plus
the memory latency.
In short, larger caches decrease the miss probability, but increase the overall
memory access latency.
Figure \ref{into:fig:accessLatencies} provides and overview over size and
access times for the memory hierarchy.
The displayed access times describe the duration from issuing the request until
receiving the data.
%
\begin{figure}[h]
  \includegraphics[width=\textwidth]{images/mem_access_hierarchy_latency}
  \caption{Size and access latencies for different levels of the
    memory hierarchy.
    The numbers displayed are from a Nehalem generation XEON server CPU.
    Figure from \cite[72]{hennessy_computer_2012}.}
  \label{into:fig:accessLatencies}
\end{figure}

Two hardware improvements minimize the time a CPU idles, while waiting for
memory: out-of-order execution and simultaneous multi-threading.
Out-of-order CPUs reorder instructions, such that a instruction waiting for
input data is surpassed by instructions with available input.
After execution the results are written to memory in their original order.
Whereas simultaneous multi-threading employs two or more hardware threads
executing on the same physical hardware.
If one hardware thread waits for memory or just uses a subset of all available
execution units, the others can utilize the idle resources.

Besides these hardware solutions, software can also reduce the number of cache
misses it produces.
Programmers of performance critical software invest time and effort to adjust
their program's data to fit into cache lines and allow for efficient prefetching.
However, besides time and money these custom adjustments are limited by the
environment the software executes in.
Two memory intensive threads sharing a processor will evict each others
cache lines, thus decreasing each others performance as they need to fetch
data from memory again.
In the era of multi-cores, to decide if these two programs will run on
the same core and cache is the job of the scheduler and load balancer.

While Linux runs a scheduler which is also in charge of load balancing, these
are two separate jobs.
It is the scheduler's duty to provide an equal amount of CPU time to each
running task, depending on the task's priority.
To decide, which tasks runs on which core on the other hand,
that is the responsibility of the load balancer.

Thus, the load balancer is not necessarily part of the kernel.
To allow for workload specific load balancers, Linux provides an interface to
overrule kernel decisions.
This is also the case for the Fiasco.OC microkernel and the L4Re user-land
environment.

A specialized load balancer can take offline knowledge into account, not
accessible by the kernel;
for example offline analysis of a program's cache usage, or communication
patterns between different parts of one or different applications,
respectively.

Such an specialized load balancer can improve the runtime of its clients by
improving cache utilization through distribution or reduce communication latencies
through co-location.
A general-purpose load balancer most likely does not know its client tasks
beforehand, thus limiting the amount of information available.

Besides performance improvements, a load balancer can also satisfy isolation
requirements of client applications; for example by assigning a security
critical application an exclusive core to exacerbate side-channel attacks.
Also real-time applications benefit from isolation, as an exclusive cache
decreases runtime influences.
\pagebreak

This thesis explores possibilities to gather runtime information about the
behaviour of a load balancer's clients, to improve each clients execution time.
Besides execution time needs, the load balancer should care for cache usage,
communication behaviour, and isolation requirements.
I discuss different sources for these information with their merits and flaws and
implement a prototype to show the benefits for client behaviour-aware load
balancing.
\\

Chapter \ref{sec:state} defines terminology for this work and presents previous
publications on \gls{smt}, shared resource usage and
congestion, and communication awareness.
It also presents Linux's completely fair scheduler, Fiasco.OC and L4Re, and
properties of the CPU architecture the prototype runs on.
In chapter \ref{sec:design}, I discuss the building blocks for the load
balancer: \gls{smt}, isolation needs, source for communication behaviour, an energy
consumption model, and the architecture of the load balancer itself.
Chapter \ref{sec:implementation} presents implementation details and
implemented algorithms.
Subsequently, chapter \ref{sec:evaluation} evaluates the prototype and answers
the question, how beneficial the balancing is.
Chapter \ref{sec:futurework} motivates future research on this topic and
concludes this thesis.


\cleardoublepage

%%% Local Variables:
%%% TeX-master: "diplom"
%%% End:
