% vim:set ft=tex:
\chapter{Introduction}
\label{sec:intro}

% Die Einleitung schreibt man zuletzt, wenn die Arbeit im Großen und
% Ganzen schon fertig ist. (Wenn man mit der Einleitung beginnt - ein
% häufiger Fehler - braucht man viel länger und wirft sie später doch
% wieder weg). Sie hat als wesentliche Aufgabe, den Kontext für die
% unterschiedlichen Klassen von Lesern herzustellen. Man muß hier die
% Leser für sich gewinnen. Das Problem, mit dem sich die Arbeit befaßt,
% sollte am Ende wenigsten in Grundzügen klar sein und dem Leser
% interessant erscheinen. Das Kapitel schließt mit einer Übersicht über
% den Rest der Arbeit. Meist braucht man mindestens 4 Seiten dafür, mehr
% als 10 Seiten liest keiner.

% list of glossary entries
%\newacronym{intel}{}{INTEL\textregistered{}}
\newglossaryentry{intel}{name=Intel\textregistered{},
description={Intel\textregistered{} Corp.} }

% list of acronyms
\newacronym{ht}{HT}{Hyper-Threading}
\newacronym{smt}{SMT}{Simultaneous Multi-Threading}
\newacronym{cmp}{CMP}{Chip Multiprocessor}
\newacronym{smp}{SMP}{Symmetric Multiprocessor}
\newacronym{utcb}{UTCB}{User Thread Control Block}
\newacronym{sc}{SC}{Scheduling Context}
\newacronym{fifo}{FIFO}{First-In-First-Out}
\newacronym{lru}{LRU}{Least-Recently-Used}
\newacronym{llc}{LLC}{Last-Level Cache}
\newacronym{ipc}{L4-IPC}{L4 Inter-Process Communication}
\newacronym{instpc}{IPC}{Instructions Per Cycle}
\newacronym{isa}{ISA}{Instruction Set Architecture}
\newacronym{eu}{EU}{Execution Unit}

%todo adopt title page
\begin{comment}
Starting with the common availability of computer systems in the 1980's, computing
and memory facilities have developed in different speeds.
While memory size grew considerably, their access speed did not change as much.
On the contrary, processor's performance increased in magnitudes until hitting
the frequency wall, which was jumped over by using many processors in a single
system.
However, processors need to access the memory to process data and since the
memory is relatively slow, processors are sitting idle waiting for data.
This effect is amplified by multi-processor systems where even more processors
query the memory.
Multiple levels of different sized caches where introduced to hide memory
access latencies, as shown in figure \ref{intro:fig:accessLatencies}.
However, in today's multi-core systems, placing programs on the right core for
efficient execution and cache use remains a challenge.

In this work
\end{comment}

Starting with the common availability of computer systems in the 1980's,
computing and memory facilities have developed in different directions.
Uniprocessor speeds increased until they hit the frequency wall, which then led
to the rise of multi-core processors.
Whereas memory sizes increased significantly, memory speed did not see an
equal development.
The increasing difference in processing speed, displayed in figure
\ref{intro:fig:cpuMemGap}, led the processor vendors to obscure the gap by
introducing numerous levels of on-die caches.
%, differing in access speed andsize.
These caches allow to prefetch memory based on typical memory access patterns
and reduce the number of times the CPU sits idle.
However, search duration increases with cache size and if the requested data is
not present, the search duration increases the memory access latency additionally.
Figure \ref{intro:fig:accessLatencies} shows size and latency of a Nehalem
generation Intel CPU.

Besides reducing the number of waits for data from memory, the waiting time can be
used for computation.
Out-of-order processors execute other instructions while waiting for data and
therefore reduce the idle time of the processor.
Another technology to use idle processing resources is simultaneous
multi-threading.
It employs several hardware threads sharing a processor's execution units to
provide several instruction streams.
While one instruction stream waits for memory, others are able to utilize the
otherwise idle resources.
%
\begin{figure}[h]
  \centering
  \includegraphics[width=0.65\textwidth]{images/cpu_mem_gap}
  \caption[Processor and memory performance gap 1980 - 2010.]
    {CPU and memory performance development since 1980.
      CPU performance is plotted as memory request per second, memory
      performance as requests served per second.
      Figure from \cite[73]{hennessy_computer_2012}.}
  \label{intro:fig:cpuMemGap}
\end{figure}
%
\begin{comment}
To reduce the number of waits and obscure the gap, several levels of caches
are present on today's processors, thus decreasing the impact of memory latency.
The drawback of several layers of caches and increasing size of caches is the
access latency and the additional cost for a cache miss, when the data
requested is not in any cache layer.
Each cache level adds the time it takes to search it, to the overall
time it takes to request the data from memory.
Thus, three cache levels cost search time for the first, second, and third plus
the memory latency.
In short, larger caches decrease the miss probability, but increase the overall
memory access latency.
Figure \ref{into:fig:accessLatencies} provides and overview over size and
access times for the memory hierarchy.
The displayed access times describe the duration from issuing the request until
receiving the data.
\end{comment}
%
\begin{figure}[h]
  \includegraphics[width=\textwidth]{images/mem_access_hierarchy_latency}
  \caption{Size and access latencies for different levels of the
    memory hierarchy.
    The numbers displayed are from a Nehalem generation
    Xeon\texttrademark{} server CPU.
    Figure from \cite[72]{hennessy_computer_2012}.}
  \label{intro:fig:accessLatencies}
\end{figure}
\begin{comment}
Two hardware improvements minimize the time a CPU idles, while waiting for
memory: out-of-order execution and simultaneous multi-threading.
Out-of-order CPUs reorder instructions, such that a instruction waiting for
input data is surpassed by instructions with available input.
After execution the results are written to memory in their original order.
Whereas simultaneous multi-threading employs two or more hardware threads
executing on the same physical hardware.
If one hardware thread waits for memory or just uses a subset of all available
execution units, the others can utilize the idle resources.
\end{comment}

Besides hardware solutions, software can also increase memory efficiency.
Programmers of performance-critical software invest time and effort to adjust
their program's data to fit into cache lines and allow for efficient prefetching.
However, besides these custom adjustments are limited by the
environment the software executes in.
Two memory intensive threads sharing a processor will evict each other's
cache lines, thus decreasing each other's performance as they need to fetch the
data from memory again.
In the era of multi-cores, to decide if these two programs will run on
the same core and cache is the challenge the load balancer faces.

\begin{comment}
While Linux runs a scheduler which is also in charge of load balancing, these
are two separate jobs.
It is the scheduler's duty to provide an equal amount of CPU time to each
running task, depending on the task's priority.
To decide, which tasks runs on which core on the other hand,
that is the responsibility of the load balancer.

Thus, the load balancer is not necessarily part of the kernel.
To allow for workload specific load balancers, Linux provides an interface to
overrule kernel decisions.
This is also the case for the Fiasco.OC microkernel and the L4Re user-land
environment.
\end{comment}

A specialized load balancer can take offline knowledge into account that is not
accessible by the kernel;
for example offline analysis of a program's cache usage.
Such a specialized load balancer can improve the run-time of its clients by
improving cache utilization.
A general-purpose load balancer does not know its client tasks
beforehand, thus limiting the amount of information available.
Besides performance improvements, the load balancer can also satisfy isolation
requirements of client applications; for example by assigning a security
critical application an exclusive core to mitigate side-channel attacks.

This thesis explores possibilities to gather run-time information about the
behaviour of a load balancer's clients, to provide for efficient execution of
all clients.
Besides execution time requirements, the load balancer should account for cache usage,
communication patterns, and isolation requirements.
I discuss various sources for these information with their merits and flaws and
implement a prototype to show the benefits of load balancing that is aware of
client behaviour.
\pagebreak

Chapter \ref{sec:state} defines terminology for this work and presents previous
publications on \gls{smt}, shared resource congestion, and communication awareness.
It also presents Linux's completely fair scheduler, Fiasco.OC and L4Re, and
properties of the CPU architecture the prototype runs on.
After presenting the current state of research, chapter \ref{sec:design} discusses
the building blocks for the load
balancer: \gls{smt}, isolation needs, sources for communication behaviour, an energy
consumption model, and the architecture of the load balancer itself.
Chapter \ref{sec:implementation} presents implementation details and
implemented algorithms.
Subsequently, chapter \ref{sec:evaluation} evaluates the prototype and answers
the question, how beneficial the balancing is.
Chapter \ref{sec:futurework} motivates further research on this topic and
concludes this thesis.


\cleardoublepage

%%% Local Variables:
%%% TeX-master: "diplom"
%%% End:
