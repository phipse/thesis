\section{Related Work}
\label{state:related}

This section presents prior work and puts emphasis on the parts pertinent to
this thesis.
One assumption for this work is already in place:
the target architecture is a single ISA, partly symmetric homogeneous
multicore.
Partly symmetric means the target architecture supports \gls{smt},
but besides this, it is a \gls{smp} architecture.
Hence, research proposing hardware changes, e.g. \cite{cruz_dynamic_2014},
is out of scope.
Also research focusing on heterogeneous or multi-ISA architectures, e.g.
\cite{sarma_smartbalance_2015}, is not part of the review.

%One restriction applies to this review: mechanism proposing hardware changes
%are not in the focus of this work.
%This restriction excludes interesting ideas to gather behavioural information,
%e.g. observation of cache coherency traffic proposed in
%\cite{cruz_dynamic_2014}.


In literature, different approaches to load balancing on a multicore have been
explored.
\citeauthor{snavely_symbiotic_2000} introduced the term \emph{symbiotic
scheduling} in \cite{snavely_symbiotic_2000}, describing a scheduling technique
for \gls{smt}-systems.
They propose scheduling those two jobs on a shared core, which provide the best
utilization of said core.
The combination of two jobs on two corresponding \gls{smt}-cores call they a
co-schedule.
The performance of a co-schedule is defined as the turnaround or response time:
the time between the arrival of a job in the ready queue and its completion.

In their experiments they run independent jobs on a simulator modeling a Compaq
Alpha 21264 out-of-order processor with hardware additions for multi-threading.
This simulator model includes performance counters to ``capture dynamic execution
information''(\autocite[236]{snavely_symbiotic_2000}).
To determine a good schedule the authors compute the weighted speedup \textit{WS} for
a fixed time interval.
For each job the change in instructions per cycle (IPC) compared to
single-threaded execution is calculated.
The sum of these values for all jobs is the weighted speedup of this
schedule.
$$ WS(t) = \sum_{i=1}^n (\text{realized IPC job}_i / \text{single-threaded IPC
job}_i)$$
Their developed scheduler consists of two phases: sampling and symbiosis.
During the sampling phase different job combinations are measured.
When enough samples are gathered, the scheduler moves to the symbiosis phase
and uses schedules, which promise the highest weighted speedup.
The scheduler will move back and forth between these phases, as the job-mix
changes.

The measurements of the performance counters are used to create several
predictors for the behaviour of jobs in the future and, hence, the best schedule.
The best predictor in their study made a majority vote of all other predictors
to determine the best schedule.
\citeauthor{snavely_symbiotic_2000} conclude, that good job symbiosis depends
not only on high IPC, but on a multitude of other factors, too, e.g. usage
conflicts in shared hardware resources or cache hit rates.
\\


In \citeyear{snavely_symbiotic_2002} \citeauthor{snavely_symbiotic_2002}
discussed the topic of priorities in symbiotic schedules.
In their earlier work (\cite{snavely_symbiotic_2000}) they assumed same
priority jobs.
Now, they present two meanings for priority: ``(A) guarantee a fraction of
machine proportional to priority'' or ``(B) guarantee a fraction of
single-threaded performance proportional to priority''
(\autocite[70]{snavely_symbiotic_2002}).
The authors base their priority mechanisms on two values: SOLOFRAC and CS.
SOLOFRAC is the, from the priority derived fraction of the CPU time,
a job needs to be scheduled alone and CS is the fraction of the CPU time,
where no job is solo scheduled.

The 'Naive' priority mechanism schedules a job alone for its SOLOFRAC
share and co-schedules for CS cycles. Behind this stand two assumptions: first,
during co-scheduling each job is an equal share of the cycles assigned.
And second, the number of jobs does not exceed the number of \gls{smt}-cores.

The proposed 'Symb' mechanism on the other hand, can handle more jobs
than available \gls{smt}-cores and observes job behaviour. By running jobs
solo, 'Symb' determines the job's ``'natural' IPC''
(\autocite[71]{snavely_symbiotic_2002})
and then evaluates the job's performance during a phase of co-scheduling.
Based on this measurement, the SOLOFRAC portion is reduced such that the job get its
rightful share of cycles. The increase in co-scheduling time leads to increased
utilization of the system.

Other presented approaches in \cite{snavely_symbiotic_2002} require hardware
changes and are therefore out of scope.
\\

While \citeauthor{snavely_symbiotic_2000} claim in
\cite{snavely_symbiotic_2000} a reduction in average turnaround time by up to
17\%, \citeauthor{eyerman_revisiting_2015} state a mere 3\% to 6\% increase in
average throughput with an optimal symbiotic scheduler compared to a
first-come-first-serve scheduler.
The notable difference explain the authors in \cite{eyerman_revisiting_2015} as
different experimental views. \citeauthor{snavely_symbiotic_2000} study a fully
loaded system with twice as much ready jobs as SMT-cores and observe the
turnaround time.
The turnaround time of a job is mainly influenced by the time it waits in the
ready queue, when all cores are loaded.
If the maximum throughput increases slightly, the waiting time is reduced,
hence, a significant reduction in turnaround time is observed.
Also, a job reordering in a shortest remaining time first manner, can also
reduce the average turnaround time.
For these reasons \citeauthor{eyerman_revisiting_2015} make the case,
that turnaround time alone can be a misleading measure.

Furthermore, maximum throughput cannot be used, because systems are not
designed to run at peak load all the time, as this leads to long queues and
turnaround times.
Average throughput equals average load if the system is not fully loaded,
therefore, it is a useless comparator.
Instead, they advocate to evaluate processor utilization (understood as average
number of executing cores) and empty periods.
If throughput is improved, the processor utilization decreases and empty
periods are enlarged, as the time to execute the current load is reduced.
Hence, using these two measures scheduler caused improvements can be compared.

\citeauthor{eyerman_revisiting_2015} conclude that one should not expect large
improvements through symbiotic scheduling, but particular workloads may benefit.
Furthermore, previous studies on partially symmetric mutlicores showed
notable throughput gains by symbiotically co-scheduling applications on
SMT-packages.
\todo{which particular workloads? Which previous studies? What's
a notable gain?}
\\

Measuring the performance of a specific schedule in a \gls{smt}-system is
difficult, especially, if no offline knowledge is used.
But even then, pinpointing the performance gain to the scheduling decision is
only manageable with fine granular observation of the system.
However, \cite{snavely_symbiotic_2000} and \cite{eyerman_revisiting_2015}
showed the relevance of a symbiotic co-schedule to fully utilize the shared
hardware resources.
Modern \gls{intel} processors use \gls{smt} in each physical core, but besides
parts of the cache and memory subsystem, the cores are separated and
identical.
This limits the effectiveness of symbiosis analysis.
\\

\todo{separate each research block with a paragraph?}
A different techniques is presented by \citeauthor{banikazemi_pam_2008}
regarding thread-to-core mappings on a \gls{cmp}-system.
In \cite{banikazemi_pam_2008} they present \emph{pam}: a performance/power-aware
meta-scheduler, working on top of the default linux scheduler.
While the default scheduler runs, pam gathers information via performance
counters on cache occupancy, cycles per instruction, and the miss ratio of l1
and l2 caches for each thread.
If pam can predict a better thread-to-core mapping, based on this information,
it overrules the default scheduler and enforces the better mapping.
With a 'goodness' metric pam rates the current mapping for the last
time interval.
Depending on the goodness value of the pam mapping being better than the one of
the default scheduler, pam continues to enforce its mapping or falls back to
default.
Goodness can be describing performance, power or energy consumption.

The experiments ran on a dual socket, quad-core machine, where two cores
share an l2 cache.
The processor does not support \gls{smt}.
Hence, the performance gains stem not from symbiosis, but rather from
congestion awareness.
\\
\todo{transition is a bit short}

Reducing cache congestion was the main point in \cite{knauerhase_using_2008}.
Like \citeauthor{banikazemi_pam_2008}, they worked on a \gls{cmp}-machine with
two separated cache groups sharing an l2 cache.
Similarly, they tried to spread memory intensive tasks as much as possible,
to reduce interference in the shared cache.
The authors introduce the metric called 'cache weight', which denotes the
\gls{llc}-cache misses per cycle for the last time interval.
Their research showed, that longer histories worsen the prediction, as thread
behaviour changes over time.
To achieve the best possible spread of all threads across cache groups,
\citeauthor{knauerhase_using_2008} balance the threads in the \gls{llc}-group
based on the aggregate cache weight.
New threads are assigned to the cache group with the lowest weight and only
cache light tasks are migrated across group boundaries.
However, to care for workload changes, an overweight thread is periodically
migrated to the lightest \gls{llc}-group.
Furthermore, cache light threads suffer next to cache intensive ones, as they
most likely start with a cold cache, each time they are scheduled.
To compensate for this, \citeauthor{knauerhase_using_2008} propose to transfer
CPU time from the cache heavy to the suffering threads.
This fairness compensation reduced the performance degradation of the most
suffering thread to less than 1\%.
\\


But \gls{cmp} and \gls{smp} systems share more resources than just the cache.
In \cite{fedorova_managing_2010}, \cite{zhuravlev_addressing_2010}, and
\cite{zhuravlev_survey_2012} \citeauthor{fedorova_managing_2010} and
\citeauthor{zhuravlev_addressing_2010} further investigate congestion on shared
resources.
Although their assumption in \cite{fedorova_managing_2010} was, that memory
reuse patterns are the indicator of
choice to predict cache interference between threads, their research lead to
the conclusion that \gls{llc}-misses are a superior predictor.
Memory reuse patterns model the access frequency of a memory area and the miss
frequency for a particular application.
This analysis has to be done offline and assumes, that applications with high
reuse and low miss rates do not affect other applications negatively, but
applications with low reuse and high miss rates are likely to trash the cache
of others.

To do this analysis online the authors derived a metric called 'Pain', which
consists of two parts: sensitivity and intensity.
The former describes how much an application suffers from sharing caches with
others, the latter how much an application will hurt others.
The pain application A feels when scheduled with B is the product of A's
suffering and B's intensity.
Vice versa for B.
The sum of both describes the expected performance degradation for these two
applications, when being scheduled together on the same \gls{llc}.
To approximate sensitivity and intensity online, they assumed cache miss and
access rates would be a good measure, but it turned out only the cache miss
rate is relevant.
As memory reuse patterns only focus on the cache as a shared resource, the
cache miss rate also displays the load on prefetching hardware, memory
controller and memory bus.
Load on these shared resources showed to be the main reason for performance
degradation in most of their benchmarks (\cite{zhuravlev_addressing_2010}).

\citeauthor{zhuravlev_addressing_2010} implemented their off- and online
derived pain metrics in two scheduling algorithms: Distributed Intensity (DI)
and Distributed Intensity Online (DIO). \todo{emph?}
DI uses the Pain metric derived offline from the stack-distance profiles,
whereas DIO uses Approx-Pain derived from the \gls{llc} miss rate.
Like \citetitle{banikazemi_pam_2008}, both schedulers are implemented in the
Linux user level and override the default scheduler.

The DI algorithm orders the applications based on their solo cache miss rate derived
offline.
This order is used to split the application set into two halfs: above and below
average miss rate.
Each core runs one application from each half.
If the miss rate changes at runtime, the order changes, which could impair
the performance.
However, \citeauthor{zhuravlev_addressing_2010} observed only minor place
switching, which they not expect to harm the performance.

DIO on the other hand, constantly measures the applications miss rate and is
thereby able to react to phase changes during runtime.
The algorithm collects data every billion cycles and uses a running average to
sort the applications.
Besides this it acts like DI.

\citeauthor{zhuravlev_addressing_2010} ran tests on a \gls{intel}
Xeon and AMD Opteron.
The Xeon consists of eight cores on four chips sharing the L2 cache.
The Opteron eight core has two chips sharing a L3 cache each.
Leading to four cache groups in the Xeon case and two in the Opterons.
Their tests show only small performance differences for their tested workloads
on both machines.
The authors ran eight different workloads on each machine and compared the
performance improvement to the default Linux scheduler.
Of the total of 18 tests only three tests show performance differences above
two percent.
In the Opteron's case DIO improved all workloads, whereas DI showed gains in
all but one.
The Xeon showed a mixed picture for DI and DIO: from 14 percent gains to four percent loss
compared to Linux's default scheduler.
The good overall performance of the default scheduler account
\citeauthor{zhuravlev_addressing_2010} to the small test set and the relatively
low probability to select the worst thread combination.
They emphasise the performance gain of each individual application with DI and
DIO scheduling and the reduced variance in execution time from run to run.

The tested workloads consisted of pairwise independent applications. Thus the
trade off between placing two communicating, high miss rate applications on the
same core, or communicating across chip boundaries was no research question.
Additionally, \gls{cmp} architectures gave way to \gls{smp} architectures,
which do not necessarily provide strictly separated cache groups.
Hence, the performance gain must be reevaluated.
\\

Up to now, all research covered assumed independent, non-interacting threads.
\citeauthor{hofmeyr_load_2010} present in \cite{hofmeyr_load_2010} a load
balancing technique explicitly assuming communicating, parallel applications.
They propose a technique called \emph{speed balancing} with a notion for fast
and slow cores.
They assume an oversubscribed environment, where the number of runable threads
is greater than the number of cores.
Fast and slow cores are determined by run queue length.
Thread progress is slower on longer queues, hence, slow cores are cores with a
longer run queue.
The term speed is defined as the relation of execution time per elapsed wall
clock time.
The speed of each thread determines, if it is running on a fast or a slow core.
\todo{Is speed or run queue length defining a slow core?}
The algorithm presented by \citeauthor{hofmeyr_load_2010} runs one thread per
core producing a speed measure for each thread and the core respectively.
To compare core speeds, a global average core speed is computed.
To provide equal progress opportunities, each thread needs to run on a fast
core at least once.
Hence, the balancing thread on a fast core searches for a suitable thread to
migrate from a slow core and pull it to its local core.
Thereby, the balancing thread tries to migrate the least migrated thread to
avoid constant migration of the same thread.

The parallel application model the authors assume consists of one application
spawning several worker threads, where a computation interval is followed by a
communication barrier.
Their implementation takes this application as input, executes it and provides
equal progress to each thread.
They do neither model communication between different tasks in a client-server
notion, nor at arbitrary points in time.
Hence, reducing the time the slowest thread needs to reach the synchronization
point improves overall application performance.
\\

\paragraph{Energy efficiency}
But performance is not a one dimensional value anymore.
Performance/watt is the important measure on battery supported devices like
notebooks or smartphones, and also in datacenters, as less watts mean less
energy cost for running and cooling the datacenter.
In \cite{imes_poet_2015} a mobile \gls{intel} Haswell and a Samsung Exynos5 Octa are
compared regarding their minimal energy consumption.
The authors show that different strategies are necessary to achieve minimal
energy consumption of the two processors.
In case of the \gls{intel} Haswell processor, race to idle proved to be the
strategy to reduce energy consumption.
The Samsung processor on the other hand, should run constantly at low clock
speeds.

\cite{le_sueur_slow_2011} considers systems loads below 30\% and different
workloads: video playback and a web-server.
Their measurements suggest, that it is best to maintain low clock speeds, as
long as the system load and quality of service guarantees allow it.
During idle time, the processor changes to deep sleep states and saves
energy.
The recovery from deep sleep states -- power up core, load data into core local
caches -- increases system load only slightly.
To conserve energy, turbo boost should not be used, as the gained frequency
comes at a high energy cost.

It is interesting for this work, how a load balancer can change the assignment
of threads to cores, to reduce overall energy consumption.
In \ref{sec:design} different models will be discussed.
\\

\paragraph{Cache side-channel attacks}
The CPU cache has proved to be a great side channel to deduce private
cryptographic keys.
To different attacks are discussed here: FLUSH+RELOAD and PRIME+PROBE.

The former exploits a x86 instruction to flush a specified cache line, if read
access is allowed.
Another necessity is shared memory between attacker and victim, e.g. binary
code in shared libraries or executables.
The third part are branches in the code path of the attacked program.
\citeauthor{yarom_recovering_2014} present in \cite{yarom_recovering_2014} an
attack to recover private keys via FLUSH+RELOAD.
Thereby, a set of cache lines is flushed from the cache and after a waiting
period it is reloaded.
If the cache line was present in the cache, the time it takes to read the data
is small.
If it is not present in the cache the read from memory is significantly slower
for \gls{llc} lines.
\todo{numbers?}

The attack uses the fact, that the OpenSSL implementation of the Montgomery
ladder executes different branches depending on the bit tested is zero or one.
If the data containing the \textit{then} branch is present in the cache, the
tested bit was one, if the \textit{else} branch data is present, the bit was
zero.
The length of the waiting period presents a trade-off between missing
subsequent accesses if the period was to long or observing single accesses as
duplicates of themselves.
In \cite{yarom_recovering_2014} the private key was recovered using this
technique.
Other implementations of the Montgomery ladder, e.g. in Network and
Cryptographic Library (NaCL) by Bernstein et al.,
\todo{footnote, cite}
do not rely on branching and are therefore not susceptible to this attack.

However, shared memory with the target application is necessary.
But between virtual machines no data is shared, making FLUSH+RELOAD
impossible.
The PRIME+PROBE attack on the other hand works without this requirement.
This attack writes known data to a set of cache lines, the eviction set,
and accesses the same data after a waiting period.
If a cache line was used by other processes the data is gone and it takes
longer to reload it from memory.

The attack utilises the inclusiveness property of the CPU cache, to observe
changes in core local L1 and L2 caches in the \gls{llc}.
Each cache line of the L1 caches is present in L2 and each line present in L2
is present in the L3 cache.
\todo{picture}
Therefore the eviction set must cover the cache lines in the \gls{llc} that
correspond to the core local caches.
Hence, the core running the application, which is attacked must be known.
Now, after the right set of cache lines is found, the access frequency can be
observed.
The access to the cache set reveals squaring operations in the algorithm.
Based on the time passed between two squaring operations, the attacker deduces
a one or zero bit.
The execution path in case of a one bit is longer, hence more time passed.
Two attacks are presented in \cite{liu_last-level_2015} and
\cite{inci_seriously_2015}, both recovering a private key of an attacked
virtual machine.
\citeauthor{inci_seriously_2015} tested their attack on Amazon EC2 instances,
proving its practicality in a real world scenario.

Separating security critical applications on architectures, with shared
\gls{llc} raises the difficulty of an attack, but does not mitigate it.


\todo{linux cfs scheduler?}


% -----------------------------------------------------------------------------


\begin{comment}
\begin{itemize}
  \item Assumptions: single ISA, no heterogeneous cores, SMP, one specific CPU
    architecture \checkmark
  \item Goals: load balancer providing isolation, stable execution times for the
    same workload, reduced congestion on shared resources, two balancing
    strategies: performance and low energy usage;
  \item introduce different approaches: speed balancing, symbiotic scheduling,
    congestion aware scheduling

    \begin{itemize}
      \item concisely introduce the work
      \item highlight the key aspects, goals, achievements
      \item relevant achievements I will use
      \item drawbacks of the work
    \end{itemize}
  \item symbiotic scheduling:
    \begin{itemize}
      \item term coined by \citeauthor{snavely_symbiotic_2000} in
	\citeyear{snavely_symbiotic_2000} \checkmark
      \item technique for SMT to improve performance of threads scheduled on
	two hardware sharing SMT cores \checkmark
      \item runs threads on two corresponding SMT cores and measures their
	performance in a simulator;
      \item explain performance, turnaround/response time. How measured?
	\checkmark
      \item sampling phase needs time, but can be done, while a default
	scheduler runs \checkmark
      \item restriction on changes in workload to ease sampling phase
      \item summarize all restrictions/assumptions
      \item select thread pairs/co-schedules for smt-core-pairs with most throughput for
	all threads
      \item try to combine workloads that use different cpu hardware
	units, e.g. fp workload and integer workload; heavy cache usage, little
	cache usage
      \item Measurements in their \citeyear{snavely_symbiotic_2000} paper use
	perf counters not present in real HW, e.g. conflicts inside FP units,
	FPqueue full.

      \item  \citeauthor{snavely_symbiotic_2002}: simulator, no real hardware,
	but priority sensitive (2002) \checkmark
      \item priority is perceived as ``guarantee a fraction of machine
	proportional to priority'' or `` guarantee a fraction of
	single-threaded performance proportional to priority'' \checkmark
      \item can it also be seen as a guarantee, that no lower priority thread is
	scheduled unless I have been selected to run? Can I guarantee that in a
	multicore setup, without knowledge of the execution times? -> No.
	\todo{is an example beneficial here?}
	I can just guarantee, that the plan based on the prediction cares for
	priorities.

      \item \citetitle{eyerman_revisiting_2015}: symbiotic scheduling does only
	provide an average throughput gain of around 3%.
      \item reason: the most test case assumptions in literature are beneficial
	for symbiotic scheduling, but not close to reality.
      \item gather all information for a optimal scheduler with total knowledge
      \item theoretical optimal schedule knows the performance of all tasks in
	all possible combinations with all other tasks.
      \item use simulator to measure tasks on a 'reference core' and
	compute throughput and IPC values for different schedules.
      \item First-come-first-served scheduler is close to the theoretically
	optimal scheduler
      \item arrival time of tasks is critical for FCFS schedule throughput and
	hence determines, how close FCFS comes to the optimal scheduler
      \item difference in experiments: turnaround time reduction in latency
	experiment(\cite{snavely_symbiotic_2000}) vs. average throughput in a
	maximum throughput experiment (\cite{eyerman_revisiting_2015}).
      \item processor utilization or empty time are better indicator of
	scheduler efficiency than only presenting turnaround time, best use all
      \item compare scheduler by comparing these three indicators
      \item overload situations := job arrival > max throughput
      \item besides FCFS scheduler all other need an offline phase to measure
	the jobs, to be able to determine an optimal co-schedule.
    \end{itemize}

  \item congestion aware scheduling
    \begin{itemize}
      \item goal: minimizing usage of shared hardware resources
      \item shared hw resources are: shared caches, prefetching hardware, bus
	and memory controller
      \item options: cache line reuse / memory reuse pattern vs. miss rate
      \item \citeauthor{fedorova_managing_2010} started out with cache line
	reuse and came to the conclusion, that the offline measurement of
	memory reuse pattern and conclude, that this is not the best approach
	to approximate congestion.
      \item they develop a PAIN metric, combining sensitivity and intensity
	concerning shared cache usage: sensitivity: how much do I suffer by
	shared cache; intensity: how much do I hurt others;
      \item offline measurement possible, but what is a good online
	approximation? Cache-miss rate and cache-access rate? (Intuitively
	correlate with intensity and reuse frequency)
      \item evaluation shows best online approximation for pain metric is LLC
	miss rate: combines sensitivity and intensity
      \item miss rate expresses load on FSB, prefetching hardware and
	mem controller, which are the main cause for performance degradation,
	not the cache. (what is PAMs cache occupancy ratio worth now?)
      \item ``An application aggressively using prefetching hardware will also
	typically have a high LLC miss rate, because prefetching requests for
	data that is not in the cahce are counted as cache misses. Therefore, a
	high miss rate is also an indicator of the heavy use of prefetching
	hardware.'' (\cite{fedorova_managing_2010}).

      \item \cite{knauerhase_using_2008} use LLC-misses as predictor
	for cache interference and attempt to keep the sum of cache
	weights/LLC-misses of all running processes close to a medium value
      \item they also use task behaviour of the last time quantum as predictor
	for the behaviour in the next; temporal locality predictor.
      \item migration across cache borders: avoid migrating overweight/ high
	cache impact tasks, try to balance cache groups by migrating small
	tasks (assuming CMP architecture). How high is the impact, if LLC is
	shared anyway?
      \item periodically, migrate overweight task to find a possibly better
	load distribution
      \item fairness: give cache light tasks, that suffer a lot due to being
	scheduled next to cache intensive tasks, bonus execution time.
      \item assumed independent jobs

      \item \citetitle{banikazemi_pam_2008}: use real CMP system: 8 core two
	socket IBM Blade
      \item use symbiotic approach on CMP systems, less shared HW, no SMT
      \item relies on Linux scheduler as default
      \item only when better performing thread-to-core mapping is predicted, the
	default scheduler is overruled for a defined count of scheduling-cycles
      \item uses hardware performance counter evaluate and support decisions
      \item goodness metric to rate the thread-core-mapping if not better than
	last goodness rating of default scheduler, fall back to default
	scheduler
      \item mathematical model for the algorithm
      \item measured parameters are cache occupancy ratio, cycles per
	instruction, L1	and L2 miss ratio
      \item compute performance estimation for selected plan
    \end{itemize}

  \item security relevance of shared resources
    \begin{itemize}
  \item cache side channel attacks undermining security of OpenSSL; how thread
    placement minimizes side channel surface;
    \end{itemize}

  \item what is relevant and will be used, where improves this work the
    previous (contribution)
  \end{itemize}

\paragraph{ \cite{sarma_smartbalance_2015} }
\citeauthor{sarma_smartbalance_2015} write about their approach to balance work
in a heterogeneous system.
They assume a system with a single ISA and several CPUs with different performance
characteristics and different hardware features.
\todo{check different hw features}
They introduce categories for each performance level and measure the
performance differences between each level.
This results in a matrix displaying the performance gain or degradation, when
a work package (e.g. thread) is migrated.
Their load balancing algorithm has three phases: sense, predict, balance.
During the sense phase the algorithm observes the CPU utilization for an intervall.
An intervall consists of configurable many clock ticks.
The sense-result is then used by the prediction phase to compute a load
expectation for the different performance levels.
Based on this forecast a balancing decision is made and enforced in the third
phase.
Their model can also select the best CPU size for a optimal performance per
joule ratio.

\textbf{relevant} load balancing algorithm: sense, prediciton, balance,
intervall;

\textbf{not} heterogeneous processors, gem5, big.LITTLE, performance
matrix;

\paragraph{ \cite{hofmeyr_load_2010} }
evenly distributed run\_queues are assumed, slow cores are cores with longer
queues than others.

\textbf{relevant} notion of speed of a processor: $speed = t_{exec}/t_{real}$;
sched\_setaffinity to migrate threads; PIDs of all the threads of a task;
core speed computation and comparison algorithm;
distributed balancing thread with global synchronization;
assumptions \& drawbacks of kernel level load balancing;
in oversubscibed environments balance trumps locality;
\textbf{assume interacting tasks}


\textbf{be aware} turbo boost -> different core speeds on same CPU;
sched\_yield and sleep difference: run queue;

\textbf{not} NUMA, MPI, OpenMP;

\textbf{shortcommings} Tigerton: no \gls{ht}, no turbo-boost, no l3 cache,
and a dual-die CMP architecture.


\paragraph{ \cite{zhuravlev_survey_2012} }
\citeauthor{zhuravlev_survey_2012} provide a survey over scheduling techniques,
aiming at better usage of shared resources in multicore processors.
They focus on \gls{cmp}s, which share caches between at
least two cores, but not between all cores.
Techniques like hyper-threading are also taken into account.
\todo{redo this section, cmp, smp already in terminology}
Simultaneous multiprocessors (SMPs) are not in the focus of their survey.
\gls{intel} newer architectures, e.g. Haswell, is a \gls{smp} architecture, where each
physical core has dedicated L1 \& L2 caches, but the L3 cache is shared between
all cores.
Their algorithmic focus lies on contention-aware schedulers, which consists of
four building blocks: objective, prediction, decision, and enforcement.


\paragraph{ \cite{knauerhase_using_2008} }
In \citetitle{knauerhase_using_2008} \citeauthor{knauerhase_using_2008} present
a mechanism to observe \gls{llc} misses and references, retried instructions,
core cycles, and reference cycles.
These information describe the behaviour of the threads in the system.
They assume a \gls{cmp} system and address three issues: cache interference,
migrating threads across caches and fairness between threads.

To reduce interference between cache heavy and light threads, they use the
following heuristics: cache miss per cycle, behaviour in the last time quantum,
and sum of the cache weights.
The goal is to run cache heavy threads on different \gls{llc}-groups, meaning
cores that share the same \gls{llc}.
The cache miss per cycle has shown to be a good heuristic, as it also displays
the memory access and, hence, the load on the memory bus.
Also, behavioural history longer than the last time quantum has proven
unnecessary, as the thread behaviour changes over time and longer history
clouds the prediction.
Finally, the cache weight is the sum of the weight of each thread running on
the same \gls{llc}-group.
A threads cache weight is the number of cache entries it uses. \todo{check
that}

To decide when and which threads to migrate to another \gls{llc}-group, the
thread weight and the cache load per \gls{llc}-group is computed.
New threads are assign tho the \gls{llc}-group with the smallest cache load.
To achieve better fairness between the \gls{llc}-groups, overweight threads are
migrated between groups periodically.
In general, migrations between \gls{llc}-groups are prevented, as the migrated
thread has to repopulate the cache on the new core.

To increase the fairness between cache light and cache heavy threads,
\citeauthor{knauerhase_using_2008} propose to increase the execution time of
cache light threads, as their performance suffers from running besides cache
heavy threads.
\todo{How is performance defined?}

The authors wrote a cachebuster and spinloop thread to simulate cache heave and
cache light threads. Additionally, application from the SPEC CPU 2000 benchmark
suite were used to provide further experimental prove to their claims.

\textbf{ shortcommings } CMP system


\paragraph{ \cite{yarom_recovering_2014} }
\textbf{motivation} FLUSH+RELOAD cache side channel attack;

\paragraph{ \cite{bernstein_cache-timing_2005} }
\textbf{motivation} timing side channel attack against cache;


\paragraph{ \cite{eyerman_revisiting_2015} }
\textbf{relevant} symbiotic scheduling on SMT systems;
list of assumptions;
definition of their experiments;
def. partially symmetric homogeneous multicores;
calculation of optimal throughput of a processor;


\paragraph{ \cite{fedorova_managing_2010} }
\textbf{assumptions:} no interaction between threads: no shared data, no
communication;

\begin{itemize}
  \item LLC miss rate of threads \textit{vs.} memory-reuse pattern
  \item more arguments for mem-reuse pattern
  \item cache sensitivit and intensity of threads --> Pain metric (offline)
  \item cache-miss and cache-access rate
  \item online metric to approximate pain metric -> approx-pain
  \item approx-pain uses perfcounters to measure LLC-miss-rate, as this showed
    to be the best predictor for sensitivity and intensity
  \item LLC-miss-rate predicts contention in other shared hardware
  \item LLC-miss-rate perf counter measures prefetching miss
  \item FURTHER: Distributed Intensity Online, Power Distributed Intensity
\end{itemize}

\textbf{Drawbacks}
\begin{itemize}
  \item no interactions between threads
  \item contention focus, cooperation ignored
  \item CMP processors used; SMP architecture fundamentally different;
\end{itemize}

\textbf{Gains}
\begin{itemize}
  \item overall completion time as scheduler performance metric
\end{itemize}

\paragraph{ \cite{zhuravlev_addressing_2010} }
\begin{itemize}
  \item cache aware scheduler needs: classification scheme \& scheduling policy
  \item classification: cache light/heavy; compute light/heavy;
  \item scheduling: assignment of threads to cores based on their classification
  \item miss rate is good estimator of contention for shared ressources, as it
    counts the LLC-misses for CPU accesses and prefetching accesses. Hence, it
    measures load on FSB, DRAM ctr. and prefetching HW besides the cache
    contention.
  \item centralized sort by LLC-miss-rate per million instructions
  \item used 8core, dual socket opteron, with a cache layout similar to Haswell
  \item thread count <= core count
  \item running average miss rate for scheduling decisions
\end{itemize}

\textbf{Gains}
\begin{itemize}
  \item better average performance
  \item high performance improvement for individual applications
  \item low performance variance (best/worst case) between different runs -->
    stable performance
  \item factors for performance degradations: memory controller, FSB,
    prefetching HW
  \item classification scheme evaluation
  \item 8core opteron machine is cache-layout comparable to Haswell
\end{itemize}

\textbf{Drawbacks}
\begin{itemize}
  \item no cooperation between threads assumed (shared mem, communication)
  \item no overload situation, at much as many threads as cores in the system
\end{itemize}


\paragraph{ \cite{liu_last-level_2015} }
\textbf{questions} the ability of reducing the surface for cache
side-channel attacks;


\paragraph{ \cite{ousterhout_scheduling_1982} }
\textbf{constrains} multiprocessor systems from the '80s.

\textbf{relevant} coscheduling introduced;

\paragraph{ \cite{watts_practical_1998} }
\textbf{constrains} discusses load balancing in a distributed network of
machines, e.g. cluster;

\textbf{relevant} definition of static and dynamic load balancing;
five phases for dynamic load balancing: load evaluation, profitability
determination, work transfer vector calculation, task selection ,task migration


\begin{itemize}
  \item Given a collection of tasks comprising a computation and a set of
computers on which these may be executed, find a mapping of tasks to computers
that results in each computer having an approximately equal amount of work.
  \item first determine that a load imbalance exists
  \item if the cost of the imbalance exceeds the cost of load balancing then
    load balancing should be initiated
  \item work transfer vector calculation, how much work shall be transfered
  \item task selection, constrained by locality and task size, cost function to
    take this into account;
  \item task migration, state \& communication integrity must be maintained
\end{itemize}


\paragraph{ \cite{banikazemi_pam_2008} }
\textbf{relevant} Cpusets to describe hardware architecture hierarchies;
\gls{llc}-sharing, \gls{llc}-separate, power/energy-aware cpusets;
only high-level task-to-cpuset mapping -- scheduler does the rest;
multi-level optimizations;
IDEA: security cpuset;

\textbf{keep in mind} Model for $n$-CPU system;
number of sched. choices;
measure occupancy and miss ratio and CPI;
estimate performance of scheduling choices;
benchmarking process;

\textbf{shortcommings} CMP system


\paragraph{ \cite{zhang_processor_2007} }
\textbf{relevant} bottlenecks, metrics, sched policies (IPC \& memory bus
accesses);


\paragraph{Tradeoff Table}

\end{comment}
