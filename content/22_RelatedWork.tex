\section{Related Work}
\label{state:related}

\begin{comment}
  Structure for the description of related work:
    * Assumptions
    * Concept
    * Relevant contribution
    * Result
    * Deficits
\end{comment}

\todo{Sort related work by topic, then by author}

\paragraph{ \cite{sarma_smartbalance_2015} }
\citeauthor{sarma_smartbalance_2015} write about their approach to balance work
in a heterogeneous system.
They assume a system with a single ISA and several CPUs with different performance
characteristics and different hardware features.
\todo{check different hw features}
They introduce categories for each performance level and measure the
performance differences between each level.
This results in a matrix displaying the performance gain or degradation, when
a work package (e.g. thread) is migrated.
Their load balancing algorithm has three phases: sense, predict, balance.
During the sense phase the algorithm observes the CPU utilization for an epoch.
An epoch consists of configurable many clock ticks.
The sense-result is then used by the prediction phase to compute a load
expectation for the different performance levels.
Based on this forecast a balancing decision is made and enforced in the third
phase.
Their model can also select the best CPU size for a optimal performance per
joule ratio.

\textbf{ relevant } load balancing algorithm: sense, prediciton, balance,
epoch;

\textbf{ not} heterogeneous processors, gem5, big.LITTLE, performance
matrix;

\paragraph{ \cite{hofmeyr_load_2010} }

\textbf{ relevant } notion of speed of a processor: $speed = t_{exec}/t_{real}$;
sched\_setaffinity to migrate threads; PIDs of all the threads of a task;
core speed computation and coparison algorithm;
distributed balancing thread with global synchronization;
assumptions \& drawbacks of kernel level load balancing;
in oversubscibed environments balance trumps locality;


\textbf{be aware} turbo boost -> different core speeds on same CPU;
sched\_yield and sleep difference: run queue;

\textbf{not} NUMA, MPI, OpenMP;

\textbf{ shortcommings } Tigerton: No \gls{ht}, no turbo-boost, no l3 cache,
and a dual-die CMP architecture.


\paragraph{ \cite{cruz_dynamic_2014} }

\paragraph{ \cite{zhuravlev_survey_2012} }
\citeauthor{zhuravlev_survey_2012} provide a survey over scheduling techniques,
aiming at better usage of shared resources in multicore processors.
They focus on \gls{cmp}s, which share caches between at
least two cores, but not between all cores.
Techniques like hyper-threading are also taken into account.
\todo{redo this section, cmp, smp already in terminology}
Simultaneous multiprocessors (SMPs) are not in the focus of their survey.
\gls{intel} newer architectures, e.g. Haswell, is a \gls{smp} architecture, where each
physical core has dedicated L1 \& L2 caches, but the L3 cache is shared between
all cores.
Their algorithmic focus lies on contention-aware schedulers, which consists of
four building blocks: objective, prediction, decision, and enforcement.


\paragraph{ \cite{knauerhase_using_2008} }
In \citetitle{knauerhase_using_2008} \citeauthor{knauerhase_using_2008} present
a mechanism to observe \gls{llc} misses and references, retried instructions,
core cycles, and reference cycles.
These information describe the behaviour of the threads in the system.
They assume a \gls{cmp} system and address three issues: cache interference,
migrating threads across caches and fairness between threads.

To reduce interference between cache heavy and light threads, they use the
following heuristics: cache miss per cycle, behaviour in the last time quantum,
and sum of the cache weights.
The goal is to run cache heavy threads on different \gls{llc}-groups, meaning
cores that share the same \gls{llc}.
The cache miss per cycle has shown to be a good heuristic, as it also displays
the memory access and hence the load on the memory bus.
Also, behavioural history longer than the last time quantum has proven
unnecessary, as the thread behaviour changes over time and longer history
clouds the prediction.
Finally, the cache weight is the sum of the weight of each thread running on
the same \gls{llc}-group.
A threads cache weight is the number of cache entries it uses. \todo{check
that}

To decide when and which threads to migrate to another \gls{llc}-group, the
thread weight and the cache load per \gls{llc}-group is computed.
New threads are assign tho the \gls{llc}-group with the smallest cache load.
To achieve better fairness between the \gls{llc}-groups, overweight threads are
migrated between groups periodically.
In general, migrations between \gls{llc}-groups are prevented, as the migrated
thread has to repopulate the cache on the new core.

To increase the fairness between cache light and cache heavy threads,
\citeauthor{knauerhase_using_2008} propose to increase the execution time of
cache light threads, as their performance suffers from running besides cache
heavy threads.
\todo{How is performance defined?}

The authors wrote a cachebuster and spinloop thread to simulate cache heave and
cache light threads. Additionally, application from the SPEC CPU 2000 benchmark
suite were used to provide further experimental prove to their claims.

\textbf{ shortcommings } CMP system


\paragraph{ \cite{yarom_recovering_2014} }
\textbf{motivation} FLUSH+RELOAD cache side channel attack;

\paragraph{ \cite{bernstein_cache-timing_2005} }
\textbf{motivation} timing side channel attack against cache;



\paragraph{ \cite{eyerman_probabilistic_2010} }

\paragraph{ \cite{fedorova_managing_2010} }
\paragraph{ \cite{zhuravlev_addressing_2010} }
\paragraph{ \cite{liu_last-level_2015} }
\paragraph{ \cite{ousterhout_scheduling_1982} }
\paragraph{ \cite{watts_practical_1998} }

\paragraph{ \cite{banikazemi_pam_2008} }
\textbf{relevant} Cpusets to describe hardware architecture hierarchies;
\gls{llc}-sharing, \gls{llc}-separate, power/energy-aware cpusets;
only high-level task-to-cpuset mapping -- scheduler does the rest;
multi-level optimizations;
IDEA: security cpuset;

\textbf{keep in mind} Model for $n$-CPU system;
number of sched. choices;
measure occupancy and miss ratio and CPI;
estimate performance of scheduling choices;
benchmarking process;

\textbf{ shortcommings } CMP system

\paragraph{ \cite{ zhang_processor_2007 } }
\textbf{relevant} bottlenecks, metrics, sched policies (IPC \& memory bus
accesses);
