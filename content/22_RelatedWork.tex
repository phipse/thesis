\section{Related Work}
\label{state:related}

This section presents prior work and puts emphasis on the parts pertinent to
this work.
One restriction applies to this review: mechanism proposing hardware changes
are not in the focus of this work.
This restriction excludes interesting ideas to gather behavioural information,
e.g. observation of cache coherency traffic proposed in
\cite{cruz_dynamic_2014}.

\begin{comment}
  Structure for the description of related work:
    * Assumptions
    * Concept
    * Relevant contribution
    * Result
    * Deficits

  Alternative: Aspects of the thesis in related work;
\end{comment}

\begin{itemize}
  \item Assumptions: Single ISA, no heterogeneous cores, SMP, one specific CPU
    architecture
  \item Goals: Load Balancer providing isolation, stable execution times for the
    same workload, reduced congestion on shared ressources, two balancing
    strategies: Performance and low energy usage;
  \item introduce different approaches: speed balancing, symbiotic scheduling,
    congestion aware scheduling

    \begin{itemize}
      \item concicely introduce the work
      \item highlight the key aspects, goals, achievements
      \item relevant achievements I will use
      \item drawbacks of the work
    \end{itemize}
  \item symbiotic scheduling:
    \begin{itemize}
      \item technique for SMT to improve performance of threads scheduled on
	two hardware sharing SMT cores
      \item term coined by \citeauthor{snavely_symbiotic_2002} in 2000 using a
	simulator
      \item also used for CMP systems, but less shared HW (later)
      \item runs threads on two corresponding SMT cores and measures their
	performance;
      \item explain performance, throughput; how measured?
      \item sampling phase needs time, but can be done, while a default
	scheduler runs
      \item restriction on changes in workload to ease sampling phase
      \item summarize all restrictions/assumptions
      \item select thread pairs/co-schedules for smt-core-pairs with most throughput for
	all threads
      \item try to combine workloads that use different cpu hardware
	units, e.g. fp workload and integer workload; heavy cache usage, little
	cache usage
      \item Meausrements in their \citeyear{snavely_symbiotic_2000} paper use perf counters not present in
	real HW, e.g. conflics inside FP units, FPqueue full.
      \item  \citeauthor{snavely_symbiotic_2002}: Simulator, no real hardware, but priority
	sensitive (2002)
      \item priority is perceived as ``guarantee a fraction of machine
	proportional to priority'' or `` guarantee a fraction of
	single-threaded performance proportional to priority''
      \item \citetitle{banikazemi_pam_2008}: Use real CMP system: 8 core two socket IBM Blade
      \item relies on Linux scheduler as default
      \item only when better performing thread-to-core mapping is predicted, the
	default scheduler is overruled for a defined count of scheduling-cycles
      \item uses hardware performance counter evaluate and support decisions
      \item goodness metric to rate the thread-core-mapping
      \item mathematical model for the algorithm
      \item \citetitle{eyerman_revisiting_2015}: Symbiotic scheduling does only provide an average
	throughput gain of around 3%.
      \item reason: the most test case assumptions in literature are benifical
	for symbiotic scheduling, but not close to reality.
      \item First-come-first-served scheduler is close to the theoreticaly
	optimal scheduler
      \item theoretical optimal schedule knows the performance of all tasks in
	all possible combinations with all other tasks.
      \item use simulator to measure tasks on a 'reference core' and
	compute throughput and IPC values for different schedules.
      \item arrival time of tasks is critical for FCFS schedule throughput and
	hence determines, how close FCFS comes to the optimal scheduler
      \item processor utilization or empty time are better indicator of
	throughput improvement than turnaround time
      \item overload situations?
    \end{itemize}

  \item congestion aware scheduling
    \begin{itemize}
      \item goal: minimizing usage of shared hardware resources
      \item focus on shared caches, but also prefetching hardware and memory
	controller
      \item options: cache line reuse vs. miss rate
      \item miss rate expresses load on prefetching hardware and mem controller
      \item
    \end{itemize}

  \item security relevance of shared ressources
    \begin{itemize}
  \item cache side channel attacks undermining security of OpenSSL; how thread
    placement minimizes side channel surface;
    \end{itemize}

  \item what is relevant and will be used, where improves this work the
    previous (contribution)
  \end{itemize}

\paragraph{ \cite{sarma_smartbalance_2015} }
\citeauthor{sarma_smartbalance_2015} write about their approach to balance work
in a heterogeneous system.
They assume a system with a single ISA and several CPUs with different performance
characteristics and different hardware features.
\todo{check different hw features}
They introduce categories for each performance level and measure the
performance differences between each level.
This results in a matrix displaying the performance gain or degradation, when
a work package (e.g. thread) is migrated.
Their load balancing algorithm has three phases: sense, predict, balance.
During the sense phase the algorithm observes the CPU utilization for an intervall.
An intervall consists of configurable many clock ticks.
The sense-result is then used by the prediction phase to compute a load
expectation for the different performance levels.
Based on this forecast a balancing decision is made and enforced in the third
phase.
Their model can also select the best CPU size for a optimal performance per
joule ratio.

\textbf{relevant} load balancing algorithm: sense, prediciton, balance,
intervall;

\textbf{not} heterogeneous processors, gem5, big.LITTLE, performance
matrix;

\paragraph{ \cite{hofmeyr_load_2010} }

\textbf{relevant} notion of speed of a processor: $speed = t_{exec}/t_{real}$;
sched\_setaffinity to migrate threads; PIDs of all the threads of a task;
core speed computation and coparison algorithm;
distributed balancing thread with global synchronization;
assumptions \& drawbacks of kernel level load balancing;
in oversubscibed environments balance trumps locality;


\textbf{be aware} turbo boost -> different core speeds on same CPU;
sched\_yield and sleep difference: run queue;

\textbf{not} NUMA, MPI, OpenMP;

\textbf{shortcommings} Tigerton: No \gls{ht}, no turbo-boost, no l3 cache,
and a dual-die CMP architecture.


\paragraph{ \cite{zhuravlev_survey_2012} }
\citeauthor{zhuravlev_survey_2012} provide a survey over scheduling techniques,
aiming at better usage of shared resources in multicore processors.
They focus on \gls{cmp}s, which share caches between at
least two cores, but not between all cores.
Techniques like hyper-threading are also taken into account.
\todo{redo this section, cmp, smp already in terminology}
Simultaneous multiprocessors (SMPs) are not in the focus of their survey.
\gls{intel} newer architectures, e.g. Haswell, is a \gls{smp} architecture, where each
physical core has dedicated L1 \& L2 caches, but the L3 cache is shared between
all cores.
Their algorithmic focus lies on contention-aware schedulers, which consists of
four building blocks: objective, prediction, decision, and enforcement.


\paragraph{ \cite{knauerhase_using_2008} }
In \citetitle{knauerhase_using_2008} \citeauthor{knauerhase_using_2008} present
a mechanism to observe \gls{llc} misses and references, retried instructions,
core cycles, and reference cycles.
These information describe the behaviour of the threads in the system.
They assume a \gls{cmp} system and address three issues: cache interference,
migrating threads across caches and fairness between threads.

To reduce interference between cache heavy and light threads, they use the
following heuristics: cache miss per cycle, behaviour in the last time quantum,
and sum of the cache weights.
The goal is to run cache heavy threads on different \gls{llc}-groups, meaning
cores that share the same \gls{llc}.
The cache miss per cycle has shown to be a good heuristic, as it also displays
the memory access and hence the load on the memory bus.
Also, behavioural history longer than the last time quantum has proven
unnecessary, as the thread behaviour changes over time and longer history
clouds the prediction.
Finally, the cache weight is the sum of the weight of each thread running on
the same \gls{llc}-group.
A threads cache weight is the number of cache entries it uses. \todo{check
that}

To decide when and which threads to migrate to another \gls{llc}-group, the
thread weight and the cache load per \gls{llc}-group is computed.
New threads are assign tho the \gls{llc}-group with the smallest cache load.
To achieve better fairness between the \gls{llc}-groups, overweight threads are
migrated between groups periodically.
In general, migrations between \gls{llc}-groups are prevented, as the migrated
thread has to repopulate the cache on the new core.

To increase the fairness between cache light and cache heavy threads,
\citeauthor{knauerhase_using_2008} propose to increase the execution time of
cache light threads, as their performance suffers from running besides cache
heavy threads.
\todo{How is performance defined?}

The authors wrote a cachebuster and spinloop thread to simulate cache heave and
cache light threads. Additionally, application from the SPEC CPU 2000 benchmark
suite were used to provide further experimental prove to their claims.

\textbf{ shortcommings } CMP system


\paragraph{ \cite{yarom_recovering_2014} }
\textbf{motivation} FLUSH+RELOAD cache side channel attack;

\paragraph{ \cite{bernstein_cache-timing_2005} }
\textbf{motivation} timing side channel attack against cache;


\paragraph{ \cite{eyerman_revisiting_2015} }
\textbf{relevant} symbiotic scheduling on SMT systems;
list of assumptions;
definition of their experiments;
def. partially symmetric homogeneous multicores;
calculation of optimal throughput of a processor;


\paragraph{ \cite{fedorova_managing_2010} }
\textbf{assumptions:} no interaction between threads: no shared data, no
communication;

\begin{itemize}
  \item LLC miss rate of threads \textit{vs.} memory-reuse pattern
  \item more arguments for mem-reuse pattern
  \item cache sensitivit and intensity of threads --> Pain metric (offline)
  \item cache-miss and cache-access rate
  \item online metric to approximate pain metric -> approx-pain
  \item approx-pain uses perfcounters to measure LLC-miss-rate, as this showed
    to be the best predictor for sensitivity and intensity
  \item LLC-miss-rate predicts contention in other shared hardware
  \item LLC-miss-rate perf counter measures prefetching miss
  \item FURTHER: Distributed Intensity Online, Power Distributed Intensity
\end{itemize}

\textbf{Drawbacks}
\begin{itemize}
  \item no interactions between threads
  \item contention focus, cooperation ignored
  \item CMP processors used; SMP architecture fundamentally different;
\end{itemize}

\textbf{Gains}
\begin{itemize}
  \item overall completion time as scheduler performance metric
\end{itemize}

\paragraph{ \cite{zhuravlev_addressing_2010} }
\begin{itemize}
  \item cache aware scheduler needs: classification scheme \& scheduling policy
  \item classification: cache light/heavy; compute light/heavy;
  \item scheduling: assignment of threads to cores based on their classification
  \item miss rate is good estimator of contention for shared ressources, as it
    counts the LLC-misses for CPU accesses and prefetching accesses. Hence, it
    measures load on FSB, DRAM ctr. and prefetching HW besides the cache
    contention.
  \item centralized sort by LLC-miss-rate per million instructions
  \item used 8core, dual socket opteron, with a cache layout similar to Haswell
  \item thread count <= core count
  \item running average miss rate for scheduling decisions
\end{itemize}

\textbf{Gains}
\begin{itemize}
  \item better average performance
  \item high performance improvement for individual applications
  \item low performance variance (best/worst case) between different runs -->
    stable performance
  \item factors for performance degradations: memory controller, FSB,
    prefetching HW
  \item classification scheme evaluation
  \item 8core opteron machine is cache-layout comparable to Haswell
\end{itemize}

\textbf{Drawbacks}
\begin{itemize}
  \item no cooperation between threads assumed (shared mem, communication)
  \item no overload situation, at much as many threads as cores in the system
\end{itemize}


\paragraph{ \cite{liu_last-level_2015} }
\textbf{questions} the ability of reducing the surface for cache
side-channel attacks;


\paragraph{ \cite{ousterhout_scheduling_1982} }
\textbf{constrains} multiprocessor systems from the '80s.

\textbf{relevant} coscheduling introduced;

\paragraph{ \cite{watts_practical_1998} }
\textbf{constrains} discusses load balancing in a distributed network of
machines, e.g. cluster;

\textbf{relevant} definition of static and dynamic load balancing;
five phases for dynamic load balancing: load evaluation, profitability
determination, work transfer vector calculation, task selection ,task migration


\begin{itemize}
  \item Given a collection of tasks comprising a computation and a set of
computers on which these may be executed, find a mapping of tasks to computers
that results in each computer having an approximately equal amount of work.
  \item first determine that a load imbalance exists
  \item if the cost of the imbalance exceeds the cost of load balancing then
    load balancing should be initiated
  \item work transfer vector calculation, how much work shall be transfered
  \item task selection, constrained by locality and task size, cost function to
    take this into account;
  \item task migration, state \& communication integrity must be maintained
\end{itemize}


\paragraph{ \cite{banikazemi_pam_2008} }
\textbf{relevant} Cpusets to describe hardware architecture hierarchies;
\gls{llc}-sharing, \gls{llc}-separate, power/energy-aware cpusets;
only high-level task-to-cpuset mapping -- scheduler does the rest;
multi-level optimizations;
IDEA: security cpuset;

\textbf{keep in mind} Model for $n$-CPU system;
number of sched. choices;
measure occupancy and miss ratio and CPI;
estimate performance of scheduling choices;
benchmarking process;

\textbf{shortcommings} CMP system


\paragraph{ \cite{zhang_processor_2007} }
\textbf{relevant} bottlenecks, metrics, sched policies (IPC \& memory bus
accesses);


\section{Assumptions}
The mentioned research leads to the following assumptions for this work:


\begin{description}
  \item[Hardware]
  \item[Job types]
  \item[Job type distribution]
  \item[knowledge of execution time]
  \item[Energy efficency] In case of overload equal to performance algorithm

\end{description}

keywords:
online
load balancing
thread-to-core-mapping
meta-scheduler
user-land
on a $µ$-kernel

\paragraph{Tradeoff Table}
thread bahaviour history over several intervalls
\textbf{vs.}
thread behaviour only in last intervall


\paragraph{Contributions}
\begin{itemize}
  \item Current processor architectures don't use CMP processors any more, therefore
    the cache layout is different. My work evaluates the research results on the
    new HW layout. But AMD Opteron Barcelona had a similar cache layout.
  \item User level scheduling on a $µ$-kernel. But user level scheduling was
    done before.
  \item More scheduling parameters \textit{or} less assumptions about threads.
  \item No offline measurements, only online information gathering.
  \item Thread interaction possible (communication partner, security flag)
  \item Designated cores for security critical applications.
\end{itemize}
