\section{Related Work}
\label{state:related}

This section presents prior work and emphasises the parts pertinent to
this thesis.
One assumption for this work is already in place:
the target architecture is a single \gls{isa}, partly symmetric homogeneous
multi-core.
Partly symmetric means the target architecture supports \gls{smt},
but besides this, it is an \gls{smp} architecture.
Hence, research proposing hardware changes is out of scope; such as
\cite{cruz_dynamic_2014}, where cache coherence messages are monitored to
increase a hardware communication counter.
Also, research focusing on heterogeneous or multi-\gls{isa} architectures is
not part of this review; for example in \cite{sarma_smartbalance_2015}
the authors compare different core categories regarding expected performance of
the current workload.

% todo: write overview over topic, first smt, second  goodness, third
% congestion, fourth communication, fifth energy, sixth security.
This review will focus on research regarding five topics: \gls{smt},
resource congestion, communication, energy usage, and security.
A load balancer must address all these topics, to efficiently use available resources
and to make repeated execution more predictable.
\\

\paragraph{Simultaneous Multi-Threading}
\citeauthor{snavely_symbiotic_2000} introduce the term \emph{symbiotic
scheduling}, describing a scheduling technique
for \gls{smt}-systems in \cite{snavely_symbiotic_2000}.
They propose scheduling those two threads on a shared core, which provide the best
utilization of said core.
The combination of two threads on two corresponding \gls{smt}-cores is called a
co-schedule.
A co-schedule's performance is defined as the turnaround.

In their experiments they run independent threads on a simulator modeling a Compaq
Alpha 21264 out-of-order processor with hardware additions for \gls{smt}.
This simulator model includes performance counters to ``capture dynamic execution
information''(\autocite[236]{snavely_symbiotic_2000}).
To determine a good schedule, the authors compute the weighted speedup \textit{WS} for
a fixed time interval.
For each thread the change in instructions per cycle (IPC) compared to
single-threaded execution is calculated.
The sum of these values for all threads is the weighted speedup of this
schedule.
$$ WS(t) = \sum_{i=1}^n (\text{realized IPC thread}_i / \text{single-threaded IPC
thread}_i)$$
Their developed scheduler consists of two phases: sampling and symbiosis.
The sampling phase measures different thread combinations and moves to the
symbiosis phase, when enough samples were gathered.
During in the symbiosis phase, the scheduler selects the schedule, which
promises the highest weighted speedup.
As the thread mix changes, the scheduler will move back and forth between these two
phases.

A sampling phase measure describes the performance of the threads in a
co-schedule, as measured by performance counters.
The predictors use these measures of each single thread to predict the
performance of a future co-schedule.
Among others, the authors created predictors for the following:
data cache hit rate, floating point unit conflicts, and floating point queue
conflicts.
However, the best predictor in their study made a majority vote of all other
predictors to determine the best schedule.
\citeauthor{snavely_symbiotic_2000} conclude, that good thread symbiosis depends
not only on high IPC, but also for examples usage conflicts in shared hardware
resources or cache hit rates.
\\

\begin{comment}
In \citeyear{snavely_symbiotic_2002} \citeauthor{snavely_symbiotic_2002}
discuss the topic of priorities in symbiotic schedules.
In their earlier work (\cite{snavely_symbiotic_2000}) they assume same
priority jobs.
Now, they present two meanings for priority: ``(A) guarantee a fraction of
machine proportional to priority'' or ``(B) guarantee a fraction of
single-threaded performance proportional to priority''
(\autocite[70]{snavely_symbiotic_2002}).
The authors base their priority mechanisms on two values: SOLOFRAC and CS.
SOLOFRAC is the, from the priority derived fraction of the CPU time,
a job needs to be scheduled alone and CS is the fraction of the CPU time,
where no job is solo scheduled.

The 'Naive' priority mechanism schedules a job alone for its SOLOFRAC
share and co-schedules for CS cycles. Behind this stand two assumptions: first,
during co-scheduling each job is an equal share of the cycles assigned.
And second, the number of jobs does not exceed the number of \gls{smt}-cores.

The proposed 'Symb' mechanism on the other hand, can handle more jobs
than available \gls{smt}-cores and observes job behaviour. By running jobs
solo, 'Symb' determines the job's ``'natural' IPC''
(\autocite[71]{snavely_symbiotic_2002})
and then evaluates the job's performance during a phase of co-scheduling.
Based on this measurement, the SOLOFRAC portion is reduced such that the job get its
rightful share of cycles. The increase in co-scheduling time leads to increased
utilization of the system.

Other presented approaches in \cite{snavely_symbiotic_2002} require hardware
changes and are therefore out of scope.
\\
\end{comment}

% todo connection
Still, measuring performance improvements due to increased symbiosis is
difficult.
While \citeauthor{snavely_symbiotic_2000} claim a reduction in average
turnaround time by up to 17\% in \cite{snavely_symbiotic_2000},
\citeauthor{eyerman_revisiting_2015} state a mere 3\% to 6\% increase in
average throughput with an optimal symbiotic scheduler compared to a
first-come-first-serve scheduler.
The authors explain the notable difference in \cite{eyerman_revisiting_2015} as
different experimental views.
\citeauthor{snavely_symbiotic_2000} study a fully loaded system with twice
as much ready threads as SMT-cores and observe the turnaround time.
In this scenario, the time the thread spends waiting in the ready queue causes the
largest increase in turnaround time.
A slight increase in maximum throughput reduces the waiting time and
consequently leads to a significant decrease in turnaround time.
Also, reordering the threads in a shortest remaining time first manner can
reduce the average turnaround time, too.
For these reasons \citeauthor{eyerman_revisiting_2015} make the case
that turnaround time alone can be a misleading measure.

Also, maximum throughput cannot be used, because systems are not
designed to run at peak load all the time, as this leads to long queues and
long turnaround times.
If the system is not fully loaded, average throughput equals average load,
therefore, this too is a useless comparator.
Instead, they advocate to evaluate processor utilization --- understood as average
number of executing cores --- and empty periods.
If throughput improves, the processor utilization decreases and empty
periods expand, as the time to execute the current load is reduced.
Hence, using these two measures, improvements caused by the scheduler are
comparable.
\\

Although no large performance improvements can be expected, symbiotic
scheduling proves to be relevant to improve resource usage.
However, if no offline knowledge is available, it is difficult to connect performance
gains to scheduling decisions.
As modern \gls{intel} multi-cores use \gls{smt} in each physical core, symbiotic
scheduling provides an opportunity for performance gains.
Together with hardware peformance counters, it might be possible to at least
avoid bad co-schedules.
\\

\paragraph{Resource Congestion}
A different technique is presented by \citeauthor{banikazemi_pam_2008}
regarding thread-to-core mappings on a chip multiprocessor system.
In \cite{banikazemi_pam_2008} they present PAM: a performance/power-aware
meta-scheduler, working on top of the default linux scheduler.
While the default scheduler runs, PAM gathers information via performance
counters on cache occupancy, cycles per instruction, and the miss ratio of L1
and L2 caches for each thread.
If PAM can predict a better thread-to-core mapping based on this information,
it overrules the default scheduler and enforces the better mapping.
PAM uses a \emph{goodness} metric to rate the mapping of the last time interval.
If the goodness value of the PAM mapping is better than the last observed
default mapping, PAM continues to overrule the default scheduler.
If it is worse, PAM falls back to observing the default scheduler.
The goodness metric can describe performance, power or energy consumption.

Their experiments run on a dual socket machine with one quad-core per socket,
where two cores of each quad-core share an L2 cache.
The processor does not support \gls{smt}.
Hence, the performance gains stem not from symbiosis, but rather from
congestion awareness.
\\
% todo transition is a bit short

Becoming aware of and reducing cache congestion is also topic of
\cite{knauerhase_using_2008}.
Like \citeauthor{banikazemi_pam_2008}, they work on a \gls{cmp}-machine with
two separated cache groups sharing an L2 cache.
Similarly, they try to spread memory intensive tasks as much as possible,
to reduce interference in the shared cache.
The authors introduce the metric called \emph{cache weight}, which denotes the
\gls{llc} misses per cycle for the last time interval.
Their research shows that longer histories worsen the prediction, as thread
behaviour changes over time.
To achieve the best possible spread of all threads across cache groups,
\citeauthor{knauerhase_using_2008} balance the threads in the \gls{llc}-group
based on the aggregate cache weight.
New threads are assigned to the cache group with the lowest weight and only
cache light threads are migrated across group boundaries.
However, to care for workload changes, their scheduler periodically migrates an
overweight thread to the lightest \gls{llc}-group.

Furthermore, cache light threads suffer next to cache intensive ones, as they
most likely start with a cold cache each time they are scheduled.
To compensate for this, \citeauthor{knauerhase_using_2008} propose to transfer
CPU time from the cache heavy to the suffering threads.
In their experiments, this fairness compensation reduces the performance
degradation of the most suffering thread to less than 1\%.
\\

Both, \gls{cmp} and \gls{smp} systems share more resources than just the cache.
In \cite{fedorova_managing_2010}, \cite{zhuravlev_addressing_2010}, and
\cite{zhuravlev_survey_2012} \citeauthor{fedorova_managing_2010} and
\citeauthor{zhuravlev_addressing_2010} further investigate congestion on shared
resources.
Although their assumption in \cite{fedorova_managing_2010} is that memory
reuse patterns are the indicator of
choice to predict cache interference between threads, their research leads to
the conclusion that \gls{llc}-misses are a superior predictor.
Memory reuse patterns model the access and miss frequency to a memory area for
a particular application.
This analysis is to be done offline and assumes that applications with high
reuse and low miss rates do not affect other applications negatively.
But applications with low reuse and high miss rates are likely to evict cache
lines of others.

\citeauthor{zhuravlev_addressing_2010} use this analysis and develop a
metric called \emph{pain}, which consists of two parts: sensitivity and intensity.
The former describes how much an application suffers from sharing caches with
others, the latter how much an application will hurt others.
The pain application A feels, when scheduled with B, is the product of A's
suffering and B's intensity.
Vice versa for B.
The sum of both describes the expected performance degradation for these two
applications, when sharing the same \gls{llc}.

To approximate sensitivity and intensity online, they assume cache miss and
access rates to be a good measure, but it turns out, only the cache miss
rate is relevant.
In contrast to memory reuse patterns, which focus solely on the cache as shared
resource, the cache miss rate also displays the load on prefetching hardware,
memory controller and memory bus.
Load on these shared resources turnes out to be the main reason for performance
degradation in most of their benchmarks (\cite{zhuravlev_addressing_2010}).

\citeauthor{zhuravlev_addressing_2010} implemente their off- and online
derived pain metrics in two scheduling algorithms: Distributed Intensity (DI)
and Distributed Intensity Online (DIO).
DI uses the pain metric derived offline from the stack-distance profiles,
whereas DIO uses \emph{approx-pain} derived from the \gls{llc} miss rate.
Like \cite{banikazemi_pam_2008}, both schedulers are implemented in the
Linux user level and override the default scheduler.

The DI algorithm sorts the applications based on their solo cache miss rate derived
offline.
This order is used to split the application set into two halfs: above and below
average miss rate.
Each core runs one application from each half.
If the miss rate changes at runtime, the order changes, which could impair
the performance.
However, \citeauthor{zhuravlev_addressing_2010} observe only minor place
switching, which they do not expect to harm the performance.

DIO on the other hand, constantly measures the applications miss rate and is
therefore able to react to phase changes during runtime.
The algorithm collects data every $10^9$ cycles and uses a running average to
sort the applications.
Besides this it acts like DI.

\citeauthor{zhuravlev_addressing_2010} run tests on a \gls{intel}
Xeon and AMD Opteron.
The Xeon consists of eight cores on four chips sharing the L2 cache.
The Opteron eight core has two chips sharing an L3 cache each.
Leading to four cache groups in the Xeon case and two in the Opteron case.
Their tests show only small performance differences for their tested workloads
on both machines.
The authors run eight different workloads on each machine and compare the
performance improvement to the default Linux scheduler.
Of the total of 16 tests only three tests show performance differences above
two percent between DI and DIO.
In the Opteron's case DIO improves all workloads, whereas DI shows gains in
all but one.
The Xeon shows a mixed picture for DI and DIO: from 14 percent gains to four
percent loss compared to Linux's default scheduler.
According to \citeauthor{zhuravlev_addressing_2010}, the good overall
performance of the default scheduler can be accounted to the small test set and
the relatively low probability to select the worst thread combination.
They emphasise the performance gain of each individual application with DI and
DIO scheduling and the reduced variance in execution time from run to run.

The test workloads consist of independent applications.
Thus, the trade-off between placing two communicating, high miss rate
applications on the same core, or communicating across chip boundaries was no
research question.
Additionally, \gls{cmp} architectures gave way to \gls{smp} architectures,
which do not necessarily provide strictly separated cache groups.
Therefore, the performance gain must be reevaluated.
\\

\paragraph{Communication}
Up to now, all research covered assumes independent, non-interacting threads.
\citeauthor{hofmeyr_load_2010} present in \cite{hofmeyr_load_2010} a load
balancing technique explicitly assuming communicating, parallel applications.
They propose a technique called \emph{speed balancing} with a notion for fast
and slow cores.
Their concept assumes the number of threads $N$ to be greater than the number of
cores $C$.
Consequently, the number of threads per core will be either
$T = \left \lfloor{N/C}\right \rfloor$ or $T+1$.
Fast cores are running $T$ threads, slow cores $T+1$, respectively.

For their implementation, they also assume to be running alongside other
workloads on the same hardware.
Therefore, the term \emph{speed} is defined as the relation of execution time per
elapsed wall clock time.
The authors' algorithm runs one observation thread per core measuring the speed
of each thread and of the core, respectively.
The sum of all core speeds determines the global average speed, which
distinguishes fast from slow cores.
To provide equal progress opportunities, each thread needs to run on a fast
core at least once.
So, the observation thread on a fast core searches for a suitable thread on slow
core to migrate to its own core.
Additionally, the observation thread tries to migrate the least migrated thread to
avoid constant migration of the same thread.

%todo move up after 2nd sentence; rewrite paragraph
The assumed parallel application model consists of one application
spawning several worker threads, where a computation interval is followed by a
communication barrier.
Their implementation takes this kind of application as input, executes it and
distributes its worker threads.
They do neither model communication between different tasks in a client-server
notion, nor at arbitrary points in time.
Hence, reducing the time the slowest thread needs to reach the synchronization
point improves overall application performance.
\\

\paragraph{Energy efficiency}
Performance is not a one dimensional value anymore.
Performance per watt is the important measure on battery supported devices like
notebooks or smartphones, and also in datacenters, as less watts mean less
energy cost for running and cooling.
In \cite{imes_poet_2015} the authors compare a mobile \gls{intel} Haswell and a
Samsung Exynos5 Octa regarding their minimal energy consumption.
The authors show that different strategies are necessary to achieve minimal
energy consumption for each of the two processors.
In case of the \gls{intel} Haswell processor, race to idle proves to be the
strategy of choice to reduce energy consumption.
The Samsung processor on the other hand, should run constantly at low clock
speeds.

\cite{le_sueur_slow_2011} considers systems loads below 30\% and different
workloads: video playback and a web-server.
Their measurements suggest, that for x86 processors it is best to maintain low
clock speeds, as long as the system load and quality of service guarantees
allow it.
During idle time, the processor changes to deep sleep states and saves
energy.
The recovery from deep sleep states -- power up core, load data into core local
caches -- increases system load only slightly.
To conserve energy, \emph{turbo boost} -- raising the frequency
of one core above standard, if other cores are idle -- should be avoided, as
the gained frequency comes at a high energy cost.

It is interesting for this work, how a load balancer can change the assignment
of threads to cores, to reduce overall energy consumption.
In section \ref{design:energy} different models will be discussed.
\\

\paragraph{Cache side-channel attacks}
The CPU cache has proved to be a major side channel to deduce private
cryptographic keys.
Two different attacks are discussed here: FLUSH+RELOAD and PRIME+PROBE.

Three conditions are necessary for a FLUSH+RELOAD attack:
the x86 instruction to flush a specified cache line, if read access is allowed;
shared memory between attacker and victim, e.g. binary
code in shared libraries or executables;
and conditional branches in the code path of the attacked program.
\citeauthor{yarom_recovering_2014} present in \cite{yarom_recovering_2014} an
attack to recover private keys via FLUSH+RELOAD.
To that end, they flush a set of cache lines from the cache and reload it after a
waiting period.
If the cache line was present in the cache, the time it takes to read the data
is small; reading the data from memory is significantly slower.

The attack uses the fact that the OpenSSL implementation of the Montgomery
ladder\footnote{fix published 05 Jun 2014, see
http://www.openssl.org/news/secadv/20140605.txt}
executes a different branch, depending on the bit value of the current
bit in the private key.
If the data containing the \textit{then} branch is present in the cache, the
tested bit was one, if the \textit{else} branch data is present, the bit was
zero.
The length of the waiting period -- between flushing and reloading the chache
-- presents a trade-off between missing subsequent accesses, if the period was
too long, or observing single accesses as duplicates of themselves.
In \cite{yarom_recovering_2014} the private key was recovered using this
technique.
Other implementations of the Montgomery ladder, e.g. in Network and
Cryptographic Library (NaCL)\footnote{https://nacl.cr.yp.to} by
\citeauthor{bernstein_security_2012}
do not rely on branching and are therefore not susceptible to this attack.

However, shared memory with the target application is necessary.
Since no data is shared between virtual machines FLUSH+RELOAD is impossible.
The PRIME+PROBE attack on the other hand works without this requirement.
This attack writes known data to a set of cache lines, the eviction set,
and accesses the same data after a waiting period.
If a cache line was used by other processes the data is gone and it takes
longer to reload it from memory.

The attack utilises the inclusiveness property of the CPU cache, to observe
changes in core local L1 and L2 caches through the \gls{llc}.
In inclusive caches each cache line of the L1 caches is present in L2 and each
line present in L2 is also present in the L3 cache, as displayed in figure
\ref{state:fig:inclusive}.
%
\begin{figure}[h]
  \setcapindent*{1em}
  \begin{captionbeside}[]{Caches of \gls{intel} Haswell generation processores are inclusive.
Each core contains 32KB D-, 32KB I-, 256KB L2, and 1.5MB L3 cache}[r]%
  \includestandalone[mode=tex, width=0.65\textwidth]{images/inclusive_caches}
  \end{captionbeside}
  \label{state:fig:inclusive}
\end{figure}

Therefore, the eviction set must cover the cache lines in the \gls{llc} that
correspond to the core local caches.
Hence, the core running the application under attack must be known.

Now, after the right set of cache lines is found, the access frequency can be
observed.
The access to the cache set reveals squaring operations in the algorithm.
Based on the time passed between two squaring operations, the attacker deduces
a one or zero bit.
The execution path in case of a one bit is longer, hence more time passed.
Two attacks are presented in \cite{liu_last-level_2015} and
\cite{inci_seriously_2015}, both recovering a private key of an attacked
virtual machine.
\citeauthor{inci_seriously_2015} test their attack on Amazon EC2 instances,
proving its practicality in a real world scenario.

Separating security critical applications on architectures with shared
\gls{llc} raises the difficulty of an attack, but does not mitigate it.
Only a physical separation of the caches prevents a PRIME+PROBE attack.


% -----------------------------------------------------------------------------


\begin{comment}
\begin{itemize}
  \item Assumptions: single ISA, no heterogeneous cores, SMP, one specific CPU
    architecture \checkmark
  \item Goals: load balancer providing isolation, stable execution times for the
    same workload, reduced congestion on shared resources, two balancing
    strategies: performance and low energy usage;
  \item introduce different approaches: speed balancing, symbiotic scheduling,
    congestion aware scheduling

    \begin{itemize}
      \item concisely introduce the work
      \item highlight the key aspects, goals, achievements
      \item relevant achievements I will use
      \item drawbacks of the work
    \end{itemize}
  \item symbiotic scheduling:
    \begin{itemize}
      \item term coined by \citeauthor{snavely_symbiotic_2000} in
	\citeyear{snavely_symbiotic_2000} \checkmark
      \item technique for SMT to improve performance of threads scheduled on
	two hardware sharing SMT cores \checkmark
      \item runs threads on two corresponding SMT cores and measures their
	performance in a simulator;
      \item explain performance, turnaround/response time. How measured?
	\checkmark
      \item sampling phase needs time, but can be done, while a default
	scheduler runs \checkmark
      \item restriction on changes in workload to ease sampling phase
      \item summarize all restrictions/assumptions
      \item select thread pairs/co-schedules for smt-core-pairs with most throughput for
	all threads
      \item try to combine workloads that use different cpu hardware
	units, e.g. fp workload and integer workload; heavy cache usage, little
	cache usage
      \item Measurements in their \citeyear{snavely_symbiotic_2000} paper use
	perf counters not present in real HW, e.g. conflicts inside FP units,
	FPqueue full.

      \item  \citeauthor{snavely_symbiotic_2002}: simulator, no real hardware,
	but priority sensitive (2002) \checkmark
      \item priority is perceived as ``guarantee a fraction of machine
	proportional to priority'' or `` guarantee a fraction of
	single-threaded performance proportional to priority'' \checkmark
      \item can it also be seen as a guarantee, that no lower priority thread is
	scheduled unless I have been selected to run? Can I guarantee that in a
	multicore setup, without knowledge of the execution times? -> No.
	\todo{is an example beneficial here?}
	I can just guarantee, that the plan based on the prediction cares for
	priorities.

      \item \citetitle{eyerman_revisiting_2015}: symbiotic scheduling does only
	provide an average throughput gain of around 3%.
      \item reason: the most test case assumptions in literature are beneficial
	for symbiotic scheduling, but not close to reality.
      \item gather all information for a optimal scheduler with total knowledge
      \item theoretical optimal schedule knows the performance of all tasks in
	all possible combinations with all other tasks.
      \item use simulator to measure tasks on a 'reference core' and
	compute throughput and IPC values for different schedules.
      \item First-come-first-served scheduler is close to the theoretically
	optimal scheduler
      \item arrival time of tasks is critical for FCFS schedule throughput and
	hence determines, how close FCFS comes to the optimal scheduler
      \item difference in experiments: turnaround time reduction in latency
	experiment(\cite{snavely_symbiotic_2000}) vs. average throughput in a
	maximum throughput experiment (\cite{eyerman_revisiting_2015}).
      \item processor utilization or empty time are better indicator of
	scheduler efficiency than only presenting turnaround time, best use all
      \item compare scheduler by comparing these three indicators
      \item overload situations := job arrival > max throughput
      \item besides FCFS scheduler all other need an offline phase to measure
	the jobs, to be able to determine an optimal co-schedule.
    \end{itemize}

  \item congestion aware scheduling
    \begin{itemize}
      \item goal: minimizing usage of shared hardware resources
      \item shared hw resources are: shared caches, prefetching hardware, bus
	and memory controller
      \item options: cache line reuse / memory reuse pattern vs. miss rate
      \item \citeauthor{fedorova_managing_2010} started out with cache line
	reuse and came to the conclusion, that the offline measurement of
	memory reuse pattern and conclude, that this is not the best approach
	to approximate congestion.
      \item they develop a PAIN metric, combining sensitivity and intensity
	concerning shared cache usage: sensitivity: how much do I suffer by
	shared cache; intensity: how much do I hurt others;
      \item offline measurement possible, but what is a good online
	approximation? Cache-miss rate and cache-access rate? (Intuitively
	correlate with intensity and reuse frequency)
      \item evaluation shows best online approximation for pain metric is LLC
	miss rate: combines sensitivity and intensity
      \item miss rate expresses load on FSB, prefetching hardware and
	mem controller, which are the main cause for performance degradation,
	not the cache. (what is PAMs cache occupancy ratio worth now?)
      \item ``An application aggressively using prefetching hardware will also
	typically have a high LLC miss rate, because prefetching requests for
	data that is not in the cahce are counted as cache misses. Therefore, a
	high miss rate is also an indicator of the heavy use of prefetching
	hardware.'' (\cite{fedorova_managing_2010}).

      \item \cite{knauerhase_using_2008} use LLC-misses as predictor
	for cache interference and attempt to keep the sum of cache
	weights/LLC-misses of all running processes close to a medium value
      \item they also use task behaviour of the last time quantum as predictor
	for the behaviour in the next; temporal locality predictor.
      \item migration across cache borders: avoid migrating overweight/ high
	cache impact tasks, try to balance cache groups by migrating small
	tasks (assuming CMP architecture). How high is the impact, if LLC is
	shared anyway?
      \item periodically, migrate overweight task to find a possibly better
	load distribution
      \item fairness: give cache light tasks, that suffer a lot due to being
	scheduled next to cache intensive tasks, bonus execution time.
      \item assumed independent jobs

      \item \citetitle{banikazemi_pam_2008}: use real CMP system: 8 core two
	socket IBM Blade
      \item use symbiotic approach on CMP systems, less shared HW, no SMT
      \item relies on Linux scheduler as default
      \item only when better performing thread-to-core mapping is predicted, the
	default scheduler is overruled for a defined count of scheduling-cycles
      \item uses hardware performance counter evaluate and support decisions
      \item goodness metric to rate the thread-core-mapping if not better than
	last goodness rating of default scheduler, fall back to default
	scheduler
      \item mathematical model for the algorithm
      \item measured parameters are cache occupancy ratio, cycles per
	instruction, L1	and L2 miss ratio
      \item compute performance estimation for selected plan
    \end{itemize}

  \item security relevance of shared resources
    \begin{itemize}
  \item cache side channel attacks undermining security of OpenSSL; how thread
    placement minimizes side channel surface;
    \end{itemize}

  \item what is relevant and will be used, where improves this work the
    previous (contribution)
  \end{itemize}

\paragraph{ \cite{sarma_smartbalance_2015} }
\citeauthor{sarma_smartbalance_2015} write about their approach to balance work
in a heterogeneous system.
They assume a system with a single ISA and several CPUs with different performance
characteristics and different hardware features.
\todo{check different hw features}
They introduce categories for each performance level and measure the
performance differences between each level.
This results in a matrix displaying the performance gain or degradation, when
a work package (e.g. thread) is migrated.
Their load balancing algorithm has three phases: sense, predict, balance.
During the sense phase the algorithm observes the CPU utilization for an intervall.
An intervall consists of configurable many clock ticks.
The sense-result is then used by the prediction phase to compute a load
expectation for the different performance levels.
Based on this forecast a balancing decision is made and enforced in the third
phase.
Their model can also select the best CPU size for a optimal performance per
joule ratio.

\textbf{relevant} load balancing algorithm: sense, prediciton, balance,
intervall;

\textbf{not} heterogeneous processors, gem5, big.LITTLE, performance
matrix;

\paragraph{ \cite{hofmeyr_load_2010} }
evenly distributed run\_queues are assumed, slow cores are cores with longer
queues than others.

\textbf{relevant} notion of speed of a processor: $speed = t_{exec}/t_{real}$;
sched\_setaffinity to migrate threads; PIDs of all the threads of a task;
core speed computation and comparison algorithm;
distributed balancing thread with global synchronization;
assumptions \& drawbacks of kernel level load balancing;
in oversubscibed environments balance trumps locality;
\textbf{assume interacting tasks}


\textbf{be aware} turbo boost -> different core speeds on same CPU;
sched\_yield and sleep difference: run queue;

\textbf{not} NUMA, MPI, OpenMP;

\textbf{shortcommings} Tigerton: no \gls{ht}, no turbo-boost, no l3 cache,
and a dual-die CMP architecture.


\paragraph{ \cite{zhuravlev_survey_2012} }
\citeauthor{zhuravlev_survey_2012} provide a survey over scheduling techniques,
aiming at better usage of shared resources in multicore processors.
They focus on \gls{cmp}s, which share caches between at
least two cores, but not between all cores.
Techniques like hyper-threading are also taken into account.
\todo{redo this section, cmp, smp already in terminology}
Simultaneous multiprocessors (SMPs) are not in the focus of their survey.
\gls{intel} newer architectures, e.g. Haswell, is a \gls{smp} architecture, where each
physical core has dedicated L1 \& L2 caches, but the L3 cache is shared between
all cores.
Their algorithmic focus lies on contention-aware schedulers, which consists of
four building blocks: objective, prediction, decision, and enforcement.


\paragraph{ \cite{knauerhase_using_2008} }
In \citetitle{knauerhase_using_2008} \citeauthor{knauerhase_using_2008} present
a mechanism to observe \gls{llc} misses and references, retried instructions,
core cycles, and reference cycles.
These information describe the behaviour of the threads in the system.
They assume a \gls{cmp} system and address three issues: cache interference,
migrating threads across caches and fairness between threads.

To reduce interference between cache heavy and light threads, they use the
following heuristics: cache miss per cycle, behaviour in the last time quantum,
and sum of the cache weights.
The goal is to run cache heavy threads on different \gls{llc}-groups, meaning
cores that share the same \gls{llc}.
The cache miss per cycle has shown to be a good heuristic, as it also displays
the memory access and, hence, the load on the memory bus.
Also, behavioural history longer than the last time quantum has proven
unnecessary, as the thread behaviour changes over time and longer history
clouds the prediction.
Finally, the cache weight is the sum of the weight of each thread running on
the same \gls{llc}-group.
A threads cache weight is the number of cache entries it uses. \todo{check
that}

To decide when and which threads to migrate to another \gls{llc}-group, the
thread weight and the cache load per \gls{llc}-group is computed.
New threads are assign tho the \gls{llc}-group with the smallest cache load.
To achieve better fairness between the \gls{llc}-groups, overweight threads are
migrated between groups periodically.
In general, migrations between \gls{llc}-groups are prevented, as the migrated
thread has to repopulate the cache on the new core.

To increase the fairness between cache light and cache heavy threads,
\citeauthor{knauerhase_using_2008} propose to increase the execution time of
cache light threads, as their performance suffers from running besides cache
heavy threads.
\todo{How is performance defined?}

The authors wrote a cachebuster and spinloop thread to simulate cache heave and
cache light threads. Additionally, application from the SPEC CPU 2000 benchmark
suite were used to provide further experimental prove to their claims.

\textbf{ shortcommings } CMP system


\paragraph{ \cite{yarom_recovering_2014} }
\textbf{motivation} FLUSH+RELOAD cache side channel attack;

\paragraph{ \cite{bernstein_cache-timing_2005} }
\textbf{motivation} timing side channel attack against cache;


\paragraph{ \cite{eyerman_revisiting_2015} }
\textbf{relevant} symbiotic scheduling on SMT systems;
list of assumptions;
definition of their experiments;
def. partially symmetric homogeneous multicores;
calculation of optimal throughput of a processor;


\paragraph{ \cite{fedorova_managing_2010} }
\textbf{assumptions:} no interaction between threads: no shared data, no
communication;

\begin{itemize}
  \item LLC miss rate of threads \textit{vs.} memory-reuse pattern
  \item more arguments for mem-reuse pattern
  \item cache sensitivit and intensity of threads --> Pain metric (offline)
  \item cache-miss and cache-access rate
  \item online metric to approximate pain metric -> approx-pain
  \item approx-pain uses perfcounters to measure LLC-miss-rate, as this showed
    to be the best predictor for sensitivity and intensity
  \item LLC-miss-rate predicts contention in other shared hardware
  \item LLC-miss-rate perf counter measures prefetching miss
  \item FURTHER: Distributed Intensity Online, Power Distributed Intensity
\end{itemize}

\textbf{Drawbacks}
\begin{itemize}
  \item no interactions between threads
  \item contention focus, cooperation ignored
  \item CMP processors used; SMP architecture fundamentally different;
\end{itemize}

\textbf{Gains}
\begin{itemize}
  \item overall completion time as scheduler performance metric
\end{itemize}

\paragraph{ \cite{zhuravlev_addressing_2010} }
\begin{itemize}
  \item cache aware scheduler needs: classification scheme \& scheduling policy
  \item classification: cache light/heavy; compute light/heavy;
  \item scheduling: assignment of threads to cores based on their classification
  \item miss rate is good estimator of contention for shared ressources, as it
    counts the LLC-misses for CPU accesses and prefetching accesses. Hence, it
    measures load on FSB, DRAM ctr. and prefetching HW besides the cache
    contention.
  \item centralized sort by LLC-miss-rate per million instructions
  \item used 8core, dual socket opteron, with a cache layout similar to Haswell
  \item thread count <= core count
  \item running average miss rate for scheduling decisions
\end{itemize}

\textbf{Gains}
\begin{itemize}
  \item better average performance
  \item high performance improvement for individual applications
  \item low performance variance (best/worst case) between different runs -->
    stable performance
  \item factors for performance degradations: memory controller, FSB,
    prefetching HW
  \item classification scheme evaluation
  \item 8core opteron machine is cache-layout comparable to Haswell
\end{itemize}

\textbf{Drawbacks}
\begin{itemize}
  \item no cooperation between threads assumed (shared mem, communication)
  \item no overload situation, at much as many threads as cores in the system
\end{itemize}


\paragraph{ \cite{liu_last-level_2015} }
\textbf{questions} the ability of reducing the surface for cache
side-channel attacks;


\paragraph{ \cite{ousterhout_scheduling_1982} }
\textbf{constrains} multiprocessor systems from the '80s.

\textbf{relevant} coscheduling introduced;

\paragraph{ \cite{watts_practical_1998} }
\textbf{constrains} discusses load balancing in a distributed network of
machines, e.g. cluster;

\textbf{relevant} definition of static and dynamic load balancing;
five phases for dynamic load balancing: load evaluation, profitability
determination, work transfer vector calculation, task selection ,task migration


\begin{itemize}
  \item Given a collection of tasks comprising a computation and a set of
computers on which these may be executed, find a mapping of tasks to computers
that results in each computer having an approximately equal amount of work.
  \item first determine that a load imbalance exists
  \item if the cost of the imbalance exceeds the cost of load balancing then
    load balancing should be initiated
  \item work transfer vector calculation, how much work shall be transfered
  \item task selection, constrained by locality and task size, cost function to
    take this into account;
  \item task migration, state \& communication integrity must be maintained
\end{itemize}


\paragraph{ \cite{banikazemi_pam_2008} }
\textbf{relevant} Cpusets to describe hardware architecture hierarchies;
\gls{llc}-sharing, \gls{llc}-separate, power/energy-aware cpusets;
only high-level task-to-cpuset mapping -- scheduler does the rest;
multi-level optimizations;
IDEA: security cpuset;

\textbf{keep in mind} Model for $n$-CPU system;
number of sched. choices;
measure occupancy and miss ratio and CPI;
estimate performance of scheduling choices;
benchmarking process;

\textbf{shortcommings} CMP system


\paragraph{ \cite{zhang_processor_2007} }
\textbf{relevant} bottlenecks, metrics, sched policies (IPC \& memory bus
accesses);


\paragraph{Tradeoff Table}

\end{comment}
