% vim:set ft=tex:
\section{Communication}
\label{design:comm}

A less attended behaviour property of threads is communication.
It is expensive to observe which threads are communicating with each other and
how often.
This section explores kernel and user-land approaches to acquire this
information statically and dynamically.


\paragraph{Communication patterns}
I present two patterns of communication: compute-communicate and client-server.

\citeauthor{hofmeyr_load_2010} (\cite{hofmeyr_load_2010}) assumed a program running several threads, which
compute a chunk of work and then communicate their results to each other.
This cycle repeats until all work is executed, whereas the time spent executing
is the major part of the cycle and only a small part is used for communication.
Nevertheless, the overall program performance is best, if each thread reaches
the communication synchronisation at the same time to spend as little time as
possible waiting for others.
\cite{hofmeyr_load_2010} notes this connection.
The compute-communicate pattern is typical for distributed systems and is named
distributed communication in the following.

The other pattern describes client-server communication.
To achieve the best possible performance, the communication latency, meaning
the time difference between sending and receiving the message, must be minimal.
The latency is minimal if both communication partners are on the same core.
Additionally, time slice donation, where the server executes on the time slice
of the client, is only possible in Fiasco.OC if both execute on the same core.
Furthermore, the server will not execute unless it has a client request, so a
server typically has long idle times.
Therefore, it is preferable to execute both communication partners on the same
core.

But this conclusion is wrong, if there are several clients from the same task.
If the task runs several threads, then it most likely wants these threads to
execute in parallel.
Running all threads close to the server on the same core would heavily impact
their performance.
In the worst case, the performance drops below single threaded performance, due
to context switch overhead.

The same argumentation applies, if the server has a high number of clients from
different tasks.
Executing all clients on the server's core would be a waste of resources.
Hence, the distribution of the task's threads to different cores takes priority
over low communication latencies.


\subsection{Kernel}
Naturally, acquiring communication information at kernel level is easier, as
more information context is available.
But access to kernel information from the user land must be pondered carefully.
As the following two approaches rely on global object ids, which are a debugging
feature, I consider the approaches debugging features, too.
% todo improve debugging feature argument. because of global ids are df,


\paragraph{Communication Matrix}
Communication partners as well as the rate of communication during a time
frame, are of interest.
It is important to reduce the latency of intense connections.
To model the communication relationships, each thread is assigned a global
identifier and a matrix is created, where each field contains a counter,
counting each communication event, between the two corresponding global IDs.
This matrix is symmetric, hence to save memory, only the upper half is used
and communication between two threads uses the counter in field
(lowerID, higherID).
The upper bound for the memory requirement is $1/2 * (\#threads)^2$.
As not all threads will communicate with each other, usage of a sparse matrix
will further lower the memory requirement.
%A sparse matrix allocates only memory for fields, which are actually used.

\paragraph{Map}
Another approach can be a thread local map, which contains the global
identifier of the communication partner and a counter.
Each communication attempt increases the counter by one.
Similar to the sparse matrix, this approach only allocates memory for used
counters.
Also, the used memory is directly accounted to the threads which need it.
However, as this is a thread local solution, both communication partners will
maintain their separate counters.
\\

The user-land load balancer can use the system call interface to query
and reset these counters.
The answer can contain an array of all counters for this thread, to reduce the
system call overhead.
In case of the matrix, the answer must be created from the thread ID column and
the thread ID row.


\subsection{User land}
At user level, information gathering is much more complicated.
The strong isolation guarantees make it infeasible to work with global
identifiers across all threads.
But it is already possible to ask the kernel if two capabilities point to the
same object.

\paragraph{Cooperative communication capabilities}
To communicate with a partner, a thread needs a capability in its own
capability table, pointing to a channel to the receiver.
Together with a counter value, this capability of the \gls{ipc}-gate can be
passed to the load balancer, which can then identify communication partners by
comparing this received capability with the capabilities of the threads
it manages.
The load balancer can then build a relationship graph between threads and adapt
the thread-to-core assignment.

This approach requires changes to the scheduler interface in L4Re and
thread cooperation.
But it has the benefit that each thread can decide on its own if the
communication relationship is intense enough to benefit from increased
locality.
Unfortunately, this requires changes to each thread managed by the balancer and
regular intensity updates.
Together with the interface changes, this dynamic approach is deemed infeasible
for this project.

\paragraph{Configuration parameter}
Less invasive, but static, is the usage of task specific configuration parameters.
The two communication patterns yield two configuration parameters to represent the
different behaviours.
Together with an identifier thread groups can be configured.
This extends the set of isolation parameters for the thread configuration
introduced in \ref{design:isolation}.

This approach requires careful system design, but no change of program source
code or thread cooperation.
\\

The two communication patterns are named \texttt{distr} and \texttt{clsvr}.
A group is identified by \texttt{<name>} and either distributed or clustered.
To identify the server in the client-server relationship, the server task's
group name is assigned the \texttt{\_S} postfix.
Client tasks use the \texttt{\_C} postfix.

\begin{lstlisting}
  distr = <Name>
  clsvr = <Name>_{C|S}
\end{lstlisting}
