% vim:set ft=tex:
\section{Communication}
\label{design:comm}

\newacronym{bsp}{BSP}{bulk synchronous parallel programming}

A less attended behaviour property of threads is communication.
It is runtime-wise expensive to observe which threads are communicating with
each other and how often.
In the following I explore kernel and user-land approaches to acquire this
information statically and dynamically.


\paragraph{Communication Patterns.}
I present two patterns of communication: compute-communicate and client-server.

\citeauthor{hofmeyr_load_2010} (\cite{hofmeyr_load_2010}) assume a program running several threads, which
compute a chunk of work and then communicate their results to each other.
This cycle repeats until all work items are processed, whereas the time spent executing
is the major part of the cycle and only a small part is used for communication.
Nevertheless, the overall program performance is best, if each thread reaches
the communication synchronisation at the same time to spend as little time as
possible waiting for others.
\cite{hofmeyr_load_2010} notes this connection.
This so called compute-communicate pattern is typical for distributed systems and is
called \gls{bsp} in high performance computing (\cite{mccoll_scalability_1996}).

The other pattern is called client-server communication.
To achieve the best possible performance, the communication latency, meaning
the time difference between sending and receiving a message, must be minimal.
The latency is minimal if both communication partners are on the same core.
Additionally, time slice donation, where the server executes on the time slice
of the client, is only possible in Fiasco.OC if both threads execute on the same core.
Furthermore, the server will not execute unless it has a client request, so a
server typically has long idle times.
Therefore, it is preferable to execute both communication partners on the same
core.

However, this conclusion is wrong, if the server has several clients from the
same task.
If the task runs several threads, then it most likely wants these threads to
execute in parallel.
Running all threads close to the server on the same core would contradict this
intention and heavily impact their performance.
In the worst case, the performance drops below single threaded performance, due
to the overhead of context switching.

The same argumentation applies, if the server has a high number of clients from
different tasks.
Executing all clients on the server's core would be a waste of resources.
Hence, the distribution of the task's threads to different cores takes priority
over low communication latencies.


\subsection{Kernel}
Naturally, acquiring communication information at kernel level is easier than
at user level, as more information context is available.
However, access to kernel information from the user land must be pondered carefully.
As the following two approaches rely on global object ids, which are a debugging
feature, I consider them debugging features as well.


\paragraph{Communication Matrix.}
Communication partners as well as the rate of communication during a time
frame, are of interest.
It is important to reduce the latency of connections with a high message
exchange rate.
To model the communication relationships, each thread is assigned a global
identifier and a matrix is created, where each field contains a counter,
counting each communication event between the two corresponding global IDs.
This matrix is symmetric, hence, to save memory, only the upper half is used
and communication between two threads uses the counter in field
(lowerID, higherID).
The upper bound for the memory requirement is $\sfrac{1}{2} * |\textit{threads}|^2$.
As not all threads will communicate with each other, usage of a sparse matrix
will further lower the memory requirement.
%A sparse matrix allocates only memory for fields, which are actually used.

\paragraph{Map.}
Another approach can be a thread local map, which contains the global
identifier of the communication partner and a counter.
Each communication attempt increases the counter by one.
Furthermore, the used memory is directly accounted to the threads which need it.
However, as this is a thread local solution, both communication partners will
maintain their separate counters.

The upper bound for the memory requirement is $|\textit{threads}|^2$, as each thread
needs to maintain its own set of counters.
As with the sparse matrix, only used counters are allocated to reduce the
actual memory usage.
\\

The user-land load balancer can use the system call interface to query
and reset these counters.
The response contains an array of all counters for this thread, to reduce the
system call overhead.
%In case of the matrix, the answer must be created from the thread ID column and
%the thread ID row.


\subsection{User Land}
At user level, information gathering is more complicated compared to
kernel level.
The strong isolation guarantees of Fiasco.OC make it infeasible to work with global
identifiers across all threads.
However, it is possible to ask the kernel if two capabilities point to the
same object.

\paragraph{Cooperative Communication Capabilities.}
To communicate with a partner, a thread needs a capability in its own
capability table, pointing to a channel to the receiver.
A communication channel to another task is called an \gls{ipc}-gate.
Together with a counter value, this capability of the \gls{ipc}-gate can be
passed to the load balancer, which can then identify communication partners by
comparing this received capability with the capabilities of the threads
it manages.
The load balancer can then build a relationship graph between threads and adapt
the thread-to-core assignment.

This approach requires changes to the scheduler interface in L4Re and
thread cooperation.
However, it has the benefit that each thread can decide on its own if the
amount of exchanged messages is high enough to benefit from increased
locality.
Unfortunately, this requires changes to each thread managed by the balancer and
regular updates regarding the amount of messages exchanged.
Together with the interface changes, this dynamic approach is deemed infeasible
for this project.

\paragraph{Configuration Parameter.}
Less invasive, but static, is the usage of task specific configuration parameters.
The two communication patterns I introduced at the beginning of this section
yield two configuration parameters to represent the different behaviours.
Together with an identifier they define thread groups.
The load balancer then decides depending on the type of the thread group, if
the threads should be placed close together on the same core, or distributed to
as many cores as possible.

This approach requires careful system design, but no change of program source
code or thread cooperation.
