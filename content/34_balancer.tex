% vim:set ft=tex:
\section{Load Balancer}
\label{design:threadmapper}

All the techniques introduced in the previous sections of this chapter
are now united into the central load balancing component.
The section describes the components the load balancer is build of and relies
upon.
\todo{chapter structure}


\paragraph{Load Balancer \& Task Proxies}
To achieve good load balancing performance, the load balancer needs information
about the underlying hardware, as well as the amount of work in the system.
Thereby is it also possible to build hierarchical load balancing, where
only a partial view on the hardware and only a subset of tasks runs on this
specific load balancer.
However, while leading to isolation between different subsets of the system,
the overall performance can suffer, as the system is effectively partitioned.
A surplus of computing resources in one partition is unusable by another
partition leading to wasted resources.
The partition size can only be adapted at core granularity, so a precise
resource accounting may not be possible.

Isolation between parts of the system can also be provided by a central load
balancing component.
A central load balancer as well as a hierarchical approach is part of the chain
of trust of each balanced application, so both approaches are equally
trustworthy.
But the central component can react dynamically to changes in the workloads and
shift the partition borders dynamically.
Also if tasks with security properties, as discussed in section
\ref{design:isolation}, enter or leave the system, the dynamic approach can
repurpose a core.
Redistribution of cores between hierarchical load balancer would require
additional reasoning which load balancer the free core is assigned to, or which
load balancer must release a core.
Either the load balancer must communicate their reasoning between each other or
with a central instance further up the hierarchy.
The former weakens the isolation and increases overhead, the latter means more
logic in a central instance.

To decrease the overhead and due to the easier implementation a central load
balancing component is chosen.

Next to the load balancer task specific proxy scheduler act as information
source.
Each task can have isolation (\ref{design:isolation}) and communication
properties (\ref{design:comm}).
The task proxy stores this task specific configuration and provides it to the
load balancer.

A factory creates the load balancer as well as the task proxies and creates
from the configuration parameters the configuration structure exchanged between
proxies and balancer.
The configuration structure comprises of the priority band and a list of
the group types and identifiers.


\paragraph{Thread Management}
The main management component inside the load balancer is the thread
management.
It creates a internal administration class for each thread and identifies the
thread via its capability.
Using the configuration passed, the thread manager builds the thread groups,
defined in the Ned configuration.
It answeres existance requests for threads by capability and also cleans up
threads, which left the system.
It also provides a consistent view on all managed threads for other components.


\paragraph{CPU Topology \& Accounting}
Besides the management of threads, the load balancer must also know the load on
each core and the topology of the underlying hardware.
The topology is generated via the topology enumeration capability of CPUID.
Each socket is described as a package consisting of physical cores.
If \gls{ht} is present and enabled, each physical core contains two logical
cores.

Accounting consists of a physical core and list of thread scheduled on this
core.
From this list it computes the cores load, represented as instruction
per cycle, \gls{llc}-weight, or thread count.


\paragraph{Information Gathering \& Balancing}
% each interval measure predict decide enforce cycle
% measure: for each thread, gather performance counter values and execution
  % time from kernel and store in thread administration class.
% prediction: compute a performance forecast for the next interval
% decide: select a thread to core assignment based on the prediction for the
  % next interval.
% enforce: configure the core affinity for each thread and inform the kernel
The logic of the load balancer is partitioned into four consecutive modules:
measurement, prediction, decision, and enforcement.
Each balancing interval they run once and adapt the current thread to core
assignment to the current circumstances.

First the information about the current situation is updated by gathering
performance counter and execution time values from the last interval for each
thread.
Based on these measurements, the prediction module generates a forecast for
each thread's resource needs.
This forecast forms the input for the decision module.
Together with the topology information and the accounting values for each core
from last interval, the decision module checks if the system is still balanced.
If not, selected threads must be migrated to establish balance again.
After deciding which threads must be migrated, the enforce module applies the
migrations by modifying each thread's affinity and informs the kernel.

These steps repeat every interval. A high interval duration reduces the
load balancing overhead, but also leaves the system longer in a bad state, when
thread behaviour or the workload changes.
A short interval duration has the inverse effect.
The ideal interval size increases system performance to outweigh the interval's
balancing overhead.



\begin{comment}

\subsection{Hardware performance monitoring}

\subsection{Core Accounting}
Besides the structure of topology data, threads, thread-groups and the
different thread to core assignments need to be modeled.


\paragraph{Topology description}
Represents the cache and core layout of the CPU.
On Haswell it consist of a package description containing core descriptions.
A core description consists of the hardware assigned APIC\_id, the kernel
assigned fiasco\_id, and the SMT abstraction.
It can be expanded to include multi-socket systems by adding more package
descriptions.
It maps the fiasco view on the cores onto the actual HW topology and uses CPUID
to determine corresponding logical cores.

\paragraph{Thread\_t}
is the administrative representation of a L4-thread.
It contains not only of the thread parameters passed via run\_thread(), but
also measurements for the current and last interval.
LLC-misses, execution time, an identifier, and the cores it currently runs on
and will run on in the next interval.

\paragraph{Thread Groups}


% ----------------------------------------------------------------------------

\subsection{Analyse}
  \paragraph{Hardware}
    * Use dedicated hardware --> hardware analysis future work \\
    * Haswell: per core: L1 \& L2 cache; shared L3 cache between all cores

  \paragraph{Measurement}
    * Measure the load on each core during a time interval \textbf{TI}; \\
    * measure LLC misses during last \textbf{TI};


% ----------------------------------------------------------------------------

\subsection{Predict}

  * Predict load and LLC misses of each thread for the next time intervall; \\


% ----------------------------------------------------------------------------

\subsection{Decide}

  * Decide on a thread distribution to cores, based on predictions;

  \subsubsection{Placement Generator}

  \paragraph{Pseudo-code of placement algorithm}
  \begin{verbatim}
  from all threads:
    select #core highest LLC miss rate
    select #core highest exec-time
    intersection of both are critical threads
    if threads placed on different cores
      then do nothing
    else
      move higher LLC miss rate thread to other core
    do accounting

  forall threads left do:
    bin by priority levels
    sort each bin by miss rate

  forall prio-bin in prio-bin-list do:
    while threads in prio-bin
      dequeue highest miss rate
      sort cores by lowest accounted miss rate
      place max(#core, #threads left in bin) threads RR on cores;
  \end{verbatim}

  \paragraph{\gls{smt} abstraction code}
  \begin{verbatim}
  if SMT is enabled
    sort threads once by exec time and once by LLC miss
    while duplication:
      look at next LLC-miss thread and dequeue it from exec-time
      look at next exec-miss thread and dequeue it from LLC-miss

    while threads unassigned && queue not empty:
      dequeue one thread from LLC miss list for SMT#0
      dequeue one thread from LLC-miss list for SMT#1
      dequeue one thread from exe-time list for SMT#0
      dequeue one thread form exec-time list for SMT#1
  \end{verbatim}

  \paragraph{Minimize migration pseudo-code}
  \begin{verbatim}
  sort all threads by LLC-miss
  sliding window size #threads with less than 5% LLC miss difference
  if at least 2 threads in the current window are migrated
    if two threads are swaping cores
      don't do the migration
    ALTERNATIVELY
    if the from-core-to-core-matrix has entries on opposing fields
      swap the to-values of both entries
  \end{verbatim}


% ----------------------------------------------------------------------------

\subsection{Enforce}

  * Enforce the thread-to-core assignment


\end{comment}
