% vim:set ft=tex:
\section{Load Balancer}
\label{design:threadmapper}

All the techniques introduced in the previous sections of this chapter
are now united into the central load balancing component.
The section describes the components the load balancer is build of and relies
upon.
\todo{chapter structure}


\paragraph{Load Balancer \& Task Proxies}
To achieve good load balancing performance, the load balancer needs information
about the underlying hardware, as well as the amount of work in the system.
Thereby is it also possible to build hierarchical load balancing, where
only a partial view on the hardware and only a subset of tasks runs on this
specific load balancer.
However, while leading to isolation between different subsets of the system,
the overall performance can suffer, as the system is effectively partitioned.
A surplus of computing resources in one partition is unusable by another
partition leading to wasted resources.
The partition size can only be adapted at core granularity, so a precise
resource accounting may not be possible.

Isolation between parts of the system can also be provided by a central load
balancing component.
A central load balancer as well as a hierarchical approach is part of the chain
of trust of each balanced application, so both approaches are equally
trustworthy.
But the central component can react dynamically to changes in the workloads and
shift the partition borders dynamically.
Also if tasks with security properties, as discussed in section
\ref{design:isolation}, enter or leave the system, the dynamic approach can
repurpose a core.
Redistribution of cores between hierarchical load balancer would require
additional reasoning which load balancer the free core is assigned to, or which
load balancer must release a core.
Either the load balancer must communicate their reasoning between each other or
with a central instance further up the hierarchy.
The former weakens the isolation and increases overhead, the latter means more
logic in a central instance.

To decrease the overhead and due to the easier implementation a central load
balancing component is chosen.

Next to the load balancer task specific proxy scheduler act as information
source.
Each task can have isolation (\ref{design:isolation}) and communication
properties (\ref{design:communication}).
The task proxy stores this task specific configuration and provides it to the
load balancer.

A factory creates the load balancer as well as the task proxies and creates
from the configuration parameters the configuration structure exchanged between
proxies and balancer.
The configuration structure comprises of the priority band and a list of
the group types and identifiers.


\paragraph{Thread Management}
The main management component inside the load balancer is the thread
management.
It creates a internal management class for each thread and identifies the
thread via its capability.
Using the configuration passed, the thread manager builds the thread groups,
defined in the Ned configuration.
It answeres existance requests for threads by capability and also cleans up
threads, which left the system.
It also provides a consistent view on all managed threads for other components.


\paragraph{CPU Topology \& Accounting}

\paragraph{Information Gathering \& Balancing}
% Interval cycle with Measure, Predict, Decide (PlacementGen), Enforce
% all including implementation details. Algorithms make the next chapter.
% A separated implementation chapter doesn't feel natural



\begin{comment}

\subsection{Hardware performance monitoring}

\subsection{Core Accounting}
Besides the structure of topology data, threads, thread-groups and the
different thread to core assignments need to be modeled.


\paragraph{Topology description}
Represents the cache and core layout of the CPU.
On Haswell it consist of a package description containing core descriptions.
A core description consists of the hardware assigned APIC\_id, the kernel
assigned fiasco\_id, and the SMT abstraction.
It can be expanded to include multi-socket systems by adding more package
descriptions.
It maps the fiasco view on the cores onto the actual HW topology and uses CPUID
to determine corresponding logical cores.

\paragraph{Thread\_t}
is the administrative representation of a L4-thread.
It contains not only of the thread parameters passed via run\_thread(), but
also measurements for the current and last interval.
LLC-misses, execution time, an identifier, and the cores it currently runs on
and will run on in the next interval.

\paragraph{Thread Groups}


% ----------------------------------------------------------------------------

\subsection{Analyse}
  \paragraph{Hardware}
    * Use dedicated hardware --> hardware analysis future work \\
    * Haswell: per core: L1 \& L2 cache; shared L3 cache between all cores

  \paragraph{Measurement}
    * Measure the load on each core during a time interval \textbf{TI}; \\
    * measure LLC misses during last \textbf{TI};


% ----------------------------------------------------------------------------

\subsection{Predict}

  * Predict load and LLC misses of each thread for the next time intervall; \\


% ----------------------------------------------------------------------------

\subsection{Decide}

  * Decide on a thread distribution to cores, based on predictions;

  \subsubsection{Placement Generator}

  \paragraph{Pseudo-code of placement algorithm}
  \begin{verbatim}
  from all threads:
    select #core highest LLC miss rate
    select #core highest exec-time
    intersection of both are critical threads
    if threads placed on different cores
      then do nothing
    else
      move higher LLC miss rate thread to other core
    do accounting

  forall threads left do:
    bin by priority levels
    sort each bin by miss rate

  forall prio-bin in prio-bin-list do:
    while threads in prio-bin
      dequeue highest miss rate
      sort cores by lowest accounted miss rate
      place max(#core, #threads left in bin) threads RR on cores;
  \end{verbatim}

  \paragraph{\gls{smt} abstraction code}
  \begin{verbatim}
  if SMT is enabled
    sort threads once by exec time and once by LLC miss
    while duplication:
      look at next LLC-miss thread and dequeue it from exec-time
      look at next exec-miss thread and dequeue it from LLC-miss

    while threads unassigned && queue not empty:
      dequeue one thread from LLC miss list for SMT#0
      dequeue one thread from LLC-miss list for SMT#1
      dequeue one thread from exe-time list for SMT#0
      dequeue one thread form exec-time list for SMT#1
  \end{verbatim}

  \paragraph{Minimize migration pseudo-code}
  \begin{verbatim}
  sort all threads by LLC-miss
  sliding window size #threads with less than 5% LLC miss difference
  if at least 2 threads in the current window are migrated
    if two threads are swaping cores
      don't do the migration
    ALTERNATIVELY
    if the from-core-to-core-matrix has entries on opposing fields
      swap the to-values of both entries
  \end{verbatim}


% ----------------------------------------------------------------------------

\subsection{Enforce}

  * Enforce the thread-to-core assignment


\end{comment}
