% vim:set ft=tex:
\section{Load Balancer}
\label{design:balancer}

I now unit all the techniques I introduced in the previous sections of this chapter
into the central load balancing component.
In this section I describe the components the load balancer is built of.
% todo chapter structure


\paragraph{Load Balancer \& Task Proxies.}
To achieve good load balancing performance, the load balancer needs information
about the underlying hardware, as well as the total amount of work in the system.
It is also possible to build hierarchical load balancing, where
only a partial view on the hardware is available and only a subset of tasks run
on this specific load balancer.
However, while leading to isolation between different subhierarchies,
the overall performance can suffer, as the system is effectively partitioned.
A surplus of computing resources in one partition is unusable by another
partition leading to wasted resources.
The size of the partition is only adaptable at core granularity, so meeting the
precise resource needs may not be possible, leading to overprovisioning and,
consequently, resource waste.

Isolation between parts of the system can also be provided by a central load
balancing component.
A central load balancer as well as a hierarchical approach are part of the chain
of trust of each balanced application, so both approaches are equally
trustworthy.
However, the central component can respond dynamically to changes in the workloads and
shift the partition borders as needed.
Also, if tasks with security properties, as discussed in section
\ref{design:isolation}, enter or leave the system, the dynamic approach can
repurpose a core.
Redistribution of cores between hierarchical load balancers would require
additional reasoning which load balancer the free core is assigned to, or which
load balancer must release a core.
The load balancers must communicate their reasoning either with each other, or
with a central instance further up the hierarchy.
The former weakens the isolation and increases overhead, the latter means more
logic in a central instance.
Due to the easier implementation of a central load balancing component,
I choose to implement the centralized service.
\\

Internally, the load balancer uses task-specific proxy scheduler to store the
task's configuration information.
Each task can have isolation (\ref{design:isolation}) and communication
properties (\ref{design:comm}).
The task's proxy scheduler stores the configuration and provides it to the
central load balancer, as depicted in figure \ref{state:fig:balancer_proxy}.

A factory creates the central load balancer and processes the configuration
parameters for each client task.
The client's proxy scheduler stores the configuration structure and provides it
to the central load balancer.
The configuration structure comprises the priority range and a list of
groups derived from the parameters.

\begin{figure}[h!]
  \setcapindent*{1em}
  \setlength{\columnsep}{10mm}
  \begin{captionbeside}[Load balancer interaction with clients.]
    {Load Balancer with one Task Proxy per client task.
      The Task Proxy adds the task's configuration to the \texttt{run\_thread}
    call.}
  \includegraphics[]{images/threadMapper_layout}
\end{captionbeside}
  \label{state:fig:balancer_proxy}
\end{figure}


\paragraph{Thread Management.}
All threads known to the load balancer are managed by the thread management.
It creates an internal administration class for each thread and identifies a
thread via its capability.
The load balancer evaluates the configuration passed by the task proxy and
if isolation or communication groups are set, the thread is assigned to these
groups.
The management provides a consistent view on all managed threads for other
components and is also in charge of cleanup.


\paragraph{CPU Topology \& Accounting.}
Besides the management of threads, the load balancer must also know the
topology of the underlying hardware.
The topology scanner analyses the topology via CPUID and stores it in a
hierarchical data structure.
A package describes all physical cores of one socket.
If \gls{ht} is present, each physical core representation contains data structures for
two logical cores.

Each physical core is associated with an accounting object.
Core accounting stores all assigned threads and computes the accumulated
load on its assigned core.
Additionally, it provides information on which thread to migrate in case of an
imbalance.
The accounting object connects topology, physical cores, and threads, therefore,
it is the place for the SMT abstraction introduced in \ref{design:smt}.
Each thread assigned to the physical core is distributed to its hyper-threads
by an algorithm, e.g. round robin, \gls{llc} weight, IPC, or load.
If no \gls{ht} is available, no further assignment is necessary.
The availability of \gls{ht} is dynamically deduced from the topology enumeration results.


\paragraph{Information Gathering \& Balancing.}
% each interval measure predict decide enforce cycle
% measure: for each thread, gather performance counter values and execution
  % time from kernel and store in thread administration class.
% prediction: compute a performance forecast for the next interval
% decide: select a thread to core assignment based on the prediction for the
  % next interval.
% enforce: configure the core affinity for each thread and inform the kernel
The logic of the load balancer is partitioned into four consecutive modules:
measurement, prediction, decision, and enforcement.
Each balancing interval, they run once and adapt the thread-to-core
assignment to the current circumstances.
Figure \ref{arch:fig:intervalcycle} depicts this Measure-Predict-Decide-Enforce
(MPDE) cycle.

\begin{figure}[h]
  \setcapindent*{1em}
  \setlength{\columnsep}{10mm}
  \begin{captionbeside}[MPDE Cycle with topology awareness.]
    {Sequential succession of the measure-predict-decide-enforce cycle inside the load
    balancer. This cycle executes every balancing interval to read performance
    data and to decide on migrations.}
  \includestandalone[mode=tex]{images/statusvortrag/arch_interval_cycle}
  \end{captionbeside}
  \label{arch:fig:intervalcycle}
\end{figure}

First, the information about the current situation is updated by gathering
performance counter and execution time values of the last interval for each
thread.
Based on these measurements, the prediction module generates a forecast for
each thread's resource needs.
This forecast forms the input for the decision module.
Together with the topology information and the accounting values for each core
from last interval, the decision module checks if the system is still balanced.
If not, selected threads must be migrated to establish balance again.
After deciding which threads must be migrated, the enforce module applies the
migrations by modifying each thread's affinity and informing the kernel.

These steps repeat every interval. A high interval duration reduces the
load balancing overhead, but also leaves the system in a bad state for a longer
time, when thread behaviour changes or threads enter and leave the system.
A short interval duration increases the measurement overhead and may increase
the amount of migrations.
The ideal interval size increases system performance to outweigh the interval's
own run-time overhead.
\\

The benefit of dividing the MPDE cycle into four separate components lies
in its flexibility.
If other information sources should be used, the measurement component is the
place to add them.
If different performance prediction algorithms should be tested, only the
prediction component changes.
The decision component allows to easily test balancing and placement algorithms
and also incorporating multi socket systems is neatly encapsulated in it.

Of course, if information sources arise, changes down the line are necessary,
but this approach provides a concise description of the different components.
Also, extending the load balancer to heterogeneous architectures is assumed to
work well, as the design is similar to the approach in
\cite{sarma_smartbalance_2015}.


\begin{comment}

\paragraph{Topology description}
Represents the cache and core layout of the CPU.
On Haswell it consist of a package description containing core descriptions.
A core description consists of the hardware assigned APIC\_id, the kernel
assigned fiasco\_id, and the SMT abstraction.
It can be expanded to include multi-socket systems by adding more package
descriptions.
It maps the fiasco view on the cores onto the actual HW topology and uses CPUID
to determine corresponding logical cores.

\paragraph{Thread\_t}
is the administrative representation of a L4-thread.
It contains not only of the thread parameters passed via run\_thread(), but
also measurements for the current and last interval.
LLC-misses, execution time, an identifier, and the cores it currently runs on
and will run on in the next interval.

\end{comment}
