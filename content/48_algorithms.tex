% vim:set ft=tex:
\section{Algorithms}
\label{impl:algos}

% SMT assignment algorithm
% core assignment algorithm
  % in space and in time
% group placement algorithms
% simple load distribution
% missing shared memory

The load balancer uses different algorithms for SMT placement, assignment of
groups and balancing of threads without group affiliation.
I present algorithms for these three places and also a simple behaviour-unaware
load balancing algorithm.
The simple load distribution algorithm serves as baseline to show the benefit of
behaviour analysis.

\paragraph{SMT distribution}
After the physical core assignment is done, each physical core internally
distributes its set of assigned threads to the hyper-threads.
Different options arise: simple round robin, by \gls{mpc}, by \gls{instpc}, or
by load.

Another possibility consists of one core getting the high \gls{mpc}
threads and the other one the threads with high \gls{instpc}.
I assume this leads to a good symbiosis, as the high \gls{instpc} threads compute,
while the \gls{mpc} heavy threads wait for memory.

Superiority of one of these options above the others must be empirically evaluated.


\paragraph{Two dimensional balance}
What is balance? This question takes the most effort to answer, because it
depends on several factors: isolation groups reduce the core count;
communication groups are excluded from load balancing;
and only threads without further attachments are considered for migration.
So balancing works on top of the placement of communication groups, which is
possibly already imbalanced.
The load balancer must even out these imbalances and be aware of cache usage of
these threads, too.
Also, if only parts of the system are managed by the load balancing service,
the base load needs to be considered during placement.
All this leads to a difficult environment for the load balancer and complicates
the test for the overall system balance.

The core accounting needs to keep track of unaccounted load, accounted but
not migratable load, and the migratable load.
\Gls{mpc} measurements are only accessible for managed threads, meaning not
migratable load and migratable load.
Therefore, \gls{mpc} aware placement and balancing depends on these two
groups.
\\

As mentioned in section \ref{design:load}, balance is two dimensional.
Spatial balance takes priority over temporal balance, to allow better cache usage.
But at the end, temporal balance is equally important.

A simple spatial balance metric is the following: Sort the migratable threads by
\gls{mpc} and assign the top four to one core each.
The number of threads chosen depends on the number of available cores.

To adapt preexisting imbalances regarding \gls{mpc},  the number of chosen
threads can vary; for example if the difference between the core with the
highest \gls{mpc} and the core with the second highest \gls{mpc} is larger than
the \gls{mpc} heaviest thread, assigning another thread to it makes no sense.

Temporal balancing is different, as it has to assign all remaining threads.
Thereby, the absolute core load is relevant, not only the load of the managed
threads.
Sorting the threads by load, and assigning them one by one to the
core with the least load, leads to a balanced system.

When imbalances in time are detected, the temporal balancer computes the difference
and searches for a thread to migrate.
Thereby, the ideal thread generates load that accounts for half the difference.
If no such thread is found, the thread generating the next higher amount of
load is migrated.


\paragraph{MPC-IPC-Placement}
This algorithm creates two copies of the list of all threads and sorts one copy
by \gls{mpc} and one by \gls{instpc}.
The first item of the \gls{mpc}-list is deleted from the
\gls{instpc}-list.
Then, the algorithm delets the first item of the  \gls{instpc}-list from the
\gls{mpc}-list.
Both lists are processed item by item, until the two lists are disjoint.

As long as both lists contain more threads than cores available, each core
is assigned one thread from each list.
If there are less threads than available cores, the algorithm assigns the
remaining thread to the core with the least \gls{mpc} and \gls{instpc},
respectively.

I assume, this works rather well for SMT, as each physical core gets a mixed
workload.
However, this approach does not account for execution time.
A possible improvement is to assign the remaining threads by
exexcution time instead of \gls{instpc} and \gls{mpc}.


\paragraph{First-Last-Placement}
The list of threads without group affiliation is sorted by \gls{mpc}.
Then, the algorithm assigns the first and the last element of the list to the
same core.
The core is selected based on \gls{mpc}.


\paragraph{Group placements}
% todo on what basis is first last placement used?
If the group is of type \texttt{distr}, first-last-placement is used for its
members.

\texttt{Clsvr} type groups are handled in two ways: First, it is checked which
group members belong to the same tasks.
Of each task one thread is assigned to the same core as the server thread.
% todo on what basis? why not by load? would make more sense.
The others threads are distributed by first last placement.

\texttt{Sec} and \texttt{rt} type groups get their private physical core, hence
all threads of these groups are assigned to the same core.


\paragraph{Simple Load Distribution}
The algorithm assigns incoming threads to the core with the least load.
If several cores have the same load, e.g. in an overload scenario, the
algorithm assigns the thread to the core with the least threads.

A core's load is determined by the amount of idle time since the last
placement.
When threads leave, the system becomes unbalanced, but no extra balancing
actions are taken.

To prove that behaviour analysis brings any benefit, it has to beat
this simple algorithm.


\paragraph{Missing logic for shared memory}
\todo{where is a better place for this?}
Tasks running several threads which use shared memory to communicate will have
a worse performance using these algorithms.
No communication parameter tells the load balancer that these tasks should be
grouped together.
Consider two threads with shared memory on two separated cores.
The only connection is the \gls{llc}.
The cache line holding the shared memory will bounce back and forth between
the two cores, as one writes and invalidates the cache of the other.
This means additional L1 and L2 cache miss costs, each time they communicate.

If the number of threads is equal to the number of cores, the cache line will
bounce between four different L1 and L2 caches, impairing the overall
performance of the threads.

An additional shared memory configuration parameter is necessary to help the
load balancer diminish this issue.
Then two threads could be placed, so that they run co-scheduled on the same
caches.



\begin{comment}
\paragraph{Pseudo-code of placement algorithm}
  \begin{verbatim}
  from all threads:
    select #core highest LLC miss rate
    select #core highest exec-time
    intersection of both are critical threads
    if threads placed on different cores
      then do nothing
    else
      move higher LLC miss rate thread to other core
    do accounting

  forall threads left do:
    bin by priority levels
    sort each bin by miss rate

  forall prio-bin in prio-bin-list do:
    while threads in prio-bin
      dequeue highest miss rate
      sort cores by lowest accounted miss rate
      place max(#core, #threads left in bin) threads RR on cores;
  \end{verbatim}

  \paragraph{\gls{smt} abstraction code}
  \begin{verbatim}
  if SMT is enabled
    sort threads once by exec time and once by LLC miss
    while duplication:
      look at next LLC-miss thread and dequeue it from exec-time
      look at next exec-miss thread and dequeue it from LLC-miss

    while threads unassigned && queue not empty:
      dequeue one thread from LLC miss list for SMT#0
      dequeue one thread from LLC-miss list for SMT#1
      dequeue one thread from exe-time list for SMT#0
      dequeue one thread form exec-time list for SMT#1
  \end{verbatim}

  \paragraph{Minimize migration pseudo-code}
  \begin{verbatim}
  sort all threads by LLC-miss
  sliding window size #threads with less than 5% LLC miss difference
  if at least 2 threads in the current window are migrated
    if two threads are swaping cores
      don't do the migration
    ALTERNATIVELY
    if the from-core-to-core-matrix has entries on opposing fields
      swap the to-values of both entries
  \end{verbatim}

\end{comment}
