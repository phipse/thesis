% vim:set ft=tex
\section{Algorithms}
\label{impl:algos}

The first last placement and group placement algorithms are used in the
placement generator.
The balancing need algorithm is used in the decision component to see if the
current performance values bring the system out of balance.
SMT distribution algorithms are part of the core accounting, as the SMT
abstraction is implemented there.
The ready queue balancing serves as baseline to show the benefit of behaviour
analysis.

\paragraph{Balancing Need Test}
This test takes the most effort.
First, a balance has to be established.
Either each core is evaluated separately or all cores are evaluated together.
The first approach assumes, that a system wide balance was established
previously and only if the core itself gets out of balance, actions are
necessary.
There the question arises, how far is the current value allowed to divert from
the balance value?

The second approach bears the problem of a global balance value.
Consider four cores and four threads with highly different \gls{llc} weight
values.
Of course, each core is assigned one thread, but what is then the global
balance?
Maximum, minimum, average, and overall distribution need to be considered
before a statement regarding the balance can be made.
A system in balance has minimal distance between the highest and lowest
\gls{llc} weight of each core.
But here the question also arises: How high is the tolerance for the current
balance?

And what happens, if we have six threads on four cores, two with a high,
one with medium, and three with low weight?
A pure weight balance, would assign 1-1-1-3, which is not very well balanced,
as low weight threads are assumed to have a high computational demand.
Hence, 1-1-2-2 or 1-2-1-2 appear better balanced.

As extendedly discussed in section \ref{design:smt}, an pure online load
balancing algorithm for arbitrary threads, without offline information, cannot
achieve perfect symbiosis and hence, not perfect performance.
But approximations are possible.
1-1-1-3, 1-1-2-2, and 1-2-1-2 look like a series of improved balancing
decisions.
If three high IPC threads are scheduled on the same core, their \gls{llc}
weight is not expected to change much, but their IPC value will suffer.
This suffering is observable and should trigger a migration, to try to increase
the threads performance again.

This leads to the conclusion, that a system balanced by \gls{llc} weight,
should be extended by a second metric to counter overfitting.
But here is an assumption in place: It is assumed, a better IPC value was
observed, what if not?
If a high number of threads is assigned to one core, it is likely, at least one
of them had better performance in a previous iteration.
Additionally, if a high imbalance regarding thread count per core is observed,
a thread is migrated from the high to the low count core, providing the chance
of better performance.


\paragraph{First Last Placement}
The list of threads is sorted by a metric.
As long as the number of threads is larger than the number of cores times two,
the first and last element of the sorted list are assigned to the core with the
lowest metric value.
If less threads are available, they are assigned one by one to the core with
the least threads.
If ties between core accounts are detected, the next thread or thread pair is
assigned to the core with the least assigned threads.
By assigning the first and last thread a first average is generated and the
number of iterations to assign all threads is nearly halved.
Also the imbalance regarding thread count per core, is smaller.

Let $x$ be the number of threads and $c$ the number of cores.
The number of placement iterations $i$ after sorting is
$i = x / (2*c) + x \% (2*c)$.


\paragraph{Group placements}
If the group is of type \texttt{distr} first last placement is used for its
members.

\texttt{Clsvr} type groups are handled in two ways: First it is checked, which
group members belong to the same tasks.
Of each task one thread is assigned to the same core as the server thread.
The others threads are distributed by first last placement.

\texttt{Sec} and \texttt{rt} type groups get their private physical core, hence
all threads of these groups are assigned to the same core.

\texttt{Sec} and \texttt{rt} are of linear complexity to the number of threads,
\texttt{distr} has the complexity of first last placement.
For \texttt{clsvr} the complexity estimation is dependent on the number of client
threads from different tasks and from the same task.
If only single threaded clients are in the group the complexity is linear, if
only multi threaded clients are present, the complexity is the number of
client tasks plus the server plus the complexity of first last placement for
the additional threads.
\todo{formal complexity analysis?}


\paragraph{SMT distribution}
After the physical core assignment is done, each physical core internally
distributes its set of assigned threads to the hyper-threads.
Different options arise: simple round robin or round robin after sorting the
threads by \gls{llc} weight or by IPC.

Or one core get the highest \gls{llc} weight threads and the other the threads
with high IPC.
It is assumed, this leads to a good symbiosis, as the high IPC threads compute,
while the high \gls{llc} weight threads wait for memory.

Superiority of one above the others must be empirically evaluated.


\paragraph{Balanced Ready Queue Placement}
The simple balanced ready queue assignment to cores is used to rate the
performance of the behaviour aware scheduling.
It ignores configurations and just places an incoming thread on the core with
the least threads.
As threads leave the system the ready queues shrink differently, which is
compensated by entering threads.

To prove that behaviour analysis brings any benefit, it has to clearly beat
this simple algorithm


\paragraph{Missing logic for shared memory}
Tasks running several threads which use shared memory to communicate will have
a worse performance using these algorithms.
No communication parameter tells the load balancer, that these tasks should be
grouped together.
Consider two threads with shared memory on two separated cores.
The only connection is the \gls{llc}.
The cache line holding the shared memory, will bounce back and forth between
the two cores, as one writes and invalidates the cache of the other.
This means additional L1 and L2 cache miss costs, each time they communicate.

If the number of threads is equal to the number of cores, the cache line will
bounce between four different L1 and L2 caches, impairing the overall
performance of the threads.

An additional shared memory configuration parameter is necessary to help the
load balancer diminish this issue.
Then two threads could be placed, so that they run co-scheduled on the same
caches.



\begin{comment}
\paragraph{Pseudo-code of placement algorithm}
  \begin{verbatim}
  from all threads:
    select #core highest LLC miss rate
    select #core highest exec-time
    intersection of both are critical threads
    if threads placed on different cores
      then do nothing
    else
      move higher LLC miss rate thread to other core
    do accounting

  forall threads left do:
    bin by priority levels
    sort each bin by miss rate

  forall prio-bin in prio-bin-list do:
    while threads in prio-bin
      dequeue highest miss rate
      sort cores by lowest accounted miss rate
      place max(#core, #threads left in bin) threads RR on cores;
  \end{verbatim}

  \paragraph{\gls{smt} abstraction code}
  \begin{verbatim}
  if SMT is enabled
    sort threads once by exec time and once by LLC miss
    while duplication:
      look at next LLC-miss thread and dequeue it from exec-time
      look at next exec-miss thread and dequeue it from LLC-miss

    while threads unassigned && queue not empty:
      dequeue one thread from LLC miss list for SMT#0
      dequeue one thread from LLC-miss list for SMT#1
      dequeue one thread from exe-time list for SMT#0
      dequeue one thread form exec-time list for SMT#1
  \end{verbatim}

  \paragraph{Minimize migration pseudo-code}
  \begin{verbatim}
  sort all threads by LLC-miss
  sliding window size #threads with less than 5% LLC miss difference
  if at least 2 threads in the current window are migrated
    if two threads are swaping cores
      don't do the migration
    ALTERNATIVELY
    if the from-core-to-core-matrix has entries on opposing fields
      swap the to-values of both entries
  \end{verbatim}

\end{comment}
