% vim:set ft=tex:
\section{Algorithms}
\label{impl:algos}

% SMT assignment algorithm
% core assignment algorithm
  % in space and in time
% group placement algorithms
% simple load distribution
% missing shared memory

The load balancer uses different algorithms for SMT placement, assignment of
groups and balancing of threads without group affiliation.
I present algorithms for these three places and also a simple behaviour unaware
load balancing algorithm.
The simple load distribution algorithm serves as baseline to show the benefit of
behaviour analysis.

\paragraph{SMT distribution}
After the physical core assignment is done, each physical core internally
distributes its set of assigned threads to the hyper-threads.
Different options arise: simple round robin, by \gls{mpc}, by \gls{instpc}, or
by load.

Or one core get the highest \gls{llc} weight threads and the other the threads
with high \gls{instpc}.
I assume this leads to a good symbiosis, as the high \gls{instpc} threads compute,
while the \gls{mpc} heavy threads wait for memory.

Superiority of one above the others must be empirically evaluated.


\paragraph{Two dimensional balance}
What is balance? This question takes the most effort to answer, because it
depends on several factors: isolation groups reduce the core count;
communication groups are excluded from load balancing;
only threads without further attachments are considered for migration.
So balancing works on top of the placement of communication groups, which are
possibly already imbalanced.
The load balancer must even out these imbalances and be aware of cache usage of
these threads, too.
Also, if only parts of the system are managed by the load balancing service,
the base load needs to be considered during placement.
All this leads to a difficult environment for the load balancer and complicates
the test, if the overall system is balanced.

The core accounting needs to keep track of unaccounted load, accounted but
not migratable load, and the migratable load.
\Gls{mpc} measurements are only accessible for managed threads, meaning not
migratable load and migratable load.
Therefore, \gls{mpc} aware placement and balancing depends on these two
groups.
\\

As mentioned in section \ref{design:load}, balance is two dimensional.
Space balance takes priority over time balance, to allow better cache usage.
But at the end, time balance is equally important.

A simple space balance metric is the following: Sort the migratable threads by
\gls{mpc} and assign the top four to one core each.
The number of threads chosen, depends on the number of available cores.

To adapt preexisting imbalances regarding \gls{mpc},  the number of chosen
threads can vary; for example if the difference between the core with the
currently highest \gls{mpc} and the next, is larger than the \gls{mpc} heaviest
thread, assigning another thread to it, makes no sense.

Balancing in time is different, as it has to assign all remaining threads.
Thereby, the absolute core load is relevant, not only the load of the managed
threads.
Sorting the threads by load, and assigning them one by one to the
core with the least load, leads to a balanced system.

When imbalances in time are detected, the time balancer computes the difference
and searches for a thread generating half the difference of load to migrate.
If no such thread is found, the thread generating next higher load value is
migrated.


\paragraph{MPC-IPC-Placement}
The list of threads is sorted once by \gls{mpc} and once by \gls{instpc}.
Then the two lists are processed item by item and the current item in one list,
is deleted from the other, until both list are disjoint.

As long as both list contain more threads the cores are available, each core
is assigned one thread from each list.
If there are less threads, the remaining threads are assigned to the core with
the least \gls{mpc} and \gls{instpc}, respectively.

I assume, this works rather well for SMT, as each physical core gets a mixed
workload.
However, this approach does not account for execution time.
A possible improvement is to assign the remaining threads by
exexcution time instead of \gls{instpc} and \gls{mpc}.


\paragraph{Group placements}
% todo on what basis is first last placement used?
If the group is of type \texttt{distr} first last placement is used for its
members.

\texttt{Clsvr} type groups are handled in two ways: First it is checked, which
group members belong to the same tasks.
Of each task one thread is assigned to the same core as the server thread.
% todo on what basis? why not by load? would make more sense.
The others threads are distributed by first last placement.

\texttt{Sec} and \texttt{rt} type groups get their private physical core, hence
all threads of these groups are assigned to the same core.


\paragraph{Simple Load Distribution}
The algorithm assigns incoming threads to the core with the least load.
If several cores have the same load, e.g. in an overload scenario, the
algorithm assigns the thread to the core with the least threads.

A core's load is determined by the amount of idle time since the last
placement.
When threads leave, the system becomes unbalanced, but no extra balancing
actions are taken.

To prove that behaviour analysis brings any benefit, it has to beat
this simple algorithm.


\paragraph{Missing logic for shared memory}
\todo{where is a better place for this?}
Tasks running several threads which use shared memory to communicate will have
a worse performance using these algorithms.
No communication parameter tells the load balancer, that these tasks should be
grouped together.
Consider two threads with shared memory on two separated cores.
The only connection is the \gls{llc}.
The cache line holding the shared memory, will bounce back and forth between
the two cores, as one writes and invalidates the cache of the other.
This means additional L1 and L2 cache miss costs, each time they communicate.

If the number of threads is equal to the number of cores, the cache line will
bounce between four different L1 and L2 caches, impairing the overall
performance of the threads.

An additional shared memory configuration parameter is necessary to help the
load balancer diminish this issue.
Then two threads could be placed, so that they run co-scheduled on the same
caches.



\begin{comment}
\paragraph{Pseudo-code of placement algorithm}
  \begin{verbatim}
  from all threads:
    select #core highest LLC miss rate
    select #core highest exec-time
    intersection of both are critical threads
    if threads placed on different cores
      then do nothing
    else
      move higher LLC miss rate thread to other core
    do accounting

  forall threads left do:
    bin by priority levels
    sort each bin by miss rate

  forall prio-bin in prio-bin-list do:
    while threads in prio-bin
      dequeue highest miss rate
      sort cores by lowest accounted miss rate
      place max(#core, #threads left in bin) threads RR on cores;
  \end{verbatim}

  \paragraph{\gls{smt} abstraction code}
  \begin{verbatim}
  if SMT is enabled
    sort threads once by exec time and once by LLC miss
    while duplication:
      look at next LLC-miss thread and dequeue it from exec-time
      look at next exec-miss thread and dequeue it from LLC-miss

    while threads unassigned && queue not empty:
      dequeue one thread from LLC miss list for SMT#0
      dequeue one thread from LLC-miss list for SMT#1
      dequeue one thread from exe-time list for SMT#0
      dequeue one thread form exec-time list for SMT#1
  \end{verbatim}

  \paragraph{Minimize migration pseudo-code}
  \begin{verbatim}
  sort all threads by LLC-miss
  sliding window size #threads with less than 5% LLC miss difference
  if at least 2 threads in the current window are migrated
    if two threads are swaping cores
      don't do the migration
    ALTERNATIVELY
    if the from-core-to-core-matrix has entries on opposing fields
      swap the to-values of both entries
  \end{verbatim}

\end{comment}
