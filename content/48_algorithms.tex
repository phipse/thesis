% vim:set ft=tex
\section{Algorithms}
\label{impl:algos}

The first last placement and group placement algorithms are used in the
placement generator.
The balancing need algorithm is used in the decision component to see if the
current performance values bring the system out of balance.
SMT distribution algorithms are part of the core accounting, as the SMT
abstraction is implemented there.
The ready queue balancing serves as baseline to show the benefit of behaviour
analysis.

\paragraph{Two dimensional balance}
This test takes the most effort to develop, because it depends on several
factors: isolation groups reduce the core count; communication groups are
excluded from load balancing; only threads without further attachments are
considered for migration.
So balancing works on top of the placement of communication groups, which are
possibly already imbalanced.
The load balancer must even out these imbalances and be aware of cache usage of
these threads, too.
Also, if only parts of the system are managed by the load balancing service,
the base load needs to be considered during placement.
All this leads to a difficult environment for the load balancer and complicates
the test, if the overall system is balanced.

The core accounting needs to keep track of unaccounted load, accounted but
not migratable load, and the migratable load.
\Gls{mpc} measurements are only accessible for managed threads, meaning not
migratable load and migratable load.
Therefore, \gls{mpc} aware placement and balancing can depends on these two
groups.
\\

As mentioned in section \ref{design:load}, balance is two dimensional.
Space balance takes priority over time balance, to allow better cache usage.
But at the end, time balance is equally important.

A simple space balance metric is the following: Sort the migratable threads by
\gls{mpc} and assign the top four to one core each.
The number of threads chosen, depends on the number of available cores.

To adapt preexisting imbalances regarding \gls{mpc},  the number of chosen
threads can vary; for example if the difference between the core with the
currently highest \gls{mpc} and the next, is larger than the \gls{mpc} heaviest
thread, assigning another thread to it, makes no sense.

Balancing in time is different, as it has to assign all remaining threads.
Thereby, the absolute core load is relevant, not only the load of the managed
threads.
Sorting the threads by load, and assigning them one by one to the
core with the least load, leads to a balanced system.

When imbalances in time are detected, the time balancer computes the difference
and searches for a thread generating half the difference of load to migrate.
If no such thread is found, the thread generating next higher load value is
migrated.


\paragraph{First Last Placement}
The list of threads is sorted by a metric.
As long as the number of threads is larger than the number of cores times two,
the first and last element of the sorted list are assigned to the core with the
lowest metric value.
If less threads are available, they are assigned one by one to the core with
the least threads.
If ties between core accounts are detected, the next thread or thread pair is
assigned to the core with the least assigned threads.
By assigning the first and last thread a first average is generated and the
number of iterations to assign all threads is nearly halved.
Also the imbalance regarding thread count per core, is smaller.

Let $x$ be the number of threads and $c$ the number of cores.
The number of placement iterations $i$ after sorting is
$i = x / (2*c) + x \% (2*c)$.


\paragraph{Group placements}
If the group is of type \texttt{distr} first last placement is used for its
members.

\texttt{Clsvr} type groups are handled in two ways: First it is checked, which
group members belong to the same tasks.
Of each task one thread is assigned to the same core as the server thread.
The others threads are distributed by first last placement.

\texttt{Sec} and \texttt{rt} type groups get their private physical core, hence
all threads of these groups are assigned to the same core.

\texttt{Sec} and \texttt{rt} are of linear complexity to the number of threads,
\texttt{distr} has the complexity of first last placement.
For \texttt{clsvr} the complexity estimation is dependent on the number of client
threads from different tasks and from the same task.
If only single threaded clients are in the group the complexity is linear, if
only multi threaded clients are present, the complexity is the number of
client tasks plus the server plus the complexity of first last placement for
the additional threads.
\todo{formal complexity analysis?}


\paragraph{SMT distribution}
After the physical core assignment is done, each physical core internally
distributes its set of assigned threads to the hyper-threads.
Different options arise: simple round robin or round robin after sorting the
threads by \gls{llc} weight or by IPC.

Or one core get the highest \gls{llc} weight threads and the other the threads
with high IPC.
It is assumed, this leads to a good symbiosis, as the high IPC threads compute,
while the high \gls{llc} weight threads wait for memory.

Superiority of one above the others must be empirically evaluated.


\paragraph{Simple Logic Distribution}
The algorithm assigns incoming threads to the core with the least load.
If several cores have the same load, e.g. in an overload scenario, the
algorithm assigns the thread to the core with the least threads.

A core's load is determined by the amount of idle time since the last
placement.
When threads leave, the system becomes unbalanced, but not extra balancing
actions are taken.

To prove that behaviour analysis brings any benefit, it has to clearly beat
this simple algorithm.


\paragraph{Missing logic for shared memory}
Tasks running several threads which use shared memory to communicate will have
a worse performance using these algorithms.
No communication parameter tells the load balancer, that these tasks should be
grouped together.
Consider two threads with shared memory on two separated cores.
The only connection is the \gls{llc}.
The cache line holding the shared memory, will bounce back and forth between
the two cores, as one writes and invalidates the cache of the other.
This means additional L1 and L2 cache miss costs, each time they communicate.

If the number of threads is equal to the number of cores, the cache line will
bounce between four different L1 and L2 caches, impairing the overall
performance of the threads.

An additional shared memory configuration parameter is necessary to help the
load balancer diminish this issue.
Then two threads could be placed, so that they run co-scheduled on the same
caches.



\begin{comment}
\paragraph{Pseudo-code of placement algorithm}
  \begin{verbatim}
  from all threads:
    select #core highest LLC miss rate
    select #core highest exec-time
    intersection of both are critical threads
    if threads placed on different cores
      then do nothing
    else
      move higher LLC miss rate thread to other core
    do accounting

  forall threads left do:
    bin by priority levels
    sort each bin by miss rate

  forall prio-bin in prio-bin-list do:
    while threads in prio-bin
      dequeue highest miss rate
      sort cores by lowest accounted miss rate
      place max(#core, #threads left in bin) threads RR on cores;
  \end{verbatim}

  \paragraph{\gls{smt} abstraction code}
  \begin{verbatim}
  if SMT is enabled
    sort threads once by exec time and once by LLC miss
    while duplication:
      look at next LLC-miss thread and dequeue it from exec-time
      look at next exec-miss thread and dequeue it from LLC-miss

    while threads unassigned && queue not empty:
      dequeue one thread from LLC miss list for SMT#0
      dequeue one thread from LLC-miss list for SMT#1
      dequeue one thread from exe-time list for SMT#0
      dequeue one thread form exec-time list for SMT#1
  \end{verbatim}

  \paragraph{Minimize migration pseudo-code}
  \begin{verbatim}
  sort all threads by LLC-miss
  sliding window size #threads with less than 5% LLC miss difference
  if at least 2 threads in the current window are migrated
    if two threads are swaping cores
      don't do the migration
    ALTERNATIVELY
    if the from-core-to-core-matrix has entries on opposing fields
      swap the to-values of both entries
  \end{verbatim}

\end{comment}
