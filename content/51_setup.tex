% vim:set ft=tex:
\section{Benchmark and Measurement Setup}

In the following, I describe the benchmark applications and the
system environment, the benchmarks run in.

\paragraph{Benchmarks.}
I use four single threaded applications from the \emph{SPEC CPU2006} benchmarking
suit\footnote{\url{http://spec.org/cpu2006/}}:
namely, 403.gcc, 416.gamess, 429.mcf, and 470.lbm.
They display the algorithm's performance under load.

416.gamess is a floating point benchmark written in Fortran executing a quantum
chemistry simulation.
It is a compute-bound benchmark and generates close to zero \gls{mpc}.
The 403.gcc integer benchmark compiles a fixed amount of source code.
It has nine different input workloads, from which I selected the 166.i
workload.
The benchmark is also compute-bound and generates a low number of \gls{mpc}.
470.lbm simulates fluid dynamics and stresses the floating point units.
It generates a high amount of \gls{mpc}, therefore it is memory-bound.
429.mcf generates the highest number of \gls{mpc}.
It computes a combinatorial optimization using the integer units.
The benchmark set consists of two compute-bound and two memory-bound
benchmarks, with one integer and one floating point benchmark within each group.

\begin{comment}
Besides the SPEC benchmarks, I employ two other applications to proof the
group configuration awareness of the load balancer one the one side, and the
performance benefits for such a configuration on the other side.
To that end, I use a multi-threaded matrix multiplication for a matrix with
one thousand entries and a client-server application printing the number of the
core it currently runs on.

The latter benchmark shows that client and server are migrated together, as
long as there is only one client per task.
If a task spawn several threads, the balancer distributes the additional
threads to other cores.
\end{comment}


\paragraph{Algorithms.}
I will measure the performance of four different algorithms I introduced in
section \ref{impl:algos}.
The simple load distribution (SLD) algorithm distributes each new thread to the
core with the largest amount of idle time.
It does no further migrations.

The space-time-balance (STB) algorithm observes temporal and spatial thread
behaviour.
It distributes the threads with the highest memory usage to different caches
and balances the rest depending on their consumed execution time during the
last interval.

The third algorithm is a small adaption of the space-time-balance algorithm.
It omits the spatial balancing and solely performs temporal balancing (TB).

The fourth and last algorithm differs from the above, as it runs in
conjunction with the kernel-level scheduler; it is Linux's \gls{cfs}.


\paragraph{Time Measurement.}
For each application I measure the time it takes to start the application,
execute all threads to completion, and exit.
I repeat the measurement one hundred times, or if several benchmarks run
concurrently, until the slowest benchmark has repeated one hundred times.
Consequently, fast benchmarks -- 403.gcc and  416.gamess -- provide more
measurements than slow benchmarks -- 429.mcf and 470.lbm.

On L4Re, I use the Ned startup script to measure and output the times for
each application.
On Linux, I use a ruby script to measure the durations and to store them in a
file for each application.

Furthermore, I do not use the time stamp counter (TSC), as this counter depends
on the core the time stamp is taken on.
Instead I use \texttt{clock\_gettime(CLOCK\_MONOTONIC)} on Linux and
\texttt{l4\_kip\_clock(l4re\_kip())} on L4Re.


\paragraph{64bit Issue.}
Running the benchmarks on 64bit, raises an issue in the memory subsystem of L4Re.
Memory allocation and deallocation increases the run-time of 403.gcc by several
seconds for each repetition.
The load balancing algorithm is not the cause for this behaviour, as it also
presents itself if no load balancing is done.
The task runs to completion and then restarts, so I expect each allocated
page to be free after completion.
The L4Re internal memory management should now merge free chunks of memory.
However, the size of the list of free memory areas increases constantly and
shows allocated pages in between, preventing compaction.
I suspect the time spent iterating through the growing free list to search for
possible compactions or chunk sizes fitting a memory request to be the cause
for the increase in run-time.

This issue does not visibly affect the run-time of 416.gamess, but the other memory-bound
benchmarks are affected as well.
429.mcf shows slight but constant increase with each run.
429.mcf allocates all memory at the beginning of the benchmark, so the memory
subsystem is not used constantly, as is the case for 403.gcc.

When 416.gamess, 429.mcf, and 470.lbm run together on L4Re the run-times
increase occasionally between 2.5 and 4 times.
This increase cannot solely be explained by bad load balancing decisions.
Also, the average run-time for the first 50 iterations of 470.lbm is 60 seconds
lower than the average run-time of the second 50 iterations.
Minimum, maximum and median run-time show a similar difference.

Providing a different memory allocator to each task via the ned script,
did not eliminate this problem.
As this issue did not present itself for 32bit, all benchmarks are executed
in a 32bit environment. This issue will be addressed by the L4Re developers.


\begin{comment}
\paragraph{Benchmarks.}
SPEC progs:
solo runs: SLD, STB, MIPC, CFS
group runs: SLD, STB, MIPC, CFS
--> degradation of median compared  to solo runs

Group runs for different SMT algos: RR, load,
Group runs for different MIPC assignments: load, mpc-ipc

pingpong  clsvr group;
openmp-mmul distribution group; -- load generated by other mmul or fractal



\paragraph{Group Configuration Benchmarks}
The first is a simple \gls{ipc} ping-pong, where the server answers a call from
the client with the number of the core it is currently running on.
The client measures the time for the \gls{ipc} call and prints the answer of
the server together with its own core number and the \gls{ipc} duration.

\end{comment}
