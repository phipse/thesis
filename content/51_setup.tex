% vim:set ft=tex:
\section{Benchmark and Measurement Setup}

In the following, I describe the benchmark applications and the
system environment, the benchmarks run in.

\paragraph{Benchmarks.}
I use four applications from the \emph{SPEC CPU2006} benchmarking
suit\footnote{\url{http://spec.org/cpu2006/}}:
namely, 403.gcc, 416.gamess, 429.mcf, and 470.lbm.
They will display the algorithms performance under load.

416.gamess is a floating point benchmark written in Fortran executing a quantum
chemistry simulation.
It is a compute-bound benchmark and generates close to zero \gls{mpc}.
The 403.gcc integer benchmark compiles a fixed amount of source code.
It has nine different input workloads, from which I selected the 166.i
workload.
The benchmark is also compute-bound and generates a low number of \gls{mpc}.
470.lbm simulates fluid dynamics and stresses the floating point units.
It generates a high amount of \gls{mpc}, therefore it is memory-bound.
429.mcf generates the highest number of \gls{mpc}.
It computes a combinatorial optimization using the integer units.
The benchmark set consists of two compute-bound and two memory-bound
benchmarks, with one integer and one floating point benchmark each.

Besides the SPEC benchmarks, I employ two other applications to proof the
group configuration awareness of the load balancer one the one side, and the
performance benefits for such a configuration on the other side.
To that end, I use a multi-threaded matrix multiplication for a matrix with
one thousand entries and a client-server application printing the number of the
core it currently runs on.

The latter benchmark shows that client and server are migrated together, as
long as there is only one client per task.
If a task spawn several threads, the balancer distributes the additional
threads to other cores.


\paragraph{Algorithms.}
I will measure the performance of four different algorithms.
The simple load distribution algorithms introduced in \ref{impl:algos},
distributes each new thread to the core with the largest amount of idle time.
It does no further migrations.

The space-time-balance algorithm, also introduced in \ref{impl:algos},
observes temporal and spatial thread behaviour.
It distributes the threads with the highest memory usage to different caches
and balances the rest depending on their execution time.

The third algorithm is a small adaption of the space-time-balance algorithm.
It just omits the spatial balancing part and only performs temporal balancing.

The fourth and last algorithm different from the above, as it runs in
conjunction with the scheduler at kernel level; it is Linux's \gls{cfs}.


\paragraph{Time Measurement.}
For each application I measure the time it takes to start the application,
execute it until completion, and exit it again.
I repeat the measurement one hundred times, or if several benchmarks run
concurrently, until the slowest benchmark has repeated one hundred times.
Consequently, fast benchmarks -- 403.gcc and  416.gamess -- provide more
measurements than slow benchmarks -- 429.mcf and 470.lbm.

On L4Re, I use the Ned startup script to measure and output the times for
each application.
On Linux, I use a ruby script to measure the durations and to store them in a
file for each application.

Furthermore, I do not use the time stamp counter (TSC), as this counter depends
on the core the time stamp is taken on.
Instead I use \texttt{clock\_gettime(CLOCK\_MONOTONIC)} on Linux and
\texttt{l4\_kip\_clock(l4re\_kip())} on L4Re.


\paragraph{64bit Issue.}
Running the benchmarks on 64bit, raises an issue in the memory subsystem of L4Re.
Memory allocation and deallocation increases the runtime of 403.gcc by several
seconds for each repetition, when it runs alone in the system.
It is irrelevant if a simple load distribution, space-time-balancing,
or no load balancing runs the benchmark.
The task is run to completion and then restarted, so I expect each allocated
page to be free after completion.
The L4Re internal memory management should now merge free chunks of memory.
However, the size of the list of free memory areas increases constantly and
shows allocated pages in between, preventing compaction.
I suspect the time spent to search for possible merges and to iterate through
the list to be the cause for the increase in runtime.

This issue does not affect the runtime of 416.games, but the other memory-bound
benchmarks are affected as well.
429.mcf shows slight but constant increase with each run.
429.mcf allocates all memory at the beginning of the benchmark, so the memory
subsystem is not used constantly, as is the case for 403.gcc.

When 416.gamess, 429.mcf, and 470.lbm run together on L4Re the runtimes
increase occasionally between 2.5 and 4 times.
This increase cannot solely be explained by bad load balancing decisions.
Also, the average runtime for the first 50 iterations of 470.lbm is 60 seconds
lower than the average runtime of the second 50 iterations.
Minimum, maximum and median runtime show a similar difference.

Providing a different memory allocator to each task created via the ned script,
did not eliminate this problem.

As this issue did not present itself for 32bit, all benchmarks are executed
in a 32bit environment.


\begin{comment}
\paragraph{Benchmarks.}
SPEC progs:
solo runs: SLD, STB, MIPC, CFS
group runs: SLD, STB, MIPC, CFS
--> degradation of median compared  to solo runs

Group runs for different SMT algos: RR, load,
Group runs for different MIPC assignments: load, mpc-ipc

pingpong  clsvr group;
openmp-mmul distribution group; -- load generated by other mmul or fractal



\paragraph{Group Configuration Benchmarks}
The first is a simple \gls{ipc} ping-pong, where the server answers a call from
the client with the number of the core it is currently running on.
The client measures the time for the \gls{ipc} call and prints the answer of
the server together with its own core number and the \gls{ipc} duration.

\end{comment}
