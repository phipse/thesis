% vim:set ft=tex:
\section{Measurements}
\label{eval:measurements}

I measure three different constellations of the SPEC applications: solo, load,
and heavy load.
The solo execution time of each application serves as baseline to compare the
performance of different algorithms in a low load situation, and to enable the
computation of the slowdown when run in parallel with other applications.
I use the terms execution time and run-time synonymously and they describe the
duration in wall clock time units between two measurements.
I described the measurement process in the previous section.

As a means to visualize the run-time measurements, I chose box plots.
However, due to the nature of the box plots, data points marked as outliers need
to be taken with a grain of salt.
Box plots have six components: median, lower and upper quartile, lower and
upper whisker, and outliers.
The distance between the lower and upper quartile values is the \emph{inter
quartile distance} or IQD.
Depending on the IQD, the lower and upper whiskers are computed as follows:
%
\begin{align*}
  \text{lower whisker} &= \text{lower quartile} - 1.5 * \text{IQD}\\
  %
  \text{upper whisker} &= \text{upper quartile} + 1.5 * \text{IQD}
\end{align*}
%
The actual value of the lower whisker is the value of the next data point
that is higher than the computed value.
For the upper whisker it is the next lower-value data point.
Box plots consider every value beyond the value of the lower or upper whisker
as outlier.

The issue arises, if the IQD is low and therefore, the whisker values are close
to their respective quartile value.
Then a lot of data points can be marked as outliers.
While they deviate from the bulk of run-time values, the actual distance
is not necessarily high.
Consequently, a high number of marked outliers is not necessarily a sign of
high deviation.

\paragraph{Cost of Balancing.}
Balancing can be expensive run-time wise.
If it migrates threads with a large cache working set to another physical core,
the threads need to reload their working set into the core local caches.
Furthermore, the MPDE cycle needs processor time.
Still, the housekeeping during the cycle can be expensive.
Because threads can leave at any time, the load balancer must check their
presence multiple times and act on failing system calls.
I believe optimizations to the source code can reduce the time, but currently,
the MPDE cycle takes around $55$\textmu{}s when managing one thread,
$170$\textmu{}s when managing ten threads.
However, I observed that this cycle can take up to two orders of magnitude
longer due to housekeeping issues.
The cycle duration depends on the number of threads the load
balancer manages and not on the length of a balancing interval.
A shorter balancing interval, however, raises the total overhead of the load
balancer, as the MPDE cycle is executed more often.
The overhead analysis raised questions about the implementation.
In the worst-case, an MPDE cycle can take up to $1.5$s.
As the measurement component gathers each thread's performance data, it has to
send a message to the core, that currently runs this thread.
This leads to several cross-core messages and the answer is also delayed, as
the request has the same priority as the running threads.
To improve this, the load balancer must employ one measurement thread per core
with a higher priority.

In the following, I analyse, how many cache misses must occur, to be the reason
for a one second run-time increase of an application, when running alone on the load
balancer.
Intel documentation (\cite{intel_perf_analysis_2009}) states $60$ns for an
\gls{llc} miss, independent
measurements\footnote{\url{http://www.7-cpu.com/cpu/Haswell.html}}
suggest $70$ns, which accounts for a difference of $3$ to $4$ cycles on a $3$GHz CPU
compared to the Intel statement.

The maximum amount of misses per second is $10^9/70 \approx 14285714$ misses.
During one interval of $100$ms $10^8/70 \approx 1428571$ or $10$ms $10^7/70 \approx
142857$ misses can be generated.

The memory-heavy Mcf application runs for $90$ seconds.
When its run-time is increased by one second due to bad balancing decisions and
subsequent cache misses, it suffers on average an additional
$14285714 / 90 \approx 158730$ cache misses per second during its run-time.
Relative to an interval length of $100$ms that means $15873$ misses per interval.
The cache size of the core local L1 and L2 caches is $64 + 256 = 320$KB or
$5120$ cache lines \`a $64$Byte.
Producing $15873$ misses during an 100ms interval would mean to flush the
whole cache $3.1$ times.
One cache miss takes $70$ns, hence flushing the whole cache $3.1$ times takes
$5120 * 3.1 * 70ns = 1111040$ns or $1.11104$ms.
So, if Mcf suffers a slowdown of one second it has to spend $1.1$ms each balancing
interval on additional cache misses.

However, forcing a core switch every $100$ms might cause losing a whole cache set
once, but not thrice.
This discussion is also simplified, as I discuss last level cache misses
and only account for core local caches.
When a core switch occurs, it is likely that the data can be fetched from the
\gls{llc} as it is an inclusive cache.
A successful \gls{llc} access takes approximately $13$ns.
Assuming reuse of all $5120$ cache lines, fetching them takes $66560$ns or
$0.06656$ms.
That leaves around $1.05$ms per balancing interval to account for.

This concludes that additional cache misses due to load balancing decisions cannot
be the sole reason for run-time deviation as observed in figure
\ref{eval:fig:box_solo}.
% todo SMT measurement issue: need micro benchmark to determine increase in
% application performance, due to smt balancing decisions.

\begin{figure}[!ht]
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/finalPlots/boxplots/pdf/boxplot_gcc_mcf_solo}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/finalPlots/boxplots/pdf/boxplot_gamess_lbm_solo}
  \end{subfigure}
  \caption{Solo run-times visualized using one box plot for each balancing
    algorithm; STB and TB use a $100$ms balancing interval.
    From top left to bottom right: SPEC Gcc, Mcf, Gamess, and Lbm.}
  \label{eval:fig:box_solo}
\end{figure}

\paragraph{Solo Execution.}
The solo run-times for each benchmark provide insight into the minimal
execution time and the program behaviour throughout different runs.
However, the benchmark is not the program under test, the balancing algorithm
is.
SLD achieves the minimal possible run-times, because it does not
migrate any running application and produces no additional overhead through
measurement and balancing action.
The green box plots in figure \ref{eval:fig:box_solo} show that nearly all SLD
run-times lie within a tenth of a second of each other.
The very deterministic run-time suggests that SLD schedules the benchmark on
the same core each run.
Also, compared to the violet and blue plots of STB and TB, the SLD run-times
are either as good as the minimal run-time of the other two or better.


\begin{figure}[!ht]
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/10msPlots/boxplots/pdf/boxplot_gcc_mcf_solo_10ms}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/10msPlots/boxplots/pdf/boxplot_gamess_lbm_solo_10ms}
  \end{subfigure}
  \caption{Solo run-times visualized using one box plot for each balancing
    algorithm; STB and TB use a $10$ms balancing interval.
    From top left to bottom right: SPEC Gcc, Mcf, Gamess, and Lbm.}
  \label{eval:fig:box_solo_10ms}
\end{figure}


\Gls{cfs} on the other hand is significantly faster, except for Gamess.
The gap for the memory-bound benchmarks is smaller than the gap for
the compute-bound Gcc.
\Gls{cfs} dynamically adjusts the time slices of the running program to reduce
the scheduling overhead.
Furthermore, the amount of time, money, and effort the Linux community put into
scheduling is reflected in these numbers.
\\

The results of STB and TB display equally fast run-times for the compute-bound
benchmarks.
However, the run-times of the memory-bound benchmarks raise questions about the
amount of migrations and the overhead of the load balancer.
Between figure \ref{eval:fig:box_solo} and figure \ref{eval:fig:box_solo_10ms}
lies a tenfold increase in overhead, as the interval duration changes from
$100$ms to $10$ms.
In case of Mcf, STB displays better run-times for best-case, worst-case, and
median, but worse run-times at all three points for Lbm.
TB's results for Mcf improved as well.
Whereas Lbm's run-times under TB balancing decreased in median and best-case,
they increased in upper quartile and worst-case.
If the tenfold increase in overhead would be a problem, the best-case run-time
would not have improved.
Due to the increased number of measurements, the measurement-time spikes during
the MPDE cycle occur more frequent, thus affects more application run-time
measurements.
Consequently, the upper quartile and worst-case run-times increase.
The number of repetitions must be increased to provide a better view on the
run-time deviations.
Additionally, the implementation must change to eliminate these spikes, as I
suggested above.

Solo runs give insights into the behaviour of the algorithms and provide
a base line to compare load runs to.


\paragraph{Concurrent Execution.}
As large as the gap between \gls{cfs} and the other balancing algorithms
sometimes is in the solo execution case, the concurrent case provides better results.
All measurements in figure \ref{eval:fig:box_all} are taken during the same
benchmarking run of the specific balancer.
Thus, the execution times of the different applications are connected, but
determining if worst case time of one benchmark correlates with best case times
of another, needs further analysis.
Which the time budget does not allow for.

At this point, I want to recall the above-mentioned connection between outliers
and the inter quartile distance in box plots.

While the performance of SLD for a single application was on a par with TB and
STB, it is no competition for the two if the system is loaded.
Figure \ref{eval:fig:box_all} depicts the results.
The inner quartile distance for Mcf, Gamess, and Lbm covers nearly the whole
deviation span of the other algorithms, and for Gcc SLD shows an already large
IQD and upper whisker value, but still a high number of bad performing outliers.

The median run-time is as good as the other algorithms', but SLD's worst case
is more than two times worse than its median.
Furthermore, in all benchmarks the distance between median and upper quartile
is much larger than the distance between lower quartile and median.
This asymmetry displays the unpredictability of the SLD placement.
In summary, SLD provides non-deterministic performance for loaded systems with a
high risk to generate a suboptimal thread-to-core mapping.
%
\begin{figure}[h!]
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/finalPlots/boxplots/pdf/boxplot_gcc_mcf_all}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/finalPlots/boxplots/pdf/boxplot_gamess_lbm_all}
  \end{subfigure}
  \caption{Concurrent run-times visualized using one box plot for each balancing
    algorithm; STB and TB use a $100$ms balancing interval and all time
    measurements are taken from the same concurrent run.
    From top left to bottom right: SPEC Gcc, Mcf, Gamess, and Lbm.
    }
    \label{eval:fig:box_all}
\end{figure}
%
\begin{figure}[!ht]
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/10msPlots/boxplots/pdf/boxplot_gcc_mcf_all_10ms}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/10msPlots/boxplots/pdf/boxplot_gamess_lbm_all_10ms}
  \end{subfigure}
  \caption{Concurrent run-times visualized using one box plot for each balancing
    algorithm; STB and TB use a $10$ms balancing interval and all time
    measurements are taken from the same concurrent run.
    From top left to bottom right: SPEC Gcc, Mcf, Gamess, and Lbm.
    }
  \label{eval:fig:box_all_10ms}
\end{figure}
\\


\Gls{cfs} on the other hand, performs well, but it is not as superior as during
the solo benchmarks.
Regarding minimum and maximum run-times, it performs better than TB and STB in
case of Gcc and Lbm and equally well for Mcf and Gamess.
Gamess has a lot of outliers, which partly can be accounted to the computation
of box plots.
However, the marks above $75$ seconds are more than $1.5$ times slower than the
median run-time, which points to other issues with this workload.
% todo why are they a problem; where do they come from; conjunction with other
% benchmarks;
% heavy migration, possible, but close to no cache working set, therefore no
% high miss rates after migration;
% Guess: coscheduled with Lbm, stress on floating point unit; -- unlikely
% Guess: coscheduling issue, as stb has less worst case and slightly better best case
%  . . .

I do not consider the printed outliers in case of Gcc as a problem, as
they are within a $1.5$ times median run-time range and compared to the other
algorithms, they are below the upper quartile run-time of STB and TB.
However, CFS is not the focus of this thesis, hence, I do not further
investigate its issues with Gamess and Gcc.
\\

The focus of this thesis lies in behaviour awareness, which the STB and TB
algorithms implement.
In difference to SLD, both perform a MPDE cycle every balancing interval.
The interval length in figure \ref{eval:fig:box_all} is $100$ms, whereas figure
\ref{eval:fig:box_all_10ms} shows a $10$ms interval.

In both figures, TB's Gamess shows a behaviour similar to CFS, while the
outliers in the $10$ms case are less and not as high.
Considering all benchmarks, TB performs better with a short interval duration:
less deviation, lower worst-case execution times, and the memory-bound
applications also show better best-case run-times.

STB offers more of a mixed picture: less deviation as well, but the median
run-time slightly increased for all applications along with an increase in
worst-case run-times.
However, when STB and TB are compared regarding deviation of all benchmarks,
STB displays more reliable results.
STB provides each application with a fair share of resources depending on the
application's run-time behaviour.
As the measurements of all applications were taken during the same run, the
best-case run-time of one is possibly connected to the worst-case run-time of
another, hence a fair load balancer provides an equal share of resources
during each execution.


\paragraph{Heavy Load.}
On a quad-core machine, the four SPEC benchmarks are sufficient to fully
utilize the processor.
However, the quad-core machine with \gls{ht} has still some underutilized
resources, thus I double the workload by running each benchmark twice.
\cite{zhuravlev_addressing_2010} also turns to this approach to fully utilize
the Xeon server processor with eight physical cores, where two cores share
one \gls{llc} each.

Figure \ref{eval:fig:box_heavy} shows the results for a $100ms$ load balancing
interval for STB and TB. SLD is not part of this measurement, as the
performance shown under less load is not competitive.

Although I run two instances of each benchmark, I only use the numbers of one
instance.
The results in figure \ref{eval:fig:box_heavy} are consistent with figure
\ref{eval:fig:box_all} in the following two ways:
First, the run-time deviation is much lower for STB compared to TB;
second, the median run-time is better in case of Gcc, but worse for the other
three applications.
However, the worst-case performance of STB is consistently better.
It is not evident from the bottom-left plot whether STB is better; the
execution time of the top mark is $373.0$s.

Because the execution times of all applications are connected, the worse
best-case performance of one application benefits the better worst-case
performance of the other.
It is preferable to improve on the median execution time of all benchmarks and
I believe there is room for algorithmic improvements.
However, a high worst-case execution time in one long running application like
Mcf or Lbm, allows for a
several good run-times in the short running Gcc or Gamess; Lbm's median is
three times as high as the Gcc's or Gamess's median.
So, the goal besides the numbers is to provide an environment where each
application uses as much resources as needed to perform well, while not causing
severe performance degradation to other applications.
\\

\begin{figure}[h!]
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/finalPlots/boxplots/pdf/boxplot_gcc_mcf_heavy}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/finalPlots/boxplots/pdf/boxplot_gamess_lbm_heavy}
  \end{subfigure}
  \caption{Concurrent run-times visualized using one box plot for each balancing
    algorithm; from top left to bottom right: SPEC Gcc, Mcf, Gamess, and Lbm.
    Time measurements are taken from the same concurrent run.}
    \label{eval:fig:box_heavy}
\end{figure}

\begin{comment}
\paragraph{Gamess}
Before I discuss the overall performance, I want to focus on the performance of
Gamess.
On Linux the benchmark spawns one thread and runs on one core, on L4Re Gamess
spawns four threads: on L4Re, every program has a L4Re-kernel thread, the
second is likely the pthread manager thread, and the third the worker thread,
leaving one unaccounted for.
In difference to Linux, Gamess prints a lot of data to console on L4Re, but as
the application is written in Fortran, maintaining it is an issue.
The measured solo run-times are consistently $55$ seconds independent of the
load balancing algorithm.
However, if Gamess runs concurrently with other applications, the best-case
run-time is $47.2$ seconds, so Gamess executed nearly eight seconds faster.
$47.2$ would $0.6$ seconds slower than CFS, which is a reasonable distance, but
where does this eight second gap come from?
Dynamic voltage and frequency scaling? Turned off.
Turbo Boost? Turned off.
Looking at the numbers, over 60 of 291 measurements are below 50 seconds under
load, so this is no coincidence either.
Scheduled on core zero or its companion core? Already unlikely for STB and TB,
but an additional tests showed, that running it without a load balancer on core
zero also takes $55$ seconds.
Also, using the load balancer to pin all threads to one core neither improves
the run-time.
The reason behind the execution time decrease under load remains an open
question.
\end{comment}

\paragraph{Overall Performance.}
I compute the overall performance of each load balancing scheme as average
run-time increase relative to average solo run-time.
Table \ref{eval:tbl:exec_inc} shows the average run-time increase per
application per balancer.
One group of bars in figure \ref{eval:fig:exec_inc} displays the differences
between all balancers per application.
On average CFS and TB perform equally well, whereas TB is 0.06\% slower.
However, in the high load scenario CFS's performance is due to the performance
of Gamess significantly worse than TB's or STB's.
In contrast to the box plots where outliers are clearly marked, these numbers
do incorporate all measurements, including the extreme
run-times of Gamess under CFS displayed in figure \ref{eval:fig:box_heavy}.
They also do not reflect the run-time deviation across multiple repetitions.
%
\begin{table}[h!]
  \includestandalone[mode=tex,width=\textwidth]{images/runtimeIncrease/table_load}
  \caption{Average increase in run-time per application and overall; for
    normal load on the left and high load on the right.}
  \label{eval:tbl:exec_inc}
\end{table}
%
\begin{figure}[h!]
  \begin{subfigure}{.49\textwidth}
  \includegraphics[width=\textwidth]{images/runtimeIncrease/barchart_load_inc}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \includegraphics[width=\textwidth]{images/runtimeIncrease/barchart_heavy_inc}
  \end{subfigure}
  \caption{Visualized difference between the balancing algorithms per
    application and overall; for normal load on the left and high load on the
    right.}
  \label{eval:fig:exec_inc}
\end{figure}
