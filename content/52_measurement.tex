% vim:set ft=tex:
\section{Measurements}
\label{eval:measurements}

I measure each of the four SPEC benchmarks twice; once running alone in the
system and once, running under load in parallel with the three other benchmarks.
The solo execution time of each application serves as baseline to compare the
performance of different algorithms in a low load situation, and to enable the
computation of the slowdown when run in parallel with other applications.
The slowdown of the computation time is computed for the best case and worst
case, the lower and upper quartile, and the median.
The result provides a complete picture of the algorithms' performance.

I also compare the algorithms using box plots to visualize
the run-time for the major part of the measurements together with outliers.
However, due to the nature of the box plots, data points marked as outlier need
to be taken with a grain of salt.
Box plots have six components: median, lower and upper quartile, lower and
upper whisker, and outliers.
Median and lower and upper quartile are actual values present in the data set,
and mark 50\%, 25\%, and 75\% of the sorted data.
The distance between the lower and upper quartile values is the \emph{inter
quartile distance} or IQD.
The lower and upper whiskers are computed as follows:
%
\begin{align*}
  \text{lower whisker} &= \text{lower quartile} - 1.5 * \text{IQD}\\
  %
  \text{upper whisker} &= \text{upper quartile} + 1.5 * \text{IQD}
\end{align*}
%
The actual value of the lower whisker is the value of the next data point,
that is higher than the computed value.
For the upper whisker its the next lower value data point.
Box plots consider every value beyond the value of the lower or upper whisker
as outlier.

Now the issue arises, if the inter quartile distance is low and therefore,
the whisker values are close to the respective quartile value, which leads to
lots of data points to be visualized as outliers.
For the cases, where this issue displays itself, I provide histograms to
analyse the overall distribution.

% todo SMT measurement issue: need micro benchmark to determine increase in
% application performance, due to smt balancing decisions.

\paragraph{Solo Execution.}
The solo run-times for each benchmark provide insight into the minimal
execution time and the program behaviour throughout different runs.
However, the application is not the program under test, it is the balancing
algorithm.
I consider SLD to achieve the minimal possible run-times, because it does not
migrate any running application.
The green box plots in figure \ref{eval:fig:box_solo} show that all SLD run-times
lie within a tenth of a second of each other.
The very deterministic run-time suggests, that SLD schedules the benchmark on
the same core each run.

Also, compared to the violet and blue plots of STB and TB, the SLD run-times
are mostly consistent with the minimal run-time of the other two.
In case of lbm, STB provides a better best case execution time, because SLD
most likely schedules the application on core 1, the \gls{ht} companion core of
core 0.
Due to the shared cache each call to L4Re services destroys parts of the cache
working set of lbm, hence the slight increase in run-time.
%
\begin{figure}[!ht]
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/finalPlots/boxplots/pdf/boxplot_gcc_mcf_solo}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/finalPlots/boxplots/pdf/boxplot_gamess_lbm_solo}
  \end{subfigure}
  \caption{Solo run-times visualized using one box plot for each balancing
    algorithm; from top left to bottom right: SPEC gcc, mcf, gamess, and lbm.}
    \label{eval:fig:box_solo}
\end{figure}
%
Does this account for one second difference?
A cache miss on a Haswell generation core i7 costs approximately
70ns\footnote{\url{http://www.7-cpu.com/cpu/Haswell.html}: \gls{llc} miss: 36
cycles on a 3Ghz machine $\approx$ 13ns, plus 57ns RAM access => 70ns}
(\cite{intel_perf_analysis_2009}).
This number varies with CPU and used memory, thus I approximate 100ns as an upper bound.
$1s  = 10^9ns \Rightarrow 10^9 / 70 = 14.3 * 10^6 \vee 10^9/100 = 10^7$.
So to generate a one second slowdown, between 10 and 14.3 million more misses must
be generated by SLD balancing.
Additionally, cross core communication latencies can account for some slowdown
as well, but as I expect the L4Re service to be running on core 0 and the
benchmark to be running on a different core in all solo benchmark cases, I deem
this unlikely.

To make a definitive statement a detailed analysis is necessary, for which time
constraints do not allow for.


\paragraph{CFS -- Another League.}
Why is \gls{cfs} so much faster than the Fiasco.OC scheduler?
\Gls{cfs} dynamically adjusts the time slices of the running programs to reduce
the amount of context switches.
Furthermore, the consistently worse run-time suggests, that this slowdown can
be accounted to the performance differences between monolithic kernels and
micro-kernels and other environmental differences.

However, the measurement for lbm provides a different picture:
the median is close to the upper quartile and the worst case run-time.
A closer look at the data reveals, that the run-times are actually split up
into two blocks: 129 and 133 seconds, with the 133 seconds block being larger.
At this point the box plot is misleading regarding the actual distribution.
STB and TB do not show such a split in run-time.
Additionally, the best case of STB is better than the median and a closer look
at the data shows, that this is not just one, but eleven measurements below
CFS's median.
Short above 10\% of the measurements do not look like a coincidence to me.
Unfortunately, lbm is a special case and the in comparison good performance
depends on the bad performance of \gls{cfs}.



\begin{comment}
\begin{figure}[ht!]
  \begin{subfigure}{.49\textwidth}
    \includegraphics[width=\textwidth]{images/finalPlots/barcharts/barchart_solo_gcc}
    \caption{SPEC GCC}
    \label{solo:gcc}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \includegraphics[width=\textwidth]{images/finalPlots/barcharts/barchart_solo_gamess}
    \caption{SPEC GAMESS}
    \label{solo:gamess}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \includegraphics[width=\textwidth]{images/finalPlots/barcharts/barchart_solo_lbm}
    \caption{SPEC LBM}
    \label{solo:lbm}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \includegraphics[width=\textwidth]{images/finalPlots/barcharts/barchart_solo_mcf}
    \caption{SPEC MCF}
    \label{solo:mcf}
  \end{subfigure}
  \caption{Solo run-times for each SPEC benchmark and each load balancer.
    Shown is the median over 100 repetitions together with minimum and maximum
    run-time.}
\end{figure}
\end{comment}

\paragraph{Concurrent Execution.}
As large as the gap between \gls{cfs} and the other balancing algorithms is in
the solo execution case, the concurrent case draws a much better picture.
All measurements in figure \ref{eval:fig:box_all} are taken during the same
benchmarking run of the specific balancer.
Thus the execution times are connected, but determining if worst case time of
one benchmark correlates with best case times of another, needs further
analysis, which, again, the time budget does not allow for.

At this point, I want to recall the above mentioned connection between outliers
and the inter quartile distance in box plots.

Where the performance of SLD for a single application was quite good, it is
downright ridiculous if the system is loaded, as is the case in figure
\ref{eval:fig:box_all}.
The median run-time is as good as for the other algorithms, but the worst case
is more than two times worse than the median, and in all benchmarks the
distance between median and upper quartile is more than twice as large, than the distance
between lower quartile and median.
In summary, SLD provides indeterministic performance for loaded systems with
the risk of up to two times longer run-times for specific applications.
%
\begin{figure}[h!]
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/finalPlots/boxplots/pdf/boxplot_gcc_mcf_all}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/finalPlots/boxplots/pdf/boxplot_gamess_lbm_all}
  \end{subfigure}
  \caption{Concurrent run-times visualized using one box plot for each balancing
    algorithm; from top left to bottom right: SPEC gcc, mcf, gamess, and lbm.
    Time measurements are taken from the same concurrent run.}
    \label{eval:fig:box_all}
\end{figure}
\\
%
\Gls{cfs} on the other hand, performs well, but it does not play in another
league anymore.
While it performs overall better for gcc and lbm, it performs equally well for
mcf and gamess.
Gamess has a lot of outliers, which partly can be accounted to the computation
of box plots on the other hand, but the marks above 75 seconds are more than
1.5 times slower than the median run-time.
% todo why are they a problem; where do they come from; conjunction with other
% benchmarks;
% heavy migration, possible, but close to no cache working set, therefore no
% high miss rates after migration;
% Guess: coscheduled with lbm, stress on floating point unit; -- unlikely
% Guess: coscheduling issue, as stb has less worst case and slightly better best case
%  . . .
I do not consider the printed outliers in case of gcc as a problem, because
they are within the 1.5 times median run-time range and compared to the other
algorithms, they are below their upper quartile run-time.
However, this is not a thesis about \gls{cfs}.
\\

The focus of this thesis lies on behaviour awareness, which the STB and TB
algorithms implement.
The run-time distribution when using STB is lower when compared to TB, thus it
provides more deterministic run-times for all benchmarks.
However, the determinism comes at the cost of higher median and best case
values, but with the benefit if better worst case performance.
While I expected a better median performance, the related work shows the
benefit of deterministic performance as well.


\paragraph{Overall Performance.}
Which scheduler performed best?
How can this be computed, when the amount of repetitions for gcc, gamess, and
mcf is different?
By weighing the repetition count by  median run-time?
Just using the sum of the run-times of 100 lbm repetitions?

