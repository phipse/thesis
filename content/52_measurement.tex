% vim:set ft=tex:
\section{Measurements}
\label{eval:measurements}

I measure three different constellations of the SPEC applications: solo, load,
and heavy load.
The solo execution time of each application serves as baseline to compare the
performance of different algorithms in a low load situation, and to enable the
computation of the slowdown when run in parallel with other applications.
The slowdown of the computation time is computed for the best and worst
case and the median.

I also compare the algorithms using box plots to visualize
the run-time for the major part of the measurements together with outliers.
However, due to the nature of the box plots, data points marked as outlier need
to be taken with a grain of salt.
Box plots have six components: median, lower and upper quartile, lower and
upper whisker, and outliers.
Lower quartile, median and upper quartile are actual values present in the data
set and mark 25\%, 50\%, and 75\% of the sorted data values.
The distance between the lower and upper quartile values is the \emph{inter
quartile distance} or IQD.
Depending on the IQD, the lower and upper whiskers are computed as follows:
%
\begin{align*}
  \text{lower whisker} &= \text{lower quartile} - 1.5 * \text{IQD}\\
  %
  \text{upper whisker} &= \text{upper quartile} + 1.5 * \text{IQD}
\end{align*}
%
The actual value of the lower whisker is the value of the next data point,
that is higher than the computed value.
For the upper whisker its the next lower-value data point.
Box plots consider every value beyond the value of the lower or upper whisker
as outlier.

Now the issue arises, if the inter quartile distance is low and therefore,
the whisker values are close to their respective quartile value, a lot of
data points can be marked as outliers.
While they deviate from the main amount of run-time values, the actual distance
is not necessarily high.
So a high number of marked outliers is not necessarily a sign of bad performance.

% todo SMT measurement issue: need micro benchmark to determine increase in
% application performance, due to smt balancing decisions.

\begin{figure}[!ht]
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/finalPlots/boxplots/pdf/boxplot_gcc_mcf_solo}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/finalPlots/boxplots/pdf/boxplot_gamess_lbm_solo}
  \end{subfigure}
  \caption{Solo run-times visualized using one box plot for each balancing
    algorithm; from top left to bottom right: SPEC gcc, mcf, gamess, and lbm.}
    \label{eval:fig:box_solo}
\end{figure}

\paragraph{Solo Execution.}
The solo run-times for each benchmark provide insight into the minimal
execution time and the program behaviour throughout different runs.
However, the benchmark is not the program under test, it is the balancing
algorithm.
I consider SLD to achieve the minimal possible run-times, because it does not
migrate any running application and produces no additional overhead through
measurement and balancing action.
The green box plots in figure \ref{eval:fig:box_solo} show that nearly all SLD
run-times lie within a tenth of a second of each other.
The very deterministic run-time suggests, that SLD schedules the benchmark on
the same core each run.
Also, compared to the violet and blue plots of STB and TB, the SLD run-times
are either as good as the minimal run-time of the other two or better.


%todo STB an TB solo performance discussion

\paragraph{CFS -- Another League.}
Why is \gls{cfs} so much faster than the Fiasco.OC scheduler?
\Gls{cfs} dynamically adjusts the time slices of the running programs to reduce
the amount of context switches.
Furthermore, the consistently worse run-time of all other load balancing
algorithms suggests, that this slowdown can also be accounted to the performance
differences between monolithic kernels and micro-kernels.

However, the measurement for lbm provides a different picture:
the median is close to the upper quartile and the worst case run-time.
A closer look at the data shown in figure \ref{eval:fig:lbm_cfs_solo} reveals,
that the run-times are actually split up into two blocks: 129 and 133 seconds,
with the 133 seconds block being larger.
At this point the box plot is misleading regarding the actual distribution.
STB and TB do not show such a split in run-time.
% todo reason for split, if this is still present after remeasure
Additionally, the best case of STB is better than the median of CFS and a closer look
at the data shows, that this is not just one, but eleven measurements below
CFS's median.
Short above 10\% of the measurements does not look like a coincidence.
Unfortunately, lbm is a special case and the in comparison good performance
depends on the bad performance of \gls{cfs}.

\begin{figure}[!ht]
  \setcapindent*{1em}
  \begin{captionbeside}[]{Histogram of the run-time distribution of lbm running
    on CFS. The run-times are divided into two groups around 129 and 133
    seconds, with the latter group being larger.}
    \includegraphics[width=0.6\textwidth]{images/linuxPlots/pdf/lbm_2016-02-06_5-18_solo_run_plot.pdf}
  \end{captionbeside}
  \label{eval:fig:lbm_cfs_solo}
\end{figure}



\begin{comment}
\begin{figure}[ht!]
  \begin{subfigure}{.49\textwidth}
    \includegraphics[width=\textwidth]{images/finalPlots/barcharts/barchart_solo_gcc}
    \caption{SPEC GCC}
    \label{solo:gcc}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \includegraphics[width=\textwidth]{images/finalPlots/barcharts/barchart_solo_gamess}
    \caption{SPEC GAMESS}
    \label{solo:gamess}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \includegraphics[width=\textwidth]{images/finalPlots/barcharts/barchart_solo_lbm}
    \caption{SPEC LBM}
    \label{solo:lbm}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \includegraphics[width=\textwidth]{images/finalPlots/barcharts/barchart_solo_mcf}
    \caption{SPEC MCF}
    \label{solo:mcf}
  \end{subfigure}
  \caption{Solo run-times for each SPEC benchmark and each load balancer.
    Shown is the median over 100 repetitions together with minimum and maximum
    run-time.}
\end{figure}
\end{comment}

\paragraph{Concurrent Execution.}
As large as the gap between \gls{cfs} and the other balancing algorithms is in
the solo execution case, the concurrent case shows a much better picture.
All measurements in figure \ref{eval:fig:box_all} are taken during the same
benchmarking run of the specific balancer.
Thus, the execution times of the different applications are connected, but
determining if worst case time of
one benchmark correlates with best case times of another, needs further
analysis, which, again, the time budget does not allow for.

At this point, I want to recall the above mentioned connection between outliers
and the inter quartile distance in box plots.

Where the performance of SLD for a single application was quite good, it is
downright ridiculous if the system is loaded, as is the case in figure
\ref{eval:fig:box_all}.
The inner quartile distance for mcf, gamess, and lbm covers nearly the whole
deviation span of the other algorithms, and for gcc SLD shows an already large
IQD and upper whisker value, but still a high number of bad performing outliers.

The median run-time is as good as the other algorithms', but SLD's worst case
is more than two times worse than its median.
Furthermore, in all benchmarks is the distance between median and upper quartile
much larger than the distance between lower quartile and median.
This asymmetry shows the unpredictability of the SLD placement.
In summary, SLD provides indeterministic performance for loaded systems with a
high risk to generate a bad performing thread-to-core mapping.
%
\begin{figure}[h!]
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/finalPlots/boxplots/pdf/boxplot_gcc_mcf_all}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/finalPlots/boxplots/pdf/boxplot_gamess_lbm_all}
  \end{subfigure}
  \caption{Concurrent run-times visualized using one box plot for each balancing
    algorithm; from top left to bottom right: SPEC gcc, mcf, gamess, and lbm.
    Time measurements are taken from the same concurrent run.}
    \label{eval:fig:box_all}
\end{figure}
\\

\Gls{cfs} on the other hand, performs well, but it does not play in another
league anymore.
Regarding minimum and maximum run-times, it performs better than TB and STB in
case of gcc and lbm and equally well for mcf and gamess.
Gamess has a lot of outliers, which partly can be accounted to the computation
of box plots.
However, the marks above 75 seconds are more than 1.5 times slower than the
median run-time, which points to other issues with this workload.
% todo why are they a problem; where do they come from; conjunction with other
% benchmarks;
% heavy migration, possible, but close to no cache working set, therefore no
% high miss rates after migration;
% Guess: coscheduled with lbm, stress on floating point unit; -- unlikely
% Guess: coscheduling issue, as stb has less worst case and slightly better best case
%  . . .

I do not consider the printed outliers in case of gcc as a problem, because
they are within the 1.5 times median run-time range and compared to the other
algorithms, they are below their upper quartile run-time.
However, CFS is not the focus of this thesis, hence, I do not further
investigate its issues with gamess and gcc.
\\

The focus of this thesis lies on behaviour awareness, which the STB and TB
algorithms implement.
In difference to SLD, both perform a information gathering and balancing cycle
every interval.
% todo this is also true for the solo execution case and explains some of the
% overhead there as well, so move up.
The default interval length is 100 ms to be able to respond to workload
changes while keeping the overhead low.
The size of the overhead depends on the number of threads managed by the load
balancer, because for each thread it executes several system calls to gather
measurements and check if the thread is still present in the system.
For around ten threads measurement, housekeeping, and balancing takes 100µs on
average or 0.1\% of the default interval length.
The interval length does not affect the duration of measurements and balancing
decisions, just the ratio changes.

% todo Transition

0.1\% of 41 seconds are 0.41 seconds, which reduces the gap between TB and SLD
for gcc to 0.3 seconds.
0.3 seconds account on Haswell for 3 to 4.38 million cache misses, a core runs
at 3GHz thus performs $3* 10^9$ cycles per second, leading to around $41 * 3 *
10^9$ overall cycles performed during a gcc run.
After doing the math, this leads to one miss ever 28000 to 41000 cycles or one
miss every 13.6µs, thus estimated 7350 misses per 100ms interval.
It seems a bit high, that migrations during the runtime of gcc can lead to this
amount of \gls{llc} misses.
Also, an interval of bad co-scheduling can impact the performance of gcc, but
this is unlikely when comparing best-cases, as the best-case slowdown compared
to solo runs for TB and SLD is 11.3s.


STB's run-time deviation is lower than TB's, thus it provides more
deterministic run-times for all benchmarks.
However, the determinism comes at the cost of worse median and best-case
values, but with the benefit of better worst-case performance.
While I expected a better median performance compared to TB, the values are not
bad.
Due to the connection between the applications, the improvement in worst-case
performance combined with the low deviation suggests, that the system fairly
distributes its resources.


\paragraph{Interval Size of 10ms}
% todo interval size of 100ms -> measurement setup -- no, because 10ms is much
% better hence there must be a separate paragraph.



\paragraph{Heavy Load.}
On a quad-core machine, the four SPEC benchmarks would be sufficient to fully
utilize the processor.
However, the quad-core machine with \gls{ht} has still some underutilized
resources, thus I double the workload by running each benchmark twice.
\cite{zhuravlev_addressing_2010} also turns to this approach to fully utilize
the Xeon server processor with eight physical cores, where each two cores share
one \gls{llc}.

Figure \ref{eval:fig:box_heavy} shows the results for a 100ms load balancing
interval for STB and TB. SLD is not part of this measurement, as the
performance shown in figure \ref{eval:fig:box_all} is not competitive.

Although I run two instances of each benchmark, I only use the numbers of one
instance.
The results in figure \ref{eval:fig:box_heavy} are consistent with figure
\ref{eval:fig:box_all} in the following two ways:
First, the run-time deviation is much lower for STB; second, the median
run-time is better in case of gcc, but worse for the other three applications,
however, the worst-case performance of STB is consistently better.
It is not evident from the bottom-left figure for lbm if STB is better; the
execution time of the top mark is 373.0.

Because the execution times of all applications is connected, the worse
best-case performance of one application benefits the better worst-case
performance of the other.
It is preferable to improve on the median execution time of all benchmarks and
I believe there is room for algorithmic improvements, however, a high
worst-case execution time in one long running application like mcf or lbm, allows for a
several good run-times in the short running gcc or gamess; Lbm's median is
three times as high as the gcc's or gamess's median.
So the goal besides the numbers is to provide an environment, where each
application uses as much resources as needed to perform well, while not causing
severe performance degradation to other applications.



\begin{figure}[h!]
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/finalPlots/boxplots/pdf/boxplot_gcc_mcf_heavy}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
  \includegraphics[width=\textwidth]{images/finalPlots/boxplots/pdf/boxplot_gamess_lbm_heavy}
  \end{subfigure}
  \caption{Concurrent run-times visualized using one box plot for each balancing
    algorithm; from top left to bottom right: SPEC gcc, mcf, gamess, and lbm.
    Time measurements are taken from the same concurrent run.}
    \label{eval:fig:box_heavy}
\end{figure}


\paragraph{Overall Performance.}
Which scheduler performed best?
How can this be computed, when the amount of repetitions for gcc, gamess, and
mcf is different?
By weighing the repetition count by  median run-time?
Just using the sum of the run-times of 100 lbm repetitions?



\paragraph{LLC Miss Math}
Intel documentation(\cite{intel_perf_analysis_2009}) states 60ns for a
\gls{llc} miss, independent
measurements\footnote{\url{http://www.7-cpu.com/cpu/Haswell.html}}
suggest 70ns, which accounts on a 3GHz CPU for a difference of 3 to 4 cycles.

So one second slowdown accounts for $10^9/70 \approx 14285714$ misses.
During one interval cycle $10^8/70 \approx 1428571$ misses.
Or one miss every 210 cycles.
