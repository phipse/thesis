% vim:set ft=tex:
\chapter{Evaluation}
\label{sec:evaluation}

% Zu jeder Arbeit in unserem Bereich gehört eine Leistungsbewertung. Aus
% diesem Kapitel sollte hervorgehen, welche Methoden angewandt worden,
% die Leistungsfähigkeit zu bewerten und welche Ergabnisse dabei erzielt
% wurden. Wichtig ist es, dem Leser nicht nur ein paar Zahlen
% hinzustellen, sondern auch eine Diskussion der Ergebnisse
% vorzunehmen. Es wird empfohlen zunächst die eigenen Erwartungen
% bezüglich der Ergebnisse zu erläutern und anschließend eventuell
% festgestellte Abweichungen zu erklären.

\ldots evaluation \ldots

\todo{write evaluation}
Numerous questions arise throughout this thesis.
Is behaviour aware load balancing on L4Re better than behaviour unaware
balancing?
What differences are observable between different SMT assignment schemes?

When compared with Linux \gls{cfs} scheduler, how different is the
performance?

Of interest are performance degradation compared to solo execution, when
executed in parallel with other applications.
Also, the related work suggests that the execution time is much more
deterministic, hence the maximum and minimum execution times are close
together.


\paragraph{Benchmarks}
SPEC progs:
solo runs: SLD, STB, MIPC, CFS
group runs: SLD, STB, MIPC, CFS
--> degradation of median compared  to solo runs

Group runs for different SMT algos: RR, load,
Group runs for different MIPC assignments: load, mpc-ipc

pingpong  clsvr group;
openmp-mmul distribution group; -- load generated by other mmul or fractal

\paragraph{SPEC benchmark description}
416.Gamess is a floating point benchmark written in Fortran executing a quantum
chemistry simulation.
It is a compute-bound benchmark and generates little \gls{mpc}.

The 403.GCC integer benchmark compiles a fixed amount of source code.
It has nine different input workloads, from which I selected the 166.i
workload.
The benchmark is also compute-bound but generates a low number of \gls{mpc}.

470.lbm simulates fluid dynamics and stresses the floating point units.
It generates a high amount of \gls{mpc}, therefore it is memory-bound.

429.mcf generates the highest number of \gls{mpc}.
It computes a combinatorial optimization using the integer units.

The benchmark set is consist of two compute-bound and two memory-bound
benchmarks, with one integer and one floating point benchmark per group.

\paragraph{Group Configuration Benchmarks}
To proof the group configuration awareness of the load balancer and to show the
performance improvement due to group awareness, I develop two benchmarks.
The first is a simple \gls{ipc} ping-pong, where the server answers a call from
the client with the number of the core it is currently running on.
The client measures the time for the \gls{ipc} call and prints the answer of
the server together with its own core number and the \gls{ipc} duration.

This setup shows, that client and server are migrated together, as long as
there is only one client per task.
If a task spawn several threads, the balancer distributes the additional
threads to other cores.

% todo  distr group benchmark: mmul-omp


\paragraph{64bit issue}
64bit raises an issue in the memory subsystem of L4Re.
Memory allocation and deallocation increases the runtime of 403.gcc by several
seconds for each repetition, when it runs alone.
Thereby, it is irrelevant if a simple load distribution, space-time-balancing,
or no load balancing runs the benchmark.
The task is run to completion and then restarted, so I expect each allocated
page to be free after completion.
The L4Re internal memory management should now merge free chunks of memory.
However, the size of the list of free memory areas increases constantly and
shows allocated pages in between, preventing compaction.
I suspect the time spent to search for possible merges and to iterate through
the list to be the cause for the increase in runtime.

The other SPEC benchmarks behave differently.
This issue does not affect the runtime of 416.games.
I suspect this is due to the little amount of \gls{llc}-misses, which points to
low memory usage, hence the memory subsystem is not stressed.
429.mcf shows slight but constant increase with each run.
429.mcf allocates all memory at the beginning of the benchmark, so the memory
subsystem is not used constantly, as is the case for 403.gcc.

470.lbm  TOBEMEASURED!


When 416.gamess, 429.mcf, and 470.lbm run together on L4Re the runtimes
increase occasionally between 2.5 and 4 times.
This increase cannot solely be explained by bad load balancing decisions.
Also, the average runtime for the first 50 iterations of 470.lbm is 60s lower
than the average runtime of the second 50 iterations.
Minimum, maximum and median runtime show a similar difference.
As 470.lbm is memory-bound, this suggest an issue with memory management
in L4Re.

Providing a different memory allocator to each task created via the ned script,
did not work.

As this issue did not present itself for 32bit and a analysis of L4Re's memory
management is not possible due to time constraints, all benchmarks are executed
in a 32bit environment.


\paragraph{Comparison}
  against Linux System \\
  against stupidly balanced l4re \\
  time for computing a fixed amount of work \\

\input{content/52_Baseline.tex}

\cleardoublepage

%%% Local Variables:
%%% TeX-master: "diplom"
%%% End:
